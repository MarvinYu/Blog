<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0"
    xmlns:content="http://purl.org/rss/1.0/modules/content/"
    xmlns:dc="http://purl.org/dc/elements/1.1/"
    xmlns:atom="http://www.w3.org/2005/Atom"
    xmlns:sy="http://purl.org/rss/1.0/modules/syndication/">

    <channel>
        <title>Martin Kleppmann's blog</title>
        <atom:link href="http://martin.kleppmann.com/feed.rss" rel="self" type="application/rss+xml" />
        <link>http://martin.kleppmann.com/</link>
        <description></description>
        <lastBuildDate>Wed, 17 Oct 2018 17:06:40 PDT</lastBuildDate>
        <language>en</language>
        <sy:updatePeriod>hourly</sy:updatePeriod>
        <sy:updateFrequency>1</sy:updateFrequency>

        
        
            <item>
                <title>Should you put several event types in the same Kafka topic?</title>
                <link>http://martin.kleppmann.com/2018/01/18/event-types-in-kafka-topic.html</link>
                <comments>http://martin.kleppmann.com/2018/01/18/event-types-in-kafka-topic.html#disqus_thread</comments>
                <pubDate>Thu, 18 Jan 2018 00:00:00 +0000</pubDate>
                <dc:creator>Martin Kleppmann</dc:creator>
                
                    <guid isPermaLink="true">http://martin.kleppmann.com/2018/01/18/event-types-in-kafka-topic.html</guid>
                
                <description><![CDATA[ This article was originally published on the Confluent blog. It has also been translated into Chinese. If you adopt a streaming data platform such as Apache Kafka, one of the most important questions to answer is: what topics are you going to use? In particular, if you have a bunch... ]]></description>
                <content:encoded><![CDATA[
                    <p><em>This article was originally published
<a href="https://www.confluent.io/blog/put-several-event-types-kafka-topic/">on the Confluent blog</a>.
It has also been <a href="http://www.infoq.com/cn/articles/event-types-in-kafka-topic">translated into Chinese</a>.</em></p>

<p>If you adopt a streaming data platform such as <a href="http://kafka.apache.org/">Apache Kafka</a>, one of the most important questions to answer is: <em>what topics are you going to use?</em>
In particular, if you have a bunch of different events that you want to publish to Kafka as messages, do you put them in the same topic, or do you split them across different topics?</p>

<p>The most important function of a topic is to allow a consumer to specify which subset of messages it wants to consume.
At the one extreme, putting absolutely all your data in a single topic is probably a bad idea, since it would mean consumers have no way of selecting the events of interest – they would just get everything.
At the other extreme, having millions of different topics is also a bad idea, since each topic in Kafka has a cost, and thus having a large number of topics will harm performance.</p>

<p>Actually, from a performance point of view, it’s the number of <em>partitions</em> that matters.
But since each topic in Kafka has at least one partition, if you have <em>n</em> topics, you inevitably have at least <em>n</em> partitions.
A while ago, <a href="https://www.confluent.io/blog/how-to-choose-the-number-of-topicspartitions-in-a-kafka-cluster/">Jun Rao wrote a blog post</a> explaining the cost of having many partitions (end-to-end latency, file descriptors, memory overhead, recovery time after a failure).
As a rule of thumb, if you care about latency, you should probably aim for (order of magnitude) hundreds of topic-partitions per broker node.
If you have thousands, or even tens of thousands of partitions per node, your latency will suffer.</p>

<p>That performance argument provides some guidance for designing your topic structure: if you’re finding yourself with many thousands of topics, it would be advisable to merge some of the fine-grained, low-throughput topics into coarser-grained topics, and thus reduce the proliferation of partitions.</p>

<p>However, performance is not the end of the story.
Even more important, in my opinion, are the data integrity and data modelling aspects of your topic structure.
We will discuss those in the rest of this article.</p>

<h2 id="topic--collection-of-events-of-the-same-type">Topic = collection of events of the same type?</h2>

<p>The common wisdom (according to several conversations I’ve had, and according to a <a href="https://groups.google.com/forum/#!topic/confluent-platform/XQTjNJd-TrU">mailing list thread</a>) seems to be: put all events of the same type in the same topic, and use different topics for different event types.
That line of thinking is reminiscent of relational databases, where a table is a collection of records with the same type (i.e. the same set of columns), so we have an analogy between a relational table and a Kafka topic.</p>

<p>The <a href="https://docs.confluent.io/current/schema-registry/docs/index.html">Confluent Avro Schema Registry</a> has traditionally reinforced this pattern, because it encourages you to use the same Avro schema for all messages in a topic.
That schema can be evolved while maintaining compatibility (e.g. by adding optional fields), but ultimately all messages have been expected to conform to a certain record type.
We’ll revisit this later in the post, after we’ve covered some more background.</p>

<p>For some types of streaming data, such as logged activity events, it makes sense to require that all messages in the same topic conform to the same schema.
However, some people are using Kafka for more database-like purposes, such as <a href="https://docs.microsoft.com/en-us/azure/architecture/patterns/event-sourcing">event sourcing</a>, or <a href="https://www.confluent.io/blog/build-services-backbone-events/">exchanging data between microservices</a>.
In this context, I believe it’s less important to define a topic a grouping of messages with the same schema.
Much more important is the fact that Kafka maintains <strong>ordering</strong> of messages within a topic-partition.</p>

<p>Imagine a scenario in which you have some entity (say, a customer), and many different things can happen to that entity: a customer is created, a customer changes their address, a customer adds a new credit card to their account, a customer makes a customer support enquiry, a customer pays an invoice, a customer closes their account.</p>

<p>The order of those events matters.
For example, we might expect that a customer is created before anything else can happen to a customer, and we might expect that after a customer closes their account nothing more will happen to them.
When using Kafka, you can preserve the order of those events by putting them all in the same partition.
In this example, you would use the customer ID as the partitioning key, and then put all these different events in the <strong>same</strong> topic.
They must be in the same topic because different topics mean different partitions, and ordering is not preserved across partitions.</p>

<h2 id="ordering-problems">Ordering problems</h2>

<p>If you did use different topics for (say) the customerCreated, customerAddressChanged, and customerInvoicePaid events, then a consumer of those topics may see the events in a nonsensical order.
For example, the consumer may see an address change for a customer that does not exist (because it has not yet been created, since the corresponding customerCreated event has been delayed).</p>

<p>The risk of reordering is particularly high if a consumer is shut down for a while, perhaps for maintenance or to deploy a new version.
While the consumer is stopped, events continue to be published, and those events are stored in the selected topic-partition on the Kafka brokers.
When the consumer starts up again, it consumes the backlog of events from all of its input partitions.
If the consumer has only one input, that’s no problem: the pending events are simply processed sequentially in the order they are stored.
But if the consumer has several input topics, it will pick input topics to read in some arbitrary order.
It may read all of the pending events from one input topic before it reads the backlog on another input topic, or it may interleave the inputs in some way.</p>

<p>Thus, if you put the customerCreated, customerAddressChanged, and customerInvoicePaid events in three separate topics, the consumer may well see all of the customerAddressChanged events before it sees any of the customerCreated events.
And so it is likely that the consumer will see a customerAddressChanged event for a customer that, according to its view of the world, has not yet been created.</p>

<p>You might be tempted to attach a timestamp to each message and use that for event ordering.
That might just about work if you are importing events into a data warehouse, where you can order the events after the fact.
But in a stream process, timestamps are not enough: if you get an event with a certain timestamp, you don’t know whether you still need to wait for some previous event with a lower timestamp, or if all previous events have arrived and you’re ready to process the event.
And relying on clock synchronisation generally leads to nightmares; for more detail on the problems with clocks, I refer you to Chapter 8 of <a href="http://dataintensive.net/">my book</a>.</p>

<h2 id="when-to-split-topics-when-to-combine">When to split topics, when to combine?</h2>

<p>Given that background, I will propose some rules of thumb to help you figure out which things to put in the same topic, and which things to split into separate topics:</p>

<ol>
  <li>
    <p>The most important rule is that <strong>any events that need to stay in a fixed order must go in the same topic</strong> (and they must also use the same partitioning key).
Most commonly, the order of events matters if they are about the same entity.
So, as a rule of thumb, we could say that all events <strong>about the same entity</strong> need to go in the same topic.</p>

    <p>The ordering of events is particularly relevant if you are using an <a href="https://msdn.microsoft.com/en-us/library/jj591559.aspx">event sourcing</a> approach to data modelling.
Here, the state of an <a href="https://www.martinfowler.com/bliki/DDD_Aggregate.html">aggregate object</a> is derived from a log of events by replaying them in a particular order.
Thus, even though there may be many different event types, all of the events that define an aggregate must go in the same topic.</p>
  </li>
  <li>
    <p>When you have events about different entities, should they go in the same topic or different topics?
I would say that if one entity depends on another (e.g. an address belongs to a customer), or if they are often needed together, they might as well go in the same topic.
On the other hand, if they are unrelated and managed by different teams, they are better put in separate topics.</p>

    <p>It also depends on the throughput of events: if one entity type has a much higher rate of events than another entity type, they are better split into separate topics, to avoid overwhelming consumers who only want the entity with low write throughput (see point four).
But several entities that all have a low rate of events can easily be merged.</p>
  </li>
  <li>
    <p>What if an event involves several entities?
For example, a purchase relates a product and a customer, and a transfer from one account to another involves at least those two accounts.</p>

    <p>I would recommend initially recording the event as a single atomic message, and not splitting it up into several messages in several topics.
It’s best to record events exactly as you receive them, in a form that is <a href="https://vimeo.com/123985284">as raw as possible</a>.
You can always split up the compound event later, using a stream processor – but it’s much harder to reconstruct the original event if you split it up prematurely.
Even better, you can give the initial event a unique ID (e.g. a UUID); that way later on when you split the original event into one event for each entity involved, you can carry that ID forward, making the provenance of each event traceable.</p>
  </li>
  <li>
    <p>Look at the number of topics that a consumer needs to subscribe to.
If several consumers all read a particular group of topics, this suggests that maybe those topics <a href="http://grokbase.com/t/kafka/users/15a7k5f1rr/mapping-events-to-topics">should be combined</a>.</p>

    <p>If you combine the fine-grained topics into coarser-grained ones, some consumers may receive unwanted events that they need to ignore.
That is not a big deal: consuming messages from Kafka is very cheap, so even if a consumer ends up ignoring half of the events, the cost of this overconsumption is probably not significant.
Only if the consumer needs to ignore the vast majority of messages (e.g. 99.9% are unwanted) would I recommend splitting the low-volume event stream from the high-volume stream.</p>
  </li>
  <li>
    <p>A changelog topic for a <a href="http://kafka.apache.org/10/documentation/streams/developer-guide#streams_duality">Kafka Streams state store</a> (KTable) should be a separate from all other topics.
In this case, the topic is managed by Kafka Streams process, and it should not be shared with anything else.</p>
  </li>
  <li>
    <p>Finally, what if none of the rules above tell you whether to put some events in the same topic or in different topics?
Then by all means group them by event type, by putting events of the same type in the same topic.
However, I think this rule is the least important of all.</p>
  </li>
</ol>

<h2 id="schema-management">Schema management</h2>

<p>If you are using a data encoding such as JSON, without a statically defined schema, you can easily put many different event types in the same topic.
However, if you are using a schema-based encoding such as Avro, a bit more thought is needed to handle multiple event types in a single topic.</p>

<p>As mentioned above, the Avro-based <a href="https://docs.confluent.io/current/schema-registry/docs/index.html">Confluent Schema Registry for Kafka</a> currently relies on the assumption that there is one schema for each topic (or rather, one schema for the key and one for the value of a message).
You can register new versions of a schema, and the registry checks that the schema changes are forward and backward compatible.
A nice thing about this design is that you can have different producers and consumers using different schema versions at the same time, and they still remain compatible with each other.</p>

<p>More precisely, when Confluent’s Avro serializer registers a schema in the registry, it does so under a <em>subject name</em>.
By default, that subject is <code>&lt;topic&gt;-key</code> for message keys and <code>&lt;topic&gt;-value</code> for message values.
The schema registry then checks the mutual compatibility of all schemas that are registered under a particular subject.</p>

<p>I have recently <a href="https://github.com/confluentinc/schema-registry/pull/680">made a patch to the Avro serializer</a> that makes the compatibility check more flexible.
The patch adds two new configuration options: <code>key.subject.name.strategy</code> (which defines how to construct the subject name for message keys), and <code>value.subject.name.strategy</code> (how to construct the subject name for message values).
The options can take one of the following values:</p>

<ul>
  <li><code>io.confluent.kafka.serializers.subject.TopicNameStrategy</code> (default): The subject name for message keys is <code>&lt;topic&gt;-key</code>, and <code>&lt;topic&gt;-value</code> for message values.
This means that the schemas of all messages in the topic must be compatible with each other.</li>
  <li><code>io.confluent.kafka.serializers.subject.RecordNameStrategy</code>: The subject name is the fully-qualified name of the Avro record type of the message.
Thus, the schema registry checks the compatibility for a particular record type, regardless of topic. This setting allows any number of different event types in the same topic.</li>
  <li><code>io.confluent.kafka.serializers.subject.TopicRecordNameStrategy</code>: The subject name is <code>&lt;topic&gt;-&lt;type&gt;</code>, where <code>&lt;topic&gt;</code> is the Kafka topic name, and <code>&lt;type&gt;</code> is the fully-qualified name of the Avro record type of the message.
This setting also allows any number of event types in the same topic, and further constrains the compatibility check to the current topic only.</li>
</ul>

<p>With this new feature, you can easily and cleanly put all the different events for a particular entity in the same topic.
Now you can freely choose the granularity of topics based on the criteria above, and not be limited to a single event type per topic.</p>

                ]]></content:encoded>
            </item>
        
            <item>
                <title>Drawing a map of distributed data systems</title>
                <link>http://martin.kleppmann.com/2017/03/15/map-distributed-data-systems.html</link>
                <comments>http://martin.kleppmann.com/2017/03/15/map-distributed-data-systems.html#disqus_thread</comments>
                <pubDate>Wed, 15 Mar 2017 00:00:00 +0000</pubDate>
                <dc:creator>Martin Kleppmann</dc:creator>
                
                    <guid isPermaLink="true">http://martin.kleppmann.com/2017/03/15/map-distributed-data-systems.html</guid>
                
                <description><![CDATA[ How we created an illustrated guide to help you find your way through the data landscape. Designing Data-Intensive Applications, the book I’ve been working on for four years, is finally finished, and should be available in your favorite bookstore in the next week or two. An incomplete beta (Early Release)... ]]></description>
                <content:encoded><![CDATA[
                    <p><strong>How we created an illustrated guide to help you find your way through the data landscape.</strong></p>

<p><a href="http://dataintensive.net/"><em>Designing Data-Intensive Applications</em></a>, the book I’ve been working on
for four years, is finally finished, and should be available in your favorite bookstore in the next
week or two. An incomplete beta (Early Release) edition has been available for the last 2½ years as
I continued working on the final chapters.</p>

<p>Throughout that process, we have been quietly working on a surprise. Something that has not been
part of any of the Early Releases of the book. In fact, something that I have never seen in any tech
book. And today we are excited to share it with you.</p>

<p>In <em>Designing Data-Intensive Applications</em>, each of the 12 chapters is accompanied by a map. The map
is a kind of graphical table of contents of the chapter, showing how some of the main ideas in the
chapter relate to each other.</p>

<p>Here is an example, from Chapter 3 (on storage engines):</p>

<p><a href="https://d3ansictanv2wj.cloudfront.net/ch03-map_lg-12f05f19df9e49098d509847518f1c03.jpg"><img src="/2017/03/ch3-map.jpg" width="550" height="419" alt="Chapter 3 map illustration from Designing Data-Intensive Applications" /></a><br />
<small>Figure 1. Map illustration from <em>Designing Data-Intensive Applications</em>, O’Reilly Media, 2017.</small></p>

<p>Don’t take it too seriously—some of it is a little tongue-in-cheek, we have taken some artistic
license, and the things included on the map are not exhaustive.</p>

<p>But it does reflect the structure of the chapter: political or geographic regions represent ways of
doing something, and cities represent particular implementations of those approaches. Similar things
are more likely to be close together, and roads or rivers represent concepts that connect different
implementations or regions.</p>

<p>Most computing books describe one particular piece of software and discuss all the aspects of how it
works. This book is structured differently: it starts with the concepts—discussing the high-level
approaches of how you might solve some problem, and comparing the pros and cons of each—and then
points out which pieces of software use which approach. The maps use the same structure: the region
in which a city is located tells you what approach it uses.</p>

<p>For example, in the map above, you can see a high-level subdivision into two countries: transaction
processing and analytics. Within transaction processing, there are two regions: log-structured
storage and B-trees, which are two ways of implementing OLTP storage engines. Within the B-tree
region, you see databases like MySQL and PostgreSQL<sup><a href="#_ftn1">[1]</a></sup>, while within
the log-structured region you see databases like Cassandra and HBase. On the analytics side, you can
see that the mountain range representing column storage reaches into both the data warehousing and
the Hadoop regions, since the approach applies to both.</p>

<p>The maps are in black and white, both because of practicalities of printing and also because I was
looking for a
<a href="http://www.tolkien.co.uk/file/IfbTdA8/5d04a105-e66b-4d9b-b218-928c691eb83d.jpg">Tolkien-esque style</a>.
You are, of course, welcome to color them in yourself. In fact, by coloring them in, you would be
following a fine tradition: for over three centuries, maps were printed in black and white from an
<a href="http://www.maphistory.info/understanding.html">engraved copper plate</a>, and then colored in by hand.</p>

<p>Each of the chapters has a map like that, focusing on the particular aspects discussed in that
chapter. This means that some cities appear on multiple islands—the data landscape is
multidimensional, so a city may lie in more than one (conceptual) realm. For example, the map below
is for Chapter 5 (on the topic of replication):</p>

<p><a href="https://d3ansictanv2wj.cloudfront.net/ch05-map_lg-32ac8f29402e391034f664a63461eef5.jpg"><img src="/2017/03/ch5-map.jpg" width="550" height="550" alt="Chapter 5 map illustration from Designing Data-Intensive Applications" /></a><br />
<small>Figure 2. Map illustration from <em>Designing Data-Intensive Applications</em>, O’Reilly Media, 2017.</small></p>

<p>Cities representing Cassandra, MongoDB, MySQL, and others appear on both this map, the Chapter 3 map
above, and some other maps, too.</p>

<p>Shipping routes connect some of the ports shown in the maps, in cases where there is a noteworthy
link between chapters. Most of the maps are of islands, but there are some exceptions. (I won’t give
away too much, but I just want to say…beware of the
<a href="https://en.wikipedia.org/wiki/Kraken">Kraken</a>.)</p>

<p>I am incredibly delighted that O’Reilly was willing to take on this crazy idea of creating maps. It
took a whole team to make them happen: from my
<a href="https://www.dropbox.com/s/yvuj7rqg66i93os/chapter3-map.jpg?dl=0">rough pencil sketches</a> (which
showed the structure but had absolutely zero artistic value),
<a href="http://shabbirdiwan.com/category/technique/scratch-board/">Shabbir Diwan</a>,
<a href="http://www.ediefreedman.com/">Edie Freedman</a>, and <a href="http://www.oreilly.com/pub/au/3771">Ron Bilodeau</a>
created the beautifully illustrated versions you see above, and
<a href="https://twitter.com/cmariebeau">Marie Beaugureau</a> patiently managed several rounds of revisions, in
which we painstakingly polished all the details.</p>

<p>Perhaps you’re curious to know how we got onto the idea of creating maps. Early on in the Early
Release of the book, some readers told me they would love some kind of flowchart to help them decide
quickly which database they should use for their application. Such flowcharts
<a href="https://medium.baqend.com/nosql-databases-a-survey-and-decision-guidance-ea7823a822d">have been attempted</a>,
but I never liked them much—it is tempting to read them out of context and jump to conclusions too
quickly, and they have to simplify the issues to the point of almost being intellectually dishonest.</p>

<p>My goal for <em>Designing Data-Intensive Applications</em> was different. I can’t in good faith give you
a recommendation for one particular tool because I don’t know enough about your particular
requirements. However, I can teach you what questions to ask and how to evaluate vendors’ claims
critically. That requires more subtlety and detail than can be expressed in a one-page flowchart,
which is why the book is 600 pages long, not one page.</p>

<p>However, I did think that some kind of graphical representation of the main ideas and structure
would be useful. I thought about <a href="https://en.wikipedia.org/wiki/Venn_diagram">Venn diagrams</a>, but
they’re excruciatingly boring. I thought about <a href="https://en.wikipedia.org/wiki/Mind_map">mind maps</a>,
and then started taking the “maps” bit more literally. I thought about the
<a href="https://www.goodreads.com/book/show/1175738.The_Atlas_of_Experience">Atlas of Experience</a> by Louise
van Swaaij and Jean Klare, a sublime book that represents aspects of human life as fictitious
<a href="http://www.imaginaryatlas.com/wp-content/uploads/2013/04/worldof-experience.jpg">places on a geographic map</a>.
(It is <a href="http://www.deharmonie.nl/titel/grote-atlas-van-de-belevingswereld/">originally in Dutch</a>;
the English translation is, sadly, out of print.)
<a href="https://www.goodreads.com/book/show/378.The_Phantom_Tollbooth">The Phantom Tollbooth</a> by Norton
Juster and Jules Feiffer does
<a href="http://bowenpeters.weebly.com/uploads/8/1/1/9/8119969/phantom_tollbooth_map.jpg">a similar thing</a>.</p>

<p>Closer to technology, similar map-style visualizations have been used to
<a href="https://xkcd.com/802/">represent online communities</a>, to visualize the
<a href="https://hpi.de/naumann/projects/rdbms-genealogy.html">history of relational databases</a> and
<a href="http://www.oreilly.com/go/languageposter">programming languages</a>, and to
<a href="https://twitter.com/linclark/status/708071521286266881">explain libraries related to Facebook’s React</a>.
I simply love the style.</p>

<p>Other inspirations for me are the ornate maps produced in the medieval and renaissance periods,
especially when it comes to whimsical
<a href="https://www.bl.uk/shop/sea-monsters-on-medieval-and-renaissance-maps/p-284">sea monsters</a>.
For example, around 1590, Abraham Ortelius published a
<a href="https://commons.wikimedia.org/wiki/File:Abraham_Ortelius-Islandia-ca_1590.jpg">wonderful map of Iceland</a>,
with <a href="https://en.wikipedia.org/wiki/Hekla">Mount Hekla</a> spewing fire, and surrounded by fantastical
sea monsters:</p>

<p><a href="/2017/03/ortelius-iceland.jpg"><img src="/2017/03/ortelius-iceland.jpg" width="550" height="410" alt="“Islandia” map by Abraham Ortelius, ca. 1590." /></a><br />
<small>Figure 3. “Islandia” map by Abraham Ortelius, ca. 1590. Source:
<a href="https://commons.wikimedia.org/wiki/File:Abraham_Ortelius-Islandia-ca_1590.jpg">Wikimedia Commons</a></small></p>

<p>I feel those maps and sea monsters reflect the 16th-century sense of excitement to explore the earth
and discover new places, as well as the dangers of sailing across unknown seas. And perhaps a bit of
that excitement exists today in our exploration of technologies for storing, processing, and using
data. There seems to be a lot of potential, but we also don’t really know what we’re doing, and it’s
sometimes a bit dangerous (raise your hand if you’ve lost data at some point because something went
wrong).</p>

<p>We hope the maps in <em>Designing Data-Intensive Applications</em> will help convey some of that
excitement, and also make you smile. In both the print and ebook editions, the map for each chapter
appears at the start of each chapter.</p>

<p>What’s more, we have taken all the individual chapter maps and assembled them into a poster—an
archipelago of islands representing technologies in the sea of distributed data. The poster also
includes some bonus sea monsters (of course).</p>

<p>If you are at
<a href="https://conferences.oreilly.com/strata/strata-ca">Strata + Hadoop World, San Jose</a>
this week, you can drop by the O’Reilly booth to pick up a free print of the poster so you have
something geeky and cool to hang on the wall in your office. Alternatively,
<a href="https://d3ansictanv2wj.cloudfront.net/ddia-poster-web-89b1c62f6eb4b57336c6cbe2148cc9a9.jpg">you can download a JPG version for free from the O’Reilly website</a>
for your personal, noncommercial use.</p>

<p>We hope you enjoy <em>Designing Data-Intensive Applications</em> and the maps as much as we enjoyed making them!</p>

<p><a href="/2017/03/ddia-poster.jpg"><img src="/2017/03/ddia-poster.jpg" width="550" height="733" alt="Me holding the poster with the maps" /></a><br />
<small>Figure 4. Me holding the poster with the maps.</small></p>

<p id="_ftn1">
[1] Footnote for the <a href="http://tirania.org/blog/archive/2011/Feb-17.html">well-actually</a>
crowd: yes, I know about hash indexes and GiST in PostgreSQL, various other index types in other
databases, and the fact that in MySQL the index type is actually a matter of the storage engine
(such as InnoDB), but those details are beside the point here. I am highlighting a dichotomy between
a page-oriented update-in-place approach and a log-structured, compaction-based approach. This
distinction is best explained with concrete examples, and the graphical representation cannot
capture all the subtleties that are discussed in the text of the book.
</p>

                ]]></content:encoded>
            </item>
        
            <item>
                <title>The probability of data loss in large clusters</title>
                <link>http://martin.kleppmann.com/2017/01/26/data-loss-in-large-clusters.html</link>
                <comments>http://martin.kleppmann.com/2017/01/26/data-loss-in-large-clusters.html#disqus_thread</comments>
                <pubDate>Thu, 26 Jan 2017 00:00:00 +0000</pubDate>
                <dc:creator>Martin Kleppmann</dc:creator>
                
                    <guid isPermaLink="true">http://martin.kleppmann.com/2017/01/26/data-loss-in-large-clusters.html</guid>
                
                <description><![CDATA[ This blog post uses MathJax to render mathematics. You need JavaScript enabled for MathJax to work. Many distributed storage systems (e.g. Cassandra, Riak, HDFS, MongoDB, Kafka, …) use replication to make data durable. They are typically deployed in a “Just a Bunch of Disks” (JBOD) configuration – that is, without... ]]></description>
                <content:encoded><![CDATA[
                    <p><em>This blog post uses <a href="https://www.mathjax.org/">MathJax</a> to render mathematics. You need JavaScript
enabled for MathJax to work.</em></p>

<p>Many distributed storage systems (e.g. Cassandra, Riak, HDFS, MongoDB, Kafka, …) use replication
to make data durable. They are typically deployed in a <a href="https://en.wikipedia.org/wiki/Non-RAID_drive_architectures">“Just a Bunch of Disks”</a> (JBOD)
configuration – that is, without RAID to handle disk failure. If one of the disks on a node dies,
that disk’s data is simply lost. To avoid losing data permanently, the database system keeps a copy
(replica) of the data on some other disks on other nodes.</p>

<p>The most common replication factor is 3 – that is, the database keeps copies of every piece of data
on three separate disks attached to three different computers. The reasoning goes something like
this: disks only die once in a while, so if a disk dies, you have a bit of time to replace it, and
then you still have two copies from which you can restore the data onto the new disk. The risk that
a second disk dies before you restore the data is quite low, and the risk that all three disks die
at the same time is so tiny that you’re more likely to get hit by an asteroid.</p>

<p>As a back-of-the-envelope calculation, if the probability of a single disk failing within some time
period is 0.1% (to pick an arbitrary number), then the probability of two disks failing is
(0.001)<sup>2</sup> = 10<sup>-6</sup>, and the probability of all three disks failing is
(0.001)<sup>3</sup> = 10<sup>-9</sup>, or one in a billion. This calculation assumes that
one disk’s failure is independent from another disk’s failure – which is not actually true, since
for example disks from the same manufacturing batch may show correlated failures – but it’s a good
enough approximation for our purposes.</p>

<p>So far the common wisdom. It sounds reasonable, but unfortunately it turns out to be untrue for many
data storage systems. In this post I will show why.</p>

<h2 id="its-so-easy-to-lose-data-la-la-laaa">It’s so easy to lose data, la la laaa</h2>

<p>If your database cluster really only consists of three machines, then the probability of all three
of them dying simultaneously is indeed very low (ignoring correlated faults, such as the datacenter
burning down). However, as you move to larger clusters, the probabilities change. The more nodes and
disks you have in your cluster, the more likely it is that you lose data.</p>

<p>This is a counter-intuitive idea. “Surely,” you think, “every piece of data is still replicated on
three disks. The probability of a disk dying doesn’t depend on the size of the cluster. So why
should the size of the cluster matter?” But I calculated the probabilities and drew a graph, and it
looked like this:</p>

<p><a href="/2017/01/dataloss.png"><img src="/2017/01/dataloss.png" width="550" alt="Graph of data loss probability depending on the number of nodes in the cluster" /></a></p>

<p>To be clear, this isn’t the probability of a single node failing – this is the probability of
<strong>permanently losing all three replicas</strong> of some piece of data, so restoring from backup (if you
have one) is the only remaining way to recover that data. The bigger your cluster, the more likely
you are haemorrhaging data. This is probably not what you intended when you decided to pay for
a replication factor of 3.</p>

<p>The y axis on that graph is a bit arbitrary, and depends on a lot of assumptions, but the direction
of the line is scary. Under the assumption that a node has a 0.1% chance of dying within some time
period, the graph shows that in a 8,000-node cluster, the chance of permanently losing all three
replicas of some piece of data (within the same time period) is about 0.2%. Yes, you read that
correctly: the risk of losing <strong>all three</strong> copies of some data is <strong>twice as great</strong> as the risk of
losing a single node! What is the point of all this replication again?</p>

<p>The intuition behind this graph is as follows: in an 8,000-node cluster it’s almost certain that
a <em>few</em> nodes are always dead at any given moment. That is normally not a problem: a certain rate of
churn and node replacement is expected and a part of routine maintenance. However, if you get
unlucky, there is <em>some piece of data</em> whose three replicas just happen to be three of those nodes
that have died – and if this is the case, that piece of data is gone forever. The data that is lost
is only a small fraction of the total dataset in the cluster, but still that’s not great: when you
use a replication factor of 3, you generally mean “I really don’t want to lose this data”, not “I
don’t mind occasionally losing a bit of this data, as long as it’s not too much”. Maybe that piece
of lost data was a particularly important one.</p>

<p>The probability that all three replicas are dead nodes depends crucially on the algorithm that the
system uses to assign data to replicas. The graph above is calculated under the assumption that the
data is split into a number of partitions (shards), and that each partition is stored on three
<em>randomly chosen</em> nodes (or pseudo-randomly with a hash function). This is the case with
<a href="https://www.akamai.com/kr/ko/multimedia/documents/technical-publication/consistent-hashing-and-random-trees-distributed-caching-protocols-for-relieving-hot-spots-on-the-world-wide-web-technical-publication.pdf">consistent hashing</a>, used in Cassandra and Riak, among others (as far as I know). With
other systems I’m not sure how the replica assignment works, so I’d appreciate any insights from
people who know about the internals of various storage systems.</p>

<h2 id="calculating-the-probability-of-data-loss">Calculating the probability of data loss</h2>

<p>Let me show you how I calculated that graph above, using a probabilistic model of a replicated
database.</p>

<p>Let’s assume that the probability of losing an individual node is \(p=P(\text{node loss})\). I am
going to ignore time in this model, and simply look at the probability of failure in some arbitrary
time period. For example, we could assume that \(p=0.001\) is the probability of a node failing
within a given day, which would make sense if it takes about a day to replace the node and restore
the lost data onto new disks. For simplicity I won’t distinguish between node failure and
<a href="https://www.backblaze.com/blog/hard-drive-reliability-update-september-2014/">disk failure</a>, and I will consider only permanent failures (ignoring crashes where
the node comes back again after a reboot).</p>

<p>Let \(n\) be the number of nodes in the cluster. Then the probability that \(f\) out of \(n\)
nodes have failed (assuming that failures are independent) is given by the
<a href="https://en.wikipedia.org/wiki/Binomial_distribution">binomial distribution</a>:</p>

<p>\[ P(f \text{ nodes failed}) = \binom{n}{f} \, p^f \, (1-p)^{n-f} \]</p>

<p>The term \(p^f\) is the probability that \(f\) nodes have failed, the term \((1-p)^{n-f}\) is
the probability that the remaining \(n-f\) have not failed, and \(\binom{n}{f}\) is the number
of different ways of picking \(f\) out of \(n\) nodes. \(\binom{n}{f}\) is pronounced
“n choose f”, and it is defined as:</p>

<p>\[ \binom{n}{f} = \frac{n!}{f! \; (n-f)!} \]</p>

<p>Let \(r\) be the replication factor (typically \(r=3\)). If we assume that \(f\) out of
\(n\) nodes have failed, what is the probability that a particular partition has all \(r\)
replicas on failed nodes?</p>

<p>Well, in a system that uses consistent hashing, each partition is assigned to nodes independently
and randomly (or pseudo-randomly). For a given partition, there are \(\binom{n}{r}\) different
ways of assigning the \(r\) replicas to nodes, and these assignments are all equally likely to
occur. Moreover, there are \(\binom{f}{r}\) different ways of choosing \(r\) replicas out of
\(f\) failed nodes – these are the ways in which all \(r\) replicas can be assigned to failed
nodes. We then work out the fraction of the assignments that result in all replicas having failed:</p>

<p>\[ P(\text{partition lost} \mid f \text{ nodes failed}) = \frac{\binom{f}{r}}{\binom{n}{r}} =
    \frac{f! \; (n-r)!}{(f-r)! \; n!} \]</p>

<p>(The vertical bar after “partition lost” is pronounced “given that”, and it indicates a
<a href="https://en.wikipedia.org/wiki/Conditional_probability">conditional probability</a>: the probability is given <em>under the assumption</em> that \(f\)
nodes have failed.)</p>

<p>So that’s the probability that all replicas of one particular partition has been lost. What about
a cluster with \(k\) partitions? If one or more partitions have been lost, we have lost data.
Thus, in order to not lose data, we require that all \(k\) partitions are not lost:</p>

<p>\begin{align}
P(\text{data loss} \mid f \text{ nodes failed})
    &amp;= 1 - P(\text{partition not lost} \mid f \text{ nodes failed})^k \\
    &amp;= 1 - \left( 1 - \frac{f! \; (n-r)!}{(f-r)! \; n!} \right)^k
\end{align}</p>

<p>Cassandra and Riak call partitions “vnodes” instead, but they are the same thing. In general, the
number of partitions \(k\) is independent from the number of nodes \(n\). In the case of
Cassandra, there is usually a <a href="http://www.datastax.com/dev/blog/virtual-nodes-in-cassandra-1-2">fixed number of partitions per node</a>; the default
is \(k=256\,n\) (configured by the <code>num_tokens</code> parameter), and this is also what I assumed for the
graph above. In Riak, the number of partitions is <a href="https://docs.basho.com/riak/kv/2.1.4/setup/planning/cluster-capacity/#ring-size-number-of-partitions">fixed when you create the cluster</a>, but
generally more nodes also mean more partitions.</p>

<p>With all of this in place, we can now work out the probability of losing one or more partitions in
a cluster of size \(n\) with a replication factor of \(r\). If the number of failures \(f\) is
less than the replication factor, we can be sure that no data is lost. Thus, we need to add up the
probabilities for all possible numbers of failures \(f\) with \(r \le f \le n\):</p>

<p>\begin{align}
P(\text{data loss})
    &amp;= \sum_{f=r}^{n} \; P(\text{data loss} \;\cap\; f \text{ nodes failed}) \\
    &amp;= \sum_{f=r}^{n} \; P(f \text{ nodes failed}) \; P(\text{data loss} \mid f \text{ nodes failed}) \\
    &amp;= \sum_{f=r}^{n} \binom{n}{f} \, p^f \, (1-p)^{n-f}
       \left[ 1 - \left( 1 - \frac{f! \; (n-r)!}{(f-r)! \; n!} \right)^k \right]
\end{align}</p>

<p>That is a bit of a mouthful, but I think it’s accurate. And if you plug in \(r=3\),
\(p=0.001\) and \(k=256\,n\), and vary \(n\) between 3 and 10,000, then you
get the graph above. I wrote <a href="https://gist.github.com/ept/1e094caaab5fa6471f529f589c4aaaf0">a little Ruby program</a> to do the calculation.</p>

<p>We can get a simpler approximation using the <a href="https://en.wikipedia.org/wiki/Boole%27s_inequality">union bound</a>:</p>

<p>\begin{align}
P(\text{data loss})
    &amp;= P(\ge\text{ 1 partition lost}) \\
    &amp;= P\left( \bigcup_{i=1}^k \text{partition } i \text{ lost} \right) \\
    &amp;\le k\, P(\text{partition lost}) = k\, p^r
\end{align}</p>

<p>Even though one partition failing is not independent from another partition failing, this
approximation still applies. And it seems to match the exact result quite closely: in the graph, the
data loss probability looks like a straight line, proportional to the number of nodes. The
approximation says that the probability is proportional to the number of partitions, which is
equivalent since we assumed a fixed 256 partitions per node.</p>

<p>Moreover, if we plug in the numbers for 10,000 nodes into the approximation, we get 
\(P(\text{data loss}) \le 256 \cdot 10^4 \cdot (10^{-3})^3 = 0.00256\), which matches the result
from the Ruby program very closely.</p>

<h2 id="and-in-practice">And in practice…?</h2>

<p>Is this a problem in practice? I don’t know. Mostly I think it’s an interesting and
counter-intuitive phenomenon. I’ve heard rumours that it is causing real data loss at companies with
large database clusters, but I’ve not seen the issue documented anywhere. If you’re aware of any
discussions on this topic, please point me at them.</p>

<p>The calculation indicates that in order to reduce the probability of data loss, you can reduce the
number of partitions or increase the replication factor. Using more replicas costs more, so it’s not
ideal for large clusters that are already expensive. However, the number of partitions presents an
interesting trade-off. Cassandra originally used one partition per node, but then
<a href="http://www.datastax.com/dev/blog/virtual-nodes-in-cassandra-1-2">switched to 256 partitions per node</a> a few years ago in order to achieve better
load distribution and more efficient rebalancing. The downside, as we can see from this calculation,
is a much higher probability of losing at least one of the partitions.</p>

<p>I think it’s probably possible to devise replica assignment algorithms in which the probability of
data loss does not grow with the cluster size, or at least does not grow as fast, but which
nevertheless have good load distribution and rebalancing properties. That would be an interesting
area to explore further. In that context, my colleague <a href="http://www.cl.cam.ac.uk/~sak70/">Stephan</a> pointed out that the expected
rate of data loss is constant in a cluster of a particular size, independent of the replica
assignment algorithm – in other words, you can choose between a high probability of losing a small
amount of data, and a low probability of losing a large amount of data! Is the latter better?</p>

<p>You need fairly large clusters before this effect really shows up, but clusters of thousands of
nodes are used by various large companies, so I’d be interested to hear from people with operational
experience at such scale. If the probability of permanently losing data in a 10,000 node cluster is
really 0.25% per day, that would mean a 60% chance of losing data in a year. That’s way higher than
the “one in a billion” getting-hit-by-an-asteroid probability that I talked about at the start.</p>

<p>Are the designers of distributed data systems aware of this issue? If I got this right, it’s
something that should be taken into account when designing replication schemes. Hopefully this blog
post will raise some awareness of the fact that just because you have three replicas you’re not
automatically guaranteed to be safe.</p>

<p><em>Thank you to <a href="https://twitter.com/matclayton">Mat Clayton</a> for bringing this issue to my attention, and to
<a href="http://www.cl.cam.ac.uk/~arb33/">Alastair Beresford</a>, <a href="http://www.cl.cam.ac.uk/~sak70/">Stephan Kollmann</a>, <a href="https://twitter.com/cmeik">Christopher Meiklejohn</a>,
and <a href="http://www.cl.cam.ac.uk/~drt24/">Daniel Thomas</a> for comments on a draft of this post.</em></p>


                ]]></content:encoded>
            </item>
        
            <item>
                <title>Announcing TRVE DATA: Placing a bit less trust in the cloud</title>
                <link>http://martin.kleppmann.com/2016/04/15/announcing-trve-data.html</link>
                <comments>http://martin.kleppmann.com/2016/04/15/announcing-trve-data.html#disqus_thread</comments>
                <pubDate>Fri, 15 Apr 2016 00:00:00 +0000</pubDate>
                <dc:creator>Martin Kleppmann</dc:creator>
                
                    <guid isPermaLink="true">http://martin.kleppmann.com/2016/04/15/announcing-trve-data.html</guid>
                
                <description><![CDATA[ In 2014, after 7 years in startups and internet companies, I left LinkedIn to take a sabbatical. (“Sabbatical” sounds better than “unemployment”, don’t you think?) For a year I worked full-time on my book, and explored what I wanted to do next. Then last year an opportunity came up that... ]]></description>
                <content:encoded><![CDATA[
                    <p>In 2014, after 7 years in startups and internet companies, I left LinkedIn to take a sabbatical.
(“Sabbatical” sounds better than “unemployment”, don’t you think?) For a year I worked full-time on
<a href="http://dataintensive.net/">my book</a>, and explored what I wanted to do next. Then last year an opportunity came up that
was just perfect. I started the new job part-time in October 2015, while finishing off the book
during my remaining time (it should be done in the next few months).</p>

<p>Today I would like to introduce the project that we are working on: <a href="https://www.cl.cam.ac.uk/research/dtg/trve/">TRVE DATA</a>, pronounced
“true data”. We’ve put up a little <a href="https://www.cl.cam.ac.uk/research/dtg/trve/">website</a> explaining the high-level idea, and in this blog
post I would like to briefly explain what it is, why we are doing it, and what makes me so excited
about it. If you want to keep in touch about the project, please <a href="https://lists.cam.ac.uk/mailman/listinfo/cl-trvedata">join our mailing
list</a>.</p>

<p>The project is based at <a href="https://www.cl.cam.ac.uk/">University of Cambridge Computer Laboratory</a>, where I am working with
some excellent people: <a href="http://www.cl.cam.ac.uk/~arb33/">Alastair Beresford</a>, <a href="http://www.cl.cam.ac.uk/~dac53/">Diana Vasile</a>, and
<a href="http://www.cl.cam.ac.uk/~sak70/">Stephan Kollmann</a>.</p>

<h2 id="placing-a-bit-less-trust-in-the-cloud">Placing a bit less trust in the cloud</h2>

<p>As you have perhaps heard, <a href="https://www.chriswatterston.com/blog/my-there-no-cloud-sticker">there is no cloud</a> – it’s just someone else’s computer. And
people are storing all sorts of sensitive data on it, blindly trusting that this computer will only
allow authorised users access. What if it is compromised?</p>

<p>It’s not just individuals’ personal data, but we’re talking about medical records, <a href="https://www.usenix.org/system/files/conference/usenixsecurity15/sec15-paper-mcgregor.pdf">journalistic
materials</a>, and data about critical infrastructure like power stations and chemical
plants. Here are a few anecdotes from conversations I have had recently:</p>

<ul>
  <li>A BBC journalist told me that they are officially banned from using Google Docs, but they use it
anyway, because it’s just so convenient.</li>
  <li>I have even heard rumours that the NHS (the English national health service) stores a worrying
amount of patient medical data in Google spreadsheets.</li>
  <li>Lawyers on high-profile court cases will happily communicate with their clients by unencrypted
email. Even though their communication enjoys <a href="https://en.wikipedia.org/wiki/Legal_professional_privilege">special protections under the law</a>, the
technology doesn’t reflect that importance.</li>
  <li>The same thing goes for <a href="https://twitter.com/csoghoian/status/700802867322441728">diplomats</a>.</li>
  <li>Some Internet-of-Things companies… oh my god, don’t ask about their security if you want to
sleep at night.</li>
</ul>

<p>I don’t object to cloud services <em>per se</em> – it’s incredibly convenient not to have to run your own
infrastructure, and Google, Amazon or Microsoft almost certainly do a better job than you would if
you were running your own server. However, I am concerned that there is too much blind trust
involved.</p>

<p>When data is stored in AWS, Google Cloud Platform, Google Docs, Evernote, iCloud, Dropbox, etc. you
have no idea what the cloud provider is doing with it. Are they using it to train neural networks?
Are they letting governments around the world access it? Are they mining it and selling the results
for advertising purposes? Do they have an untrustworthy employee who is secretly looking at the
data? Do they have a security vulnerability through which criminals can steal it? At best you have
a vaguely-worded and unenforceable privacy policy to read, but most likely you simply don’t know
what is happening to your data.</p>

<h2 id="end-to-end-encryption">End-to-end encryption</h2>

<p>Today, it is common to use SSL/TLS for encryption of data as it moves across the internet, and disk
encryption for data at rest. But that encryption ends at the server software, and almost all cloud
services today process data in the clear on the servers. Therefore, anyone who can get access to the
server can also get access to the data.</p>

<p>On the other hand, <a href="http://martin.kleppmann.com/2015/11/10/investigatory-powers-bill.html">end-to-end encryption</a> techniques mostly remove the need to trust the
server, by encrypting data on one end-user device such that only another end-user device can decrypt
it. There may still be servers and cloud services involved, but they cannot read or tamper with the
data. Someone who wants to steal the data would then have to break into one of the end-user devices
– which is <a href="https://www.theguardian.com/technology/2016/mar/21/fbi-apple-court-hearing-postpone-unlock-terrorist-iphone">still possible in most cases</a>, depending on security practices, but at
least it is a much reduced attack surface, with fewer things that can go wrong.</p>

<p>End-to-end encryption is becoming popular for messaging apps, most recently rolled out in
<a href="https://blog.whatsapp.com/10000618/end-to-end-encryption">WhatsApp</a>, along with <a href="https://whispersystems.org/">Signal</a>, iMessage (with
<a href="http://blog.cryptographyengineering.com/2016/03/attack-of-week-apple-imessage.html">reservations</a>), and others. But we have <a href="https://dymaxion.org/essays/pleasestop.html">so much other important data besides text
messages</a>! What about that?</p>

<p>The problem is that it’s fairly easy to knock together a SaaS web app with Rails, or to build
a mobile app with a backend-as-a-service, but it is really hard to do the same in a way that uses
end-to-end encryption. The crypto itself is terribly difficult to get right, and even if you use an
established secure messaging protocol, you then have the problem that many services, databases,
libraries and tools can no longer be used, since they assume they can work with unencrypted data –
so you have to start almost from scratch. At the moment it is simply not feasible for most
application developers to use end-to-end encryption.</p>

<p>And that is what we are trying to change.</p>

<h2 id="making-end-to-end-security-the-new-default">Making end-to-end security the new default</h2>

<p>The long-term goal of TRVE DATA is quite ambitious: namely, to make it just as easy to build
applications with end-to-end security, and to make those applications equally usable, as the apps
without end-to-end security today.</p>

<p>Today, using http instead of https is increasingly frowned upon; I hope that in some years time, not
using end-to-end security will be equally frowned upon. Today, we trust cloud services but not the
network; in future, I hope that we will trust neither cloud services nor the network. We will still
be using the internet and cloud services, but we will use cryptographic tools to ensure they can’t
mess with our data.</p>

<p>I want the tools for building secure applications to be so good that it will be a no-brainer to use
them. I want strong security to become the new default, and to raise the bar for all apps.</p>

<p>Of course, we have a very long way to go before this is reality. For now, we are concentrating on
a particular type of application: collaborative document editing. This is still a quite broad
category, including text documents, spreadsheets, graphics, to-do lists, notes, address books,
calendars, and so on.</p>

<p>For this kind of data, the TRVE project is building general-purpose libraries and tools that will
automatically sync data across several devices, allow sharing with other users, allow several people
to edit the same document in real time, and allow users to continue working offline. And all of the
communication between devices will, of course, be encrypted and authenticated end-to-end, with TRVE
handling key management as well as data sync.</p>

<p>The software we build will be open source and freely available. Our work-in-progress prototype is
already on GitHub, but I won’t link to it — remember, this project only started six months ago, and
I’m working on in part-time. The code is not yet in a fit state to be used. But this is where we’re
heading.</p>

<h2 id="motivation-and-concerns">Motivation and concerns</h2>

<blockquote>
  <p>“Let us speak no more of faith in man, but bind him down from mischief by the chains of cryptography.”</p>

  <p>— <a href="http://www.theatlantic.com/politics/archive/2014/05/edward-snowdens-other-motive-for-leaking/370068/">Edward Snowden</a>, invoking <a href="http://www.constitution.org/cons/kent1798.htm">Thomas Jefferson</a></p>
</blockquote>

<p>Jefferson’s original quote was about the US constitution: a document designed to deliberately
restrict the powers of government, and to keep it accountable to its citizens. History has
repeatedly shown that putting too much unchecked power in the hands of a small number of people
leads to abuses of power and various problems, even if they start with benevolent intentions.</p>

<p>Snowden’s quote is so apt because the rise of cloud services and “Big Data” have caused
a concentration of power in the hands of a small number of large companies. Cryptography is to data
what the constitution is to political power: a means of <a href="http://web.cs.ucdavis.edu/~rogaway/papers/moral.html">giving some power and control back to
individuals</a>, and keeping powerful people honest. It makes mass surveillance harder and
helps preserve civil liberties.</p>

<p>I will preempt the inevitable question: <em>“What if terrorists use this software to plan an
attack?”</em> This issue merits a longer discussion, but the short answer is: terrorists use cars, guns
and explosives as well, all of which are far more dangerous than crypto. And I don’t see any sign of
Ford stopping production of their cars because they might be used by terrorists.</p>

<p>It’s actually pretty hard to kill someone with cryptography. You can try boring someone to death, or
hitting them over the head with a crypto textbook, but that’s about it. As technologies go, crypto
is pretty non-lethal — in fact, it is a purely defensive technology.</p>

<p>On the other hand, encryption is absolutely essential for protecting data that is legitimately
sensitive, and to give some freedom to people <a href="https://twitter.com/matthew_d_green/status/720538970640269313">living under repressive regimes</a>. Weakening it
for the convenience of law enforcement, as proposed in the <a href="http://martin.kleppmann.com/2015/11/10/investigatory-powers-bill.html">Investigatory Powers Bill</a> in
the UK and <a href="http://www.burr.senate.gov/imo/media/doc/BAG16460.pdf">Feinstein-Burr</a> in the US, would be a <a href="http://data.parliament.uk/writtenevidence/committeeevidence.svc/evidencedocument/draft-investigatory-powers-bill-committee/draft-investigatory-powers-bill/written/26275.html">big mistake</a>.</p>

<h2 id="the-way-forward">The way forward</h2>

<p>I believe that end-to-end security will soon be regarded as a necessity for any sort of important
data. For example, the Bar Council of the UK (the association of lawyers who represent their clients
in court) already <a href="http://www.barcouncil.org.uk/media/407878/cloud_computing.pdf">recommends using end-to-end encryption</a> for data stored in the
cloud.</p>

<p>This trend starts with the most sensitive professions like doctors, lawyers, and <a href="http://www.cima.ned.org/wp-content/uploads/2016/03/CIMA-Journalist-Digital-Tools-03-01-15.pdf">journalists</a>,
but I expect it to grow – in order to maintain regulatory compliance, to prevent industrial
espionage, and to meet data protection requirements. The demand for better security comes not from
criminals trying to evade the law, but from professionals whose job involves dealing with important
data.</p>

<p>I am working on the TRVE DATA project because I feel this is one of the most important issues in
computing and society today, and I am hoping we will be able to make a positive difference. It’s
a long-term project, and we’re only just getting started.</p>

<p>We have set up a <a href="https://lists.cam.ac.uk/mailman/listinfo/cl-trvedata">public mailing list</a> for anyone who is interested in the project,
where we are planning to post monthly updates on our progress, and invite ideas and discussion from
anyone who would like to contribute. You can also find <a href="https://twitter.com/trvedata">@trvedata</a> on Twitter. Please join
us, and spread the word.</p>


                ]]></content:encoded>
            </item>
        
            <item>
                <title>Device security and the FBI</title>
                <link>http://martin.kleppmann.com/2016/03/30/device-security-fbi.html</link>
                <comments>http://martin.kleppmann.com/2016/03/30/device-security-fbi.html#disqus_thread</comments>
                <pubDate>Wed, 30 Mar 2016 00:00:00 +0000</pubDate>
                <dc:creator>Martin Kleppmann</dc:creator>
                
                    <guid isPermaLink="true">http://martin.kleppmann.com/2016/03/30/device-security-fbi.html</guid>
                
                <description><![CDATA[ This article was originally published on The Conversation under the title “FBI backs off from its day in court with Apple this time – but there will be others”. After a very public stand-off over an encrypted terrorist’s smartphone, the FBI has backed down in its court case against Apple,... ]]></description>
                <content:encoded><![CDATA[
                    <p><em>This article was originally <a href="https://theconversation.com/fbi-backs-off-from-its-day-in-court-with-apple-this-time-but-there-will-be-others-56932">published on The Conversation</a> under the title
“FBI backs off from its day in court with Apple this time – but there will be others”.</em></p>

<p>After a <a href="https://theconversation.com/why-apple-is-making-a-stand-against-the-fbi-54925">very public stand-off</a> over an encrypted terrorist’s smartphone, the FBI has
<a href="http://www.theguardian.com/technology/2016/mar/21/fbi-apple-court-hearing-postpone-unlock-terrorist-iphone">backed down</a> in its court case against Apple, stating that an “outside party”
– rumoured to be <a href="https://www.rt.com/usa/336948-fbi-israel-crack-iphone/">an Israeli mobile forensics company</a> – has found a way of accessing
the data on the phone.</p>

<p>The exact method is not known. Forensics experts <a href="http://www.zdziarski.com/blog/?p=5966">have speculated</a> that it involves tricking
the hardware into not recording how many passcode combinations have been tried, which would allow
all 10,000 possible four-digit passcodes to be tried within a fairly short time. This technique
would apply to the iPhone 5C in question, but not newer models, which have stronger hardware
protection through the so-called <a href="https://www.apple.com/business/docs/iOS_Security_Guide.pdf">secure enclave</a>, a chip that performs
security-critical operations in hardware. The FBI has denied that the technique involves
<a href="https://www.washingtonpost.com/world/national-security/the-fbi-is-testing-a-code-based-way-to-get-into-the-san-bernardino-iphone/2016/03/24/bc79cd14-f1dc-11e5-a61f-e9c95c06edca_story.html">copying storage chips</a>.</p>

<p>So while the details of the technique <a href="http://www.theguardian.com/technology/2016/mar/22/apple-fbi-san-bernardino-iphone-method-for-cracking">remain classified</a>, it’s reasonable to assume
that <a href="https://theintercept.com/2016/03/08/snowden-fbi-claim-that-only-apple-can-unlock-phone-is-bullshit/">any security technology can be broken</a> given sufficient resources. In fact, the
technology industry’s dirty secret is that most products are frighteningly insecure.</p>

<p>Even when security technologies are carefully designed and reviewed by experts, mistakes happen. For
example, researchers recently found a way of <a href="http://blog.cryptographyengineering.com/2016/03/attack-of-week-apple-imessage.html">breaking the encryption of Apple’s iMessage
service</a>, one of the most prominent examples of end-to-end encryption (which ensures that
even the service provider cannot read the messages travelling via its network).</p>

<p>Most products have a much worse security record, as they are not designed by security experts, and
often contain flaws that are easily found by attackers. For example, <a href="http://boingboing.net/2016/01/19/griefer-hacks-baby-monitor-te.html">internet-connected baby
monitors</a> that could be hacked and allow strangers to <a href="http://sfglobe.com/2016/01/06/stranger-hacks-familys-baby-monitor-and-talks-to-child-at-night/">talk to their child</a> at
night. Insecure cars that <a href="https://theconversation.com/auto-industry-must-tackle-its-software-problems-to-stop-hacks-as-cars-go-online-45325">could be controlled via an internet connection</a> while being driven.
Drug infusion pumps at US hospitals that could be hacked by an attacker to <a href="https://www.boxer.senate.gov/?p=release&amp;id=3254">manipulate drug dosage
levels</a>.</p>

<p>Even national infrastructure is vulnerable, with software weaknesses exploited to cause serious
damage at a <a href="http://www.bbc.co.uk/news/technology-30575104">German steel mill</a>, bring down parts of the <a href="https://theconversation.com/the-cyberattack-on-ukraines-power-grid-is-a-warning-of-whats-to-come-52832">Ukrainian power
grid</a>, and <a href="http://news.softpedia.com/news/hackers-modify-water-treatment-parameters-by-accident-502043.shtml">alter the mix of chemicals added to drinking water</a>.
While our lives depend more and more on “smart” devices, they are frequently designed in incredibly
stupid ways.</p>

<h2 id="insecure-by-design">Insecure by design</h2>

<p>The conflict between Apple and the FBI was particularly jarring to security experts, seen as an
attempt to deliberately make technology less secure and win legal precedent to gain access to other
devices in the future. Smartphones are becoming increasingly ubiquitous, and we know from the
Snowden files that the NSA can <a href="http://www.theguardian.com/world/2014/feb/01/edward-snowden-intelligence-leak-nsa-contractor-extract">turn on a phone’s microphone</a> remotely without
the owner’s knowledge. We are heading towards a state in which every inhabited space contains
a microphone (and a camera) that is connected to the internet and which might be recording anything
you say. This is not even a paranoid exaggeration.</p>

<p>So, in a world in which we are constantly struggling to make things more secure, the FBI’s desire to
create a backdoor to provide it access is like pouring gasoline on the fire.</p>

<p>The problem with security weaknesses is that it is impossible to control who can use them.
Responsible researchers report them to the vendor so that they can be fixed, and sometimes receive
a <a href="http://www.tripwire.com/state-of-security/vulnerability-management/11-essential-bug-bounty-programs-of-2015/">bug bounty</a> in return. But those who want to make more money may <a href="http://www.wired.com/2015/11/heres-a-spy-firms-price-list-for-secret-hacker-techniques/">secretly sell the
knowledge to the highest bidder</a>. Customers of this <a href="https://theconversation.com/trusting-hackers-with-your-security-youd-better-be-able-to-sort-the-whitehats-from-the-blackhats-44477">dark trade in
vulnerabilities</a> often include <a href="https://citizenlab.org/2015/08/hacking-team-leak-highlights-citizen-lab-research/">governments with repressive human rights
records</a>.</p>

<p>If the FBI has found a means of getting data off a locked phone, that means the intelligence
services of other countries have probably independently developed the same technique – or been sold
it by someone who has. So if an American citizen has data on their phone that is of intelligence
interest to another country that data is at risk if the phone is lost or stolen.</p>

<p>Most people will never be of intelligence interest of course, so perhaps such fears are overblown.
But the push from governments, for example through the pending <a href="https://theconversation.com/us/topics/investigatory-powers-bill">Investigatory Powers Bill</a>
in the UK, to allow the security services to hack devices in bulk – even if the devices belong to
people who are not suspected of any crime – cannot be ignored.</p>

<p>Bulk hacking powers, taken together with insecure, internet-connected microphones and cameras in
every room, are a worrying combination. It is a cliche to conjure up Nineteen Eighty-Four, but the
picture it paints is something very much like Orwell’s telescreens.</p>

<iframe width="440" height="260" src="https://www.youtube.com/embed/CCfW6HFP5cI?wmode=transparent&amp;start=0" frameborder="0" allowfullscreen=""></iframe>

<h2 id="used-by-one-used-by-all">Used by one, used by all</h2>

<p>To some extent law enforcement has historically benefited from poor computer security, as hacking
a poorly secured digital device is easier and cheaper than planting a microphone in someone’s house
or rifling their physical belongings. No wonder that the former CIA director <a href="http://www.wired.com/2012/03/petraeus-tv-remote/">loves the Internet of
Things</a>.</p>

<p>This convenience often tempts governments to deliberately weaken device security – the FBI’s case
against Apple is just one example. In the UK, the proposed Investigatory Powers Bill allows the
secretary of state to issue “<a href="http://www.theguardian.com/technology/2015/nov/09/tech-firms-snoopers-charter-end-strong-encryption-britain-ip-bill">technical capability notices</a>”, which are secret
government orders to demand manufacturers make a device or service deliberately less secure than it
could be. GCHQ’s new MIKEY-SAKKE standard for encrypted phone calls is also <a href="https://www.benthamsgaze.org/2016/01/19/insecure-by-design-protocols-for-encrypted-phone-calls/">deliberately
weakened</a> to allow easier surveillance.</p>

<p>But a security flaw that can be used by one can be used by all, whether legitimate police
investigations or hostile foreign intelligence services or organised crime. The fears of <a href="https://cyber.law.harvard.edu/pubrelease/dont-panic/Dont_Panic_Making_Progress_on_Going_Dark_Debate.pdf">criminals
and terrorists “going dark” are overblown</a>, but the risk to life from insecure
infrastructure is real: fixing these weaknesses should be our priority, not striving to make devices
less secure for the sake of law enforcement.</p>


                ]]></content:encoded>
            </item>
        
            <item>
                <title>Should law enforcement services have a backdoor into smartphones?</title>
                <link>http://martin.kleppmann.com/2016/02/18/law-enforcement-smartphone-backdoors.html</link>
                <comments>http://martin.kleppmann.com/2016/02/18/law-enforcement-smartphone-backdoors.html#disqus_thread</comments>
                <pubDate>Thu, 18 Feb 2016 00:00:00 +0000</pubDate>
                <dc:creator>Martin Kleppmann</dc:creator>
                
                    <guid isPermaLink="true">http://martin.kleppmann.com/2016/02/18/law-enforcement-smartphone-backdoors.html</guid>
                
                <description><![CDATA[ This article was originally supposed to be published on The Conversation, but was dropped because they already had another, very similar article. Apple has found itself challenging a judge’s ruling after it refused to help the FBI break into the secured iPhone of Syed Rizwan Farook, one of the shooters... ]]></description>
                <content:encoded><![CDATA[
                    <p><em>This article was originally supposed to be published on <a href="https://theconversation.com/">The Conversation</a>,
but was dropped because they already had another, very similar article.</em></p>

<p>Apple has found itself challenging a judge’s ruling after it <a href="http://www.theguardian.com/us-news/2016/feb/17/apple-ordered-to-hack-iphone-of-san-bernardino-shooter-for-fbi">refused to help the FBI</a>
break into the secured iPhone of Syed Rizwan Farook, one of the shooters at last year’s attack in
San Bernardino, California. As the data on the phone is encrypted and investigators need the
passcode in order to decrypt it they have <a href="https://www.documentcloud.org/documents/2714001-SB-Shooter-Order-Compelling-Apple-Asst-iPhone.html">demanded that Apple</a>, as the phone’s
manufacturer, help them.</p>

<p>On the face of it, assisting with the investigation of a terrorist act seems like a very reasonable
demand. Why would Apple refuse it? To understand we have to take a closer look at how smartphone
security works.</p>

<p>Apple’s <a href="https://www.apple.com/business/docs/iOS_Security_Guide.pdf">iOS operating system security guide</a> explains that the iPhone deliberately
introduces a delay when checking a passcode: it allows at most 12 passcode guesses per second, and
sometimes waits several seconds after an incorrect guess, depending on your settings. This is to
prevent “brute force attacks”, where software tries every possible combination in order to find the
correct passcode.</p>

<p>On recent iPhones, this delay is enforced by the so-called <a href="http://blog.cryptographyengineering.com/2014/10/why-cant-apple-decrypt-your-iphone.html">Secure Enclave</a>,
a special chip that handles security-critical aspects of the phone – such as encryption and the
fingerprint sensor – in hardware. In the case of Farook’s phone, it’s an iPhone 5C which <a href="http://blog.trailofbits.com/2016/02/17/apple-can-comply-with-the-fbi-court-order/">does not
have a Secure Enclave</a>.</p>

<p>The reason Apple develops these security features is not anti-government activism – it is simply
commercial demand. The job of big corporate IT departments is to protect sensitive company data
against industrial espionage by competitors, and against attacks from criminal gangs. In many
industries a compromise of sensitive data would breach compliance with government regulations, and
<a href="http://www.theguardian.com/society/2014/aug/26/ministry-justice-fined-180000-losing-hard-drive-sensitive-data-prisoners">risk a heavy fine</a> or even criminal charges. This is not to mention the
reputational damage, as <a href="https://theconversation.com/talktalk-hack-perhaps-well-finally-take-cybersecurity-seriously-50144">many firms have found</a>. And the same applies to individuals for
much the same reasons.</p>

<p>So if companies are willing to pay a premium for products that will keep their data safe, they will
happily buy iPhones for their employees, and require them to set a strong passcode. That way, even
if an employee accidentally forgets their phone at a bar, or it is <a href="http://www.bbc.co.uk/news/uk-england-manchester-19960966">stolen from the employee’s
home</a>, the IT department can feel confident that whoever picks up the phone won’t be able
to access the sensitive emails and business records that may be stored on it.</p>

<p>For example, even if the phone is picked up by an agent of a hostile foreign intelligence service,
and they take it to their lab and open it up, remove the storage chips and attach them to computer
forensics apparatus – even then, they probably <a href="http://www.darthnull.org/2014/10/06/ios-encryption">won’t be able to salvage the data</a>.
An unlikely scenario for most people, but if you work for a defence contractor or the diplomatic
corps, you have to worry about such things. If companies are willing to pay for security features to
protect against such scenarios, it makes sense for companies such as Apple <a href="http://www.apple.com/customer-letter/">to take note of their
wishes</a>.</p>

<p>But what about situations when it might be legitimate for law enforcement to try to access this
data? If the FBI turn up with a warrant signed by a judge, should they not be able to get their
hands on the information?</p>

<p>The problem is that if there is a special method or technique, besides knowing the right passcode,
that grants access to protected data – known as a <a href="https://theconversation.com/could-encryption-backdoors-safeguard-privacy-and-fight-terror-online-53419">backdoor</a> – then that method or technique could
be used for both legitimate and illegitimate uses. How do you control who can use that backdoor? If
a phone manufacturer creates a backdoor for police, it <a href="https://www.cl.cam.ac.uk/~rja14/Papers/doormats.pdf">will inevitably be used by criminals as
well</a>.</p>

<p>For example, imagine the FBI had a special piece of software that would allow them to unlock an
iPhone if authorised by a warrant. It would only take one rogue employee, or one hack of the FBI’s
computer systems, to leak that software onto the internet for any criminal – or foreign spies – to
use. Imagine they had a hardware device that would magically open an iPhone and reveal it’s data:
a few of those devices would end up on the black market very quickly. <a href="http://www.theguardian.com/technology/2015/jul/09/opm-hack-21-million-personal-information-stolen">Government departments can be
hacked</a>, just like anyone else, and there are always employees looking to make a quick
buck.</p>

<p>Encryption backdoors <a href="https://www.eff.org/deeplinks/2016/02/eff-support-apple-encryption-battle">deliberately weaken security</a>. This is fundamentally a bad idea, and can
have many <a href="http://blog.cryptographyengineering.com/2015/12/on-juniper-backdoor.html">unforeseen consequences</a>. Trying to restrict access to strong encryption, as the
<a href="https://en.wikipedia.org/wiki/Export_of_cryptography_from_the_United_States">US government tried and failed to do in the 1990s</a>, is counterproductive and
pointless: <a href="http://blog.cryptographyengineering.com/2015/03/attack-of-week-freak-or-factoring-nsa.html">counterproductive</a> because it only harms innocent people, whereas those its aimed
at will find a way to work around it, and pointless because strong encryption products are <a href="https://www.schneier.com/cryptography/paperfiles/worldwide-survey-of-encryption-products.pdf">found
throughout the world</a>: people will just go somewhere else for their encryption if they
can’t get it at home.</p>

<p>It is understandable that in the wake of a crime or terrorist attack, politicians and law
enforcement need to be seen as “doing something”. However, that does not mean law enforcement should
be able to do everything. It is worth remembering why security mechanisms like encryption exist in
the first place: to protect sensitive medical, legal, journalistic, financial, diplomatic,
commercial or military data from falling into the wrong hands. Trying to weaken those protections
would cause far more harm than good.</p>


                ]]></content:encoded>
            </item>
        
            <item>
                <title>How to do distributed locking</title>
                <link>http://martin.kleppmann.com/2016/02/08/how-to-do-distributed-locking.html</link>
                <comments>http://martin.kleppmann.com/2016/02/08/how-to-do-distributed-locking.html#disqus_thread</comments>
                <pubDate>Mon, 08 Feb 2016 00:00:00 +0000</pubDate>
                <dc:creator>Martin Kleppmann</dc:creator>
                
                    <guid isPermaLink="true">http://martin.kleppmann.com/2016/02/08/how-to-do-distributed-locking.html</guid>
                
                <description><![CDATA[ As part of the research for my book, I came across an algorithm called Redlock on the Redis website. The algorithm claims to implement fault-tolerant distributed locks (or rather, leases [1]) on top of Redis, and the page asks for feedback from people who are into distributed systems. The algorithm instinctively... ]]></description>
                <content:encoded><![CDATA[
                    <p>As part of the research for <a href="http://dataintensive.net/">my book</a>, I came across an algorithm called <a href="http://redis.io/topics/distlock">Redlock</a> on the
<a href="http://redis.io/">Redis</a> website. The algorithm claims to implement fault-tolerant distributed locks (or rather,
<a href="https://pdfs.semanticscholar.org/a25e/ee836dbd2a5ae680f835309a484c9f39ae4e.pdf" title="Cary G Gray and David R Cheriton. Leases: An Efficient Fault-Tolerant Mechanism for Distributed File Cache Consistency. SOSP 1989">leases</a> [1]) on top of Redis, and the page asks for feedback from people who are into
distributed systems. The algorithm instinctively set off some alarm bells in the back of my mind, so
I spent a bit of time thinking about it and writing up these notes.</p>

<p>Since there are already <a href="http://redis.io/topics/distlock">over 10 independent implementations of Redlock</a> and we don’t know
who is already relying on this algorithm, I thought it would be worth sharing my notes publicly.
I won’t go into other aspects of Redis, some of which have already been critiqued
<a href="https://aphyr.com/tags/Redis">elsewhere</a>.</p>

<p>Before I go into the details of Redlock, let me say that I quite like Redis, and I have successfully
used it in production in the past. I think it’s a good fit in situations where you want to share
some transient, approximate, fast-changing data between servers, and where it’s not a big deal if
you occasionally lose that data for whatever reason. For example, a good use case is maintaining
request counters per IP address (for rate limiting purposes) and sets of distinct IP addresses per
user ID (for abuse detection).</p>

<p>However, Redis has been gradually making inroads into areas of data management where there are
stronger consistency and durability expectations – which worries me, because this is not what Redis
is designed for. Arguably, distributed locking is one of those areas. Let’s examine it in some more
detail.</p>

<h2 id="what-are-you-using-that-lock-for">What are you using that lock for?</h2>

<p>The purpose of a lock is to ensure that among several nodes that might try to do the same piece of
work, only one actually does it (at least only one at a time). That work might be to write some data
to a shared storage system, to perform some computation, to call some external API, or suchlike. At
a high level, there are two reasons why you might want a lock in a distributed application:
<a href="http://research.google.com/archive/chubby.html" title="Mike Burrows. The Chubby lock service for loosely-coupled distributed systems. OSDI 2006">for efficiency or for correctness</a> [2]. To distinguish these cases, you can ask what
would happen if the lock failed:</p>

<ul>
  <li><strong>Efficiency:</strong> Taking a lock saves you from unnecessarily doing the same work twice (e.g. some
expensive computation). If the lock fails and two nodes end up doing the same piece of work, the
result is a minor increase in cost (you end up paying 5 cents more to AWS than you otherwise would
have) or a minor inconvenience (e.g. a user ends up getting the same email notification twice).</li>
  <li><strong>Correctness:</strong> Taking a lock prevents concurrent processes from stepping on each others’ toes
and messing up the state of your system. If the lock fails and two nodes concurrently work on the
same piece of data, the result is a corrupted file, data loss, permanent inconsistency, the wrong
dose of a drug administered to a patient, or some other serious problem.</li>
</ul>

<p>Both are valid cases for wanting a lock, but you need to be very clear about which one of the two
you are dealing with.</p>

<p>I will argue that if you are using locks merely for efficiency purposes, it is unnecessary to incur
the cost and complexity of Redlock, running 5 Redis servers and checking for a majority to acquire
your lock. You are better off just using a single Redis instance, perhaps with asynchronous
replication to a secondary instance in case the primary crashes.</p>

<p>If you use a single Redis instance, of course you will drop some locks if the power suddenly goes
out on your Redis node, or something else goes wrong. But if you’re only using the locks as an
efficiency optimization, and the crashes don’t happen too often, that’s no big deal. This “no big
deal” scenario is where Redis shines. At least if you’re relying on a single Redis instance, it is
clear to everyone who looks at the system that the locks are approximate, and only to be used for
non-critical purposes.</p>

<p>On the other hand, the Redlock algorithm, with its 5 replicas and majority voting, looks at first
glance as though it is suitable for situations in which your locking is important for <em>correctness</em>.
I will argue in the following sections that it is <em>not</em> suitable for that purpose. For the rest of
this article we will assume that your locks are important for correctness, and that it is a serious
bug if two different nodes concurrently believe that they are holding the same lock.</p>

<h2 id="protecting-a-resource-with-a-lock">Protecting a resource with a lock</h2>

<p>Let’s leave the particulars of Redlock aside for a moment, and discuss how a distributed lock is
used in general (independent of the particular locking algorithm used). It’s important to remember
that a lock in a distributed system is not like a mutex in a multi-threaded application. It’s a more
complicated beast, due to the problem that different nodes and the network can all fail
independently in various ways.</p>

<p>For example, say you have an application in which a client needs to update a file in shared storage
(e.g. HDFS or S3). A client first acquires the lock, then reads the file, makes some changes, writes
the modified file back, and finally releases the lock. The lock prevents two clients from performing
this read-modify-write cycle concurrently, which would result in lost updates. The code might look
something like this:</p>

<figure class="highlight"><pre><code class="language-js" data-lang="js"><span></span><span class="c1">// THIS CODE IS BROKEN</span>
<span class="kd">function</span> <span class="nx">writeData</span><span class="p">(</span><span class="nx">filename</span><span class="p">,</span> <span class="nx">data</span><span class="p">)</span> <span class="p">{</span>
    <span class="kd">var</span> <span class="nx">lock</span> <span class="o">=</span> <span class="nx">lockService</span><span class="p">.</span><span class="nx">acquireLock</span><span class="p">(</span><span class="nx">filename</span><span class="p">);</span>
    <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="nx">lock</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">throw</span> <span class="s1">&#39;Failed to acquire lock&#39;</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="k">try</span> <span class="p">{</span>
        <span class="kd">var</span> <span class="nx">file</span> <span class="o">=</span> <span class="nx">storage</span><span class="p">.</span><span class="nx">readFile</span><span class="p">(</span><span class="nx">filename</span><span class="p">);</span>
        <span class="kd">var</span> <span class="nx">updated</span> <span class="o">=</span> <span class="nx">updateContents</span><span class="p">(</span><span class="nx">file</span><span class="p">,</span> <span class="nx">data</span><span class="p">);</span>
        <span class="nx">storage</span><span class="p">.</span><span class="nx">writeFile</span><span class="p">(</span><span class="nx">filename</span><span class="p">,</span> <span class="nx">updated</span><span class="p">);</span>
    <span class="p">}</span> <span class="k">finally</span> <span class="p">{</span>
        <span class="nx">lock</span><span class="p">.</span><span class="nx">release</span><span class="p">();</span>
    <span class="p">}</span>
<span class="p">}</span></code></pre></figure>

<p>Unfortunately, even if you have a perfect lock service, the code above is broken. The following
diagram shows how you can end up with corrupted data:</p>

<p><img src="/2016/02/unsafe-lock.png" width="550" height="200" alt="Unsafe access to a resource protected by a distributed lock" /></p>

<p>In this example, the client that acquired the lock is paused for an extended period of time while
holding the lock – for example because the garbage collector (GC) kicked in. The lock has a timeout
(i.e. it is a lease), which is always a good idea (otherwise a crashed client could end up holding
a lock forever and never releasing it). However, if the GC pause lasts longer than the lease expiry
period, and the client doesn’t realise that it has expired, it may go ahead and make some unsafe
change.</p>

<p>This bug is not theoretical: HBase used to <a href="http://www.slideshare.net/enissoz/hbase-and-hdfs-understanding-filesystem-usage" title="Enis Söztutar. HBase and HDFS: Understanding filesystem usage in HBase. HBaseCon 2013">have this problem</a> [3,4]. Normally,
GC pauses are quite short, but “stop-the-world” GC pauses have sometimes been known to last for
<a href="http://blog.cloudera.com/blog/2011/02/avoiding-full-gcs-in-hbase-with-memstore-local-allocation-buffers-part-1/" title="Todd Lipcon. Avoiding Full GCs in Apache HBase with MemStore-Local Allocation Buffers: Part 1. 2011">several minutes</a> [5] – certainly long enough for a lease to expire. Even so-called
“concurrent” garbage collectors like the HotSpot JVM’s CMS cannot fully run in parallel with the
application code – even they <a href="http://mechanical-sympathy.blogspot.co.uk/2013/07/java-garbage-collection-distilled.html" title="Martin Thompson. Java Garbage Collection Distilled. 2013">need to stop the world</a> from time to time [6].</p>

<p>You cannot fix this problem by inserting a check on the lock expiry just before writing back to
storage. Remember that GC can pause a running thread at <em>any point</em>, including the point that is
maximally inconvenient for you (between the last check and the write operation).</p>

<p>And if you’re feeling smug because your programming language runtime doesn’t have long GC pauses,
there are many other reasons why your process might get paused. Maybe your process tried to read an
address that is not yet loaded into memory, so it gets a page fault and is paused until the page is
loaded from disk. Maybe your disk is actually EBS, and so reading a variable unwittingly turned into
a synchronous network request over Amazon’s congested network. Maybe there are many other processes
contending for CPU, and you hit a <a href="https://twitter.com/aphyr/status/682077908953792512">black node in your scheduler tree</a>. Maybe someone
accidentally sent SIGSTOP to the process. Whatever. Your processes will get paused.</p>

<p>If you still don’t believe me about process pauses, then consider instead that the file-writing
request may get delayed in the network before reaching the storage service. Packet networks such as
Ethernet and IP may delay packets <em>arbitrarily</em>, and <a href="https://queue.acm.org/detail.cfm?id=2655736" title="P Bailis and K Kingsbury. The Network is Reliable. ACM Queue 12(7), 2014.">they do</a> [7]: in a famous
<a href="https://github.com/blog/1364-downtime-last-saturday" title="Mark Imbriaco. Downtime last Saturday. 2012">incident at GitHub</a>, packets were delayed in the network for approximately 90
seconds [8]. This means that an application process may send a write request, and it may reach
the storage server a minute later when the lease has already expired.</p>

<p>Even in well-managed networks, this kind of thing can happen. You simply cannot make any assumptions
about timing, which is why the code above is fundamentally unsafe, no matter what lock service you
use.</p>

<h2 id="making-the-lock-safe-with-fencing">Making the lock safe with fencing</h2>

<p>The fix for this problem is actually pretty simple: you need to include a <em>fencing token</em> with every
write request to the storage service. In this context, a fencing token is simply a number that
increases (e.g. incremented by the lock service) every time a client acquires the lock. This is
illustrated in the following diagram:</p>

<p><img src="/2016/02/fencing-tokens.png" width="550" height="200" alt="Using fencing tokens to make resource access safe" /></p>

<p>Client 1 acquires the lease and gets a token of 33, but then it goes into a long pause and the lease
expires. Client 2 acquires the lease, gets a token of 34 (the number always increases), and then
sends its write to the storage service, including the token of 34. Later, client 1 comes back to
life and sends its write to the storage service, including its token value 33. However, the storage
server remembers that it has already processed a write with a higher token number (34), and so it
rejects the request with token 33.</p>

<p>Note this requires the storage server to take an active role in checking tokens, and rejecting any
writes on which the token has gone backwards. But this is not particularly hard, once you know the
trick. And provided that the lock service generates strictly monotonically increasing tokens, this
makes the lock safe. For example, if you are using ZooKeeper as lock service, you can use the <code>zxid</code>
or the znode version number as fencing token, and you’re in good shape [3].</p>

<p>However, this leads us to the first big problem with Redlock: <em>it does not have any facility for
generating fencing tokens</em>. The algorithm does not produce any number that is guaranteed to increase
every time a client acquires a lock. This means that even if the algorithm were otherwise perfect,
it would not be safe to use, because you cannot prevent the race condition between clients in the
case where one client is paused or its packets are delayed.</p>

<p>And it’s not obvious to me how one would change the Redlock algorithm to start generating fencing
tokens. The unique random value it uses does not provide the required monotonicity. Simply keeping
a counter on one Redis node would not be sufficient, because that node may fail. Keeping counters on
several nodes would mean they would go out of sync. It’s likely that you would need a consensus
algorithm just to generate the fencing tokens. (If only <a href="https://twitter.com/lindsey/status/575006945213485056">incrementing a counter</a> was
simple.)</p>

<h2 id="using-time-to-solve-consensus">Using time to solve consensus</h2>

<p>The fact that Redlock fails to generate fencing tokens should already be sufficient reason not to
use it in situations where correctness depends on the lock. But there are some further problems that
are worth discussing.</p>

<p>In the academic literature, the most practical system model for this kind of algorithm is the
<a href="http://courses.csail.mit.edu/6.852/08/papers/CT96-JACM.pdf" title="TD Chandra and S Toueg. Unreliable Failure Detectors for Reliable Distributed Systems. JACM 43(2):225–267, 1996">asynchronous model with unreliable failure detectors</a> [9]. In plain English,
this means that the algorithms make no assumptions about timing: processes may pause for arbitrary
lengths of time, packets may be arbitrarily delayed in the network, and clocks may be arbitrarily
wrong – and the algorithm is nevertheless expected to do the right thing. Given what we discussed
above, these are very reasonable assumptions.</p>

<p>The only purpose for which algorithms may use clocks is to generate timeouts, to avoid waiting
forever if a node is down. But timeouts do not have to be accurate: just because a request times
out, that doesn’t mean that the other node is definitely down – it could just as well be that there
is a large delay in the network, or that your local clock is wrong. When used as a failure detector,
timeouts are just a guess that something is wrong. (If they could, distributed algorithms would do
without clocks entirely, but then <a href="http://www.cs.princeton.edu/courses/archive/fall07/cos518/papers/flp.pdf" title="MJ Fischer, N Lynch, and MS Paterson. Impossibility of Distributed Consensus with One Faulty Process. JACM 32(2):374–382, 1985">consensus becomes impossible</a> [10]. Acquiring a lock is
like a compare-and-set operation, which <a href="https://cs.brown.edu/~mph/Herlihy91/p124-herlihy.pdf" title="Maurice Herlihy. Wait-Free Synchronization. TOPLAS 13(1):124–149, 1991">requires consensus</a> [11].)</p>

<p>Note that Redis <a href="https://github.com/antirez/redis/blob/edd4d555df57dc84265fdfb4ef59a4678832f6da/src/server.c#L390-L404">uses <code>gettimeofday</code></a>, not a <a href="http://linux.die.net/man/2/clock_gettime">monotonic clock</a>, to
determine the <a href="https://github.com/antirez/redis/blob/f0b168e8944af41c4161249040f01ece227cfc0c/src/db.c#L933-L959">expiry of keys</a>. The man page for <code>gettimeofday</code> <a href="http://linux.die.net/man/2/gettimeofday">explicitly
says</a> that the time it returns is subject to discontinuous jumps in system time –
that is, it might suddenly jump forwards by a few minutes, or even jump back in time (e.g. if the
clock is <a href="https://www.eecis.udel.edu/~mills/ntp/html/clock.html">stepped by NTP</a> because it differs from a NTP server by too much, or if the
clock is manually adjusted by an administrator). Thus, if the system clock is doing weird things, it
could easily happen that the expiry of a key in Redis is much faster or much slower than expected.</p>

<p>For algorithms in the asynchronous model this is not a big problem: these algorithms generally
ensure that their <em>safety</em> properties always hold, <a href="http://www.net.t-labs.tu-berlin.de/~petr/ADC-07/papers/DLS88.pdf" title="C Dwork, N Lynch, and L Stockmeyer. Consensus in the Presence of Partial Synchrony. JACM 35(2):288–323, 1988">without making any timing
assumptions</a> [12]. Only <em>liveness</em> properties depend on timeouts or some other failure
detector. In plain English, this means that even if the timings in the system are all over the place
(processes pausing, networks delaying, clocks jumping forwards and backwards), the performance of an
algorithm might go to hell, but the algorithm will never make an incorrect decision.</p>

<p>However, Redlock is not like this. Its safety depends on a lot of timing assumptions: it assumes
that all Redis nodes hold keys for approximately the right length of time before expiring; that the
network delay is small compared to the expiry duration; and that process pauses are much shorter
than the expiry duration.</p>

<h2 id="breaking-redlock-with-bad-timings">Breaking Redlock with bad timings</h2>

<p>Let’s look at some examples to demonstrate Redlock’s reliance on timing assumptions. Say the system
has five Redis nodes (A, B, C, D and E), and two clients (1 and 2). What happens if a clock on one
of the Redis nodes jumps forward?</p>

<ol>
  <li>Client 1 acquires lock on nodes A, B, C. Due to a network issue, D and E cannot be reached.</li>
  <li>The clock on node C jumps forward, causing the lock to expire.</li>
  <li>Client 2 acquires lock on nodes C, D, E. Due to a network issue, A and B cannot be reached.</li>
  <li>Clients 1 and 2 now both believe they hold the lock.</li>
</ol>

<p>A similar issue could happen if C crashes before persisting the lock to disk, and immediately
restarts. For this reason, the Redlock documentation <a href="http://redis.io/topics/distlock#performance-crash-recovery-and-fsync">recommends delaying restarts</a> of
crashed nodes for at least the time-to-live of the longest-lived lock. But this restart delay again
relies on a reasonably accurate measurement of time, and would fail if the clock jumps.</p>

<p>Okay, so maybe you think that a clock jump is unrealistic, because you’re very confident in having
correctly configured NTP to only ever slew the clock. In that case, let’s look at an example of how
a process pause may cause the algorithm to fail:</p>

<ol>
  <li>Client 1 requests lock on nodes A, B, C, D, E.</li>
  <li>While the responses to client 1 are in flight, client 1 goes into stop-the-world GC.</li>
  <li>Locks expire on all Redis nodes.</li>
  <li>Client 2 acquires lock on nodes A, B, C, D, E.</li>
  <li>Client 1 finishes GC, and receives the responses from Redis nodes indicating that it successfully
acquired the lock (they were held in client 1’s kernel network buffers while the process was
paused).</li>
  <li>Clients 1 and 2 now both believe they hold the lock.</li>
</ol>

<p>Note that even though Redis is written in C, and thus doesn’t have GC, that doesn’t help us here:
any system in which the <em>clients</em> may experience a GC pause has this problem. You can only make this
safe by preventing client 1 from performing any operations under the lock after client 2 has
acquired the lock, for example using the fencing approach above.</p>

<p>A long network delay can produce the same effect as the process pause. It perhaps depends on your
TCP user timeout – if you make the timeout significantly shorter than the Redis TTL, perhaps the
delayed network packets would be ignored, but we’d have to look in detail at the TCP implementation
to be sure. Also, with the timeout we’re back down to accuracy of time measurement again!</p>

<h2 id="the-synchrony-assumptions-of-redlock">The synchrony assumptions of Redlock</h2>

<p>These examples show that Redlock works correctly only if you assume a <em>synchronous</em> system model –
that is, a system with the following properties:</p>

<ul>
  <li>bounded network delay (you can guarantee that packets always arrive within some guaranteed maximum
delay),</li>
  <li>bounded process pauses (in other words, hard real-time constraints, which you typically only
find in car airbag systems and suchlike), and</li>
  <li>bounded clock error (cross your fingers that you don’t get your time from a <a href="http://xenia.media.mit.edu/~nelson/research/ntp-survey99/">bad NTP
server</a>).</li>
</ul>

<p>Note that a synchronous model does not mean exactly synchronised clocks: it means you are assuming
a <a href="http://www.net.t-labs.tu-berlin.de/~petr/ADC-07/papers/DLS88.pdf" title="C Dwork, N Lynch, and L Stockmeyer. Consensus in the Presence of Partial Synchrony. JACM 35(2):288–323, 1988"><em>known, fixed upper bound</em></a> on network delay, pauses and clock drift [12]. Redlock
assumes that delays, pauses and drift are all small relative to the time-to-live of a lock; if the
timing issues become as large as the time-to-live, the algorithm fails.</p>

<p>In a reasonably well-behaved datacenter environment, the timing assumptions will be satisfied <em>most</em>
of the time – this is known as a <a href="http://www.net.t-labs.tu-berlin.de/~petr/ADC-07/papers/DLS88.pdf" title="C Dwork, N Lynch, and L Stockmeyer. Consensus in the Presence of Partial Synchrony. JACM 35(2):288–323, 1988">partially synchronous system</a> [12]. But is that good
enough? As soon as those timing assumptions are broken, Redlock may violate its safety properties,
e.g. granting a lease to one client before another has expired. If you’re depending on your lock for
correctness, “most of the time” is not enough – you need it to <em>always</em> be correct.</p>

<p>There is plenty of evidence that it is not safe to assume a synchronous system model for most
practical system environments [7,8]. Keep reminding yourself of the GitHub incident with the
<a href="https://github.com/blog/1364-downtime-last-saturday" title="Mark Imbriaco. Downtime last Saturday. 2012">90-second packet delay</a>. It is unlikely that Redlock would survive a <a href="https://aphyr.com/tags/jepsen">Jepsen</a> test.</p>

<p>On the other hand, a consensus algorithm designed for a partially synchronous system model (or
asynchronous model with failure detector) actually has a chance of working. Raft, Viewstamped
Replication, Zab and Paxos all fall in this category. Such an algorithm must let go of all timing
assumptions. That’s hard: it’s so tempting to assume networks, processes and clocks are more
reliable than they really are. But in the messy reality of distributed systems, you have to be very
careful with your assumptions.</p>

<h2 id="conclusion">Conclusion</h2>

<p>I think the Redlock algorithm is a poor choice because it is “neither fish nor fowl”: it is
unnecessarily heavyweight and expensive for efficiency-optimization locks, but it is not
sufficiently safe for situations in which correctness depends on the lock.</p>

<p>In particular, the algorithm makes dangerous assumptions about timing and system clocks (essentially
assuming a synchronous system with bounded network delay and bounded execution time for operations),
and it violates safety properties if those assumptions are not met. Moreover, it lacks a facility
for generating fencing tokens (which protect a system against long delays in the network or in
paused processes).</p>

<p>If you need locks only on a best-effort basis (as an efficiency optimization, not for correctness),
I would recommend sticking with the <a href="http://redis.io/commands/set">straightforward single-node locking algorithm</a> for
Redis (conditional set-if-not-exists to obtain a lock, atomic delete-if-value-matches to release
a lock), and documenting very clearly in your code that the locks are only approximate and may
occasionally fail. Don’t bother with setting up a cluster of five Redis nodes.</p>

<p>On the other hand, if you need locks for correctness, please don’t use Redlock. Instead, please use
a proper consensus system such as <a href="https://zookeeper.apache.org/">ZooKeeper</a>, probably via one of the <a href="http://curator.apache.org/curator-recipes/index.html">Curator recipes</a>
that implements a lock. (At the very least, use a <a href="http://www.postgresql.org/">database with reasonable transactional
guarantees</a>.) And please enforce use of fencing tokens on all resource accesses under the
lock.</p>

<p>As I said at the beginning, Redis is an excellent tool if you use it correctly. None of the above
diminishes the usefulness of Redis for its intended purposes. <a href="http://antirez.com/">Salvatore</a> has been very
dedicated to the project for years, and its success is well deserved. But every tool has
limitations, and it is important to know them and to plan accordingly.</p>

<p>If you want to learn more, I explain this topic in greater detail in <a href="http://dataintensive.net/">chapters 8 and 9 of my
book</a>, now available in Early Release from O’Reilly. (The diagrams above are taken from my
book.) For learning how to use ZooKeeper, I recommend <a href="http://shop.oreilly.com/product/0636920028901.do" title="FP Junqueira and B Reed. ZooKeeper: Distributed Process Coordination. O'Reilly, 2013">Junqueira and Reed’s book</a> [3].
For a good introduction to the theory of distributed systems, I recommend <a href="http://www.distributedprogramming.net/" title="C Cachin, R Guerraoui, and L Rodrigues. Introduction to Reliable and Secure Distributed Programming, 2nd ed. Springer, 2011">Cachin, Guerraoui and
Rodrigues’ textbook</a> [13].</p>

<p><em>Thank you to <a href="https://aphyr.com">Kyle Kingsbury</a>, <a href="https://twitter.com/skamille">Camille Fournier</a>, <a href="https://twitter.com/fpjunqueira">Flavio Junqueira</a>, and
<a href="http://antirez.com/">Salvatore Sanfilippo</a> for reviewing a draft of this article. Any errors are mine, of
course.</em></p>

<p><strong>Update 9 Feb 2016:</strong> <a href="http://antirez.com/">Salvatore</a>, the original author of Redlock, has
<a href="http://antirez.com/news/101">posted a rebuttal</a> to this article (see also
<a href="https://news.ycombinator.com/item?id=11065933">HN discussion</a>). He makes some good points, but
I stand by my conclusions. I may elaborate in a follow-up post if I have time, but please form your
own opinions – and please consult the references below, many of which have received rigorous
academic peer review (unlike either of our blog posts).</p>

<h2 id="references">References</h2>

<p>[1] Cary G Gray and David R Cheriton:
“<a href="https://pdfs.semanticscholar.org/a25e/ee836dbd2a5ae680f835309a484c9f39ae4e.pdf" title="Cary G Gray and David R Cheriton. Leases: An Efficient Fault-Tolerant Mechanism for Distributed File Cache Consistency. SOSP 1989">Leases: An Efficient Fault-Tolerant Mechanism for Distributed File Cache Consistency</a>,”
at <em>12th ACM Symposium on Operating Systems Principles</em> (SOSP), December 1989.
<a href="http://dx.doi.org/10.1145/74850.74870">doi:10.1145/74850.74870</a></p>

<p>[2] Mike Burrows:
“<a href="http://research.google.com/archive/chubby.html" title="Mike Burrows. The Chubby lock service for loosely-coupled distributed systems. OSDI 2006">The Chubby lock service for loosely-coupled distributed systems</a>,”
at <em>7th USENIX Symposium on Operating System Design and Implementation</em> (OSDI), November 2006.</p>

<p>[3] Flavio P Junqueira and Benjamin Reed:
<a href="http://shop.oreilly.com/product/0636920028901.do" title="FP Junqueira and B Reed. ZooKeeper: Distributed Process Coordination. O'Reilly, 2013"><em>ZooKeeper: Distributed Process Coordination</em></a>. O’Reilly Media, November 2013.
ISBN: 978-1-4493-6130-3</p>

<p>[4] Enis Söztutar:
“<a href="http://www.slideshare.net/enissoz/hbase-and-hdfs-understanding-filesystem-usage" title="Enis Söztutar. HBase and HDFS: Understanding filesystem usage in HBase. HBaseCon 2013">HBase and HDFS: Understanding filesystem usage in HBase</a>,” at <em>HBaseCon</em>, June 2013.</p>

<p>[5] Todd Lipcon:
“<a href="http://blog.cloudera.com/blog/2011/02/avoiding-full-gcs-in-hbase-with-memstore-local-allocation-buffers-part-1/" title="Todd Lipcon. Avoiding Full GCs in Apache HBase with MemStore-Local Allocation Buffers: Part 1. 2011">Avoiding Full GCs in Apache HBase with MemStore-Local Allocation Buffers: Part 1</a>,”
blog.cloudera.com, 24 February 2011.</p>

<p>[6] Martin Thompson: “<a href="http://mechanical-sympathy.blogspot.co.uk/2013/07/java-garbage-collection-distilled.html" title="Martin Thompson. Java Garbage Collection Distilled. 2013">Java Garbage Collection Distilled</a>,”
mechanical-sympathy.blogspot.co.uk, 16 July 2013.</p>

<p>[7] Peter Bailis and Kyle Kingsbury: “<a href="https://queue.acm.org/detail.cfm?id=2655736" title="P Bailis and K Kingsbury. The Network is Reliable. ACM Queue 12(7), 2014.">The Network is Reliable</a>,”
<em>ACM Queue</em>, volume 12, number 7, July 2014.
<a href="http://dx.doi.org/10.1145/2639988.2639988">doi:10.1145/2639988.2639988</a></p>

<p>[8] Mark Imbriaco: “<a href="https://github.com/blog/1364-downtime-last-saturday" title="Mark Imbriaco. Downtime last Saturday. 2012">Downtime last Saturday</a>,” github.com, 26 December 2012.</p>

<p>[9] Tushar Deepak Chandra and Sam Toueg:
“<a href="http://courses.csail.mit.edu/6.852/08/papers/CT96-JACM.pdf" title="TD Chandra and S Toueg. Unreliable Failure Detectors for Reliable Distributed Systems. JACM 43(2):225–267, 1996">Unreliable Failure Detectors for Reliable Distributed Systems</a>,”
<em>Journal of the ACM</em>, volume 43, number 2, pages 225–267, March 1996.
<a href="http://dx.doi.org/10.1145/226643.226647">doi:10.1145/226643.226647</a></p>

<p>[10] Michael J Fischer, Nancy Lynch, and Michael S Paterson:
“<a href="http://www.cs.princeton.edu/courses/archive/fall07/cos518/papers/flp.pdf" title="MJ Fischer, N Lynch, and MS Paterson. Impossibility of Distributed Consensus with One Faulty Process. JACM 32(2):374–382, 1985">Impossibility of Distributed Consensus with One Faulty Process</a>,”
<em>Journal of the ACM</em>, volume 32, number 2, pages 374–382, April 1985.
<a href="http://dx.doi.org/10.1145/3149.214121">doi:10.1145/3149.214121</a></p>

<p>[11] Maurice P Herlihy: “<a href="https://cs.brown.edu/~mph/Herlihy91/p124-herlihy.pdf" title="Maurice Herlihy. Wait-Free Synchronization. TOPLAS 13(1):124–149, 1991">Wait-Free Synchronization</a>,”
<em>ACM Transactions on Programming Languages and Systems</em>, volume 13, number 1, pages 124–149, January 1991.
<a href="http://dx.doi.org/10.1145/114005.102808">doi:10.1145/114005.102808</a></p>

<p>[12] Cynthia Dwork, Nancy Lynch, and Larry Stockmeyer:
“<a href="http://www.net.t-labs.tu-berlin.de/~petr/ADC-07/papers/DLS88.pdf" title="C Dwork, N Lynch, and L Stockmeyer. Consensus in the Presence of Partial Synchrony. JACM 35(2):288–323, 1988">Consensus in the Presence of Partial Synchrony</a>,”
<em>Journal of the ACM</em>, volume 35, number 2, pages 288–323, April 1988.
<a href="http://dx.doi.org/10.1145/42282.42283">doi:10.1145/42282.42283</a></p>

<p>[13] Christian Cachin, Rachid Guerraoui, and Luís Rodrigues:
<a href="http://www.distributedprogramming.net/" title="C Cachin, R Guerraoui, and L Rodrigues. Introduction to Reliable and Secure Distributed Programming, 2nd ed. Springer, 2011"><em>Introduction to Reliable and Secure Distributed Programming</em></a>,
Second Edition. Springer, February 2011. ISBN: 978-3-642-15259-7,
<a href="http://dx.doi.org/10.1007/978-3-642-15260-3">doi:10.1007/978-3-642-15260-3</a></p>


                ]]></content:encoded>
            </item>
        
            <item>
                <title>My year 2015 in review</title>
                <link>http://martin.kleppmann.com/2015/12/28/year-2015-review.html</link>
                <comments>http://martin.kleppmann.com/2015/12/28/year-2015-review.html#disqus_thread</comments>
                <pubDate>Mon, 28 Dec 2015 00:00:00 +0000</pubDate>
                <dc:creator>Martin Kleppmann</dc:creator>
                
                    <guid isPermaLink="true">http://martin.kleppmann.com/2015/12/28/year-2015-review.html</guid>
                
                <description><![CDATA[ I’ve had a pretty busy and interesting year 2015. Inspired by Caitie McCaffrey and Julia Evans, who recently wrote up reflections on their past year, I will try the same. A lot of my work this year has been to write and speak in public about technical topics, and I... ]]></description>
                <content:encoded><![CDATA[
                    <p>I’ve had a pretty busy and interesting year 2015. Inspired by
<a href="http://caitiem.com/2015/12/26/2015-a-year-in-review/">Caitie McCaffrey</a> and
<a href="http://jvns.ca/blog/2015/12/26/2015-year-in-review/">Julia Evans</a>,
who recently wrote up reflections on their past year, I will try the same.</p>

<p>A lot of my work this year has been to write and speak in public about technical topics, and I am
fortunate to have received a very positive response. Judging from what people have told me, both
online and in person, it seems that my ideas have been useful for people in designing their
software. This is very gratifying: although I find these topics interesting and useful (and hence
I wanted to talk about them), that doesn’t guarantee that other people will find them useful too.</p>

<h2 id="public-speaking">Public speaking</h2>

<p>I gave approximately 25 talks and lectures over the course of the year (I’m losing track of how many
I did exactly). Many of these were at major software engineering and data conferences in the UK, US,
Germany, Belgium, Sweden and Hungary, including
<a href="/2015/09/26/transactions-at-strange-loop.html">Strange Loop</a>,
<a href="/2015/11/04/transactions-at-code-mesh.html">Code Mesh</a>,
<a href="/2015/04/24/logs-for-data-infrastructure-at-craft.html">Craft</a>,
<a href="/2015/10/30/stream-processing-patterns-at-crunch.html">Crunch</a>,
<a href="/2015/06/02/change-capture-at-berlin-buzzwords.html">Berlin Buzzwords</a>,
<a href="/2015/11/13/change-data-capture-at-all-your-base.html">All Your Base</a>,
<a href="/2015/11/06/streams-as-team-interface-at-oredev.html">Øredev</a>,
<a href="/2015/09/30/data-liberation-with-kafka-at-strata.html">Strata New York</a> and
<a href="/2015/05/06/data-agility-at-strata.html">Strata London</a>.  A few were at internal events at various
organisations. See my <a href="/talks.html">talks archive</a> for the list of talks, and my
<a href="https://www.youtube.com/playlist?list=PLeKd45zvjcDHJxge6VtYUAbYnvd_VNQCx">YouTube playlist</a> for
the selection of talk recordings that made it onto YouTube.</p>

<p>Fairly few of those talks were verbatim repeats – I count at least 17 distinct talks over the
course of the year. Some of the talks shared material, of course (I don’t have anywhere near enough
new ideas to produce 17 entirely non-overlapping talks), but I like to think that there was at least
a nugget of something new and interesting in every one. I would get bored if I kept giving the same
talk over and over again, and it’s not much fun to listen to a speaker who’s bored of their own
talk!</p>

<p>Preparing all those talks and travelling to the various conferences has been quite time-consuming
and tiring. Thus, even though it has been a great experience, and I have enjoyed many great
conversations with people at those conferences, I will be reducing my conference activity to a more
sustainable level next year.</p>

<h2 id="book-writing">Book writing</h2>

<p>In 2015, we released chapters 4, 7 and 8 of my work-in-progress book,
<a href="http://dataintensive.net/">Designing Data-Intensive Applications</a>, as early release. This means
that two thirds of the planned 12 chapters are now available. However, work on chapter 9 has been
painfully slow, and I am painfully aware that it has now been over 6 months since we last released
a chapter.</p>

<p>The reason for the delay is twofold: I was too distracted by giving lots of talks, and the topic of
chapter 9 (consistency and consensus) is really hard. I wanted to make sure I really understood the
topic all the way to the core, with no niggling uncertanties or doubts, and so I spent a huge amount
of time researching the topic, reading many papers, and challenging myself to find intuitive and
correct explanations for things.</p>

<p>Frankly, so much nonsense has been written about the topic of distributed consistency that it has
been a bit of a challenge to find good source material. But the good news is that I think I have got
it figured out now, and making good headway with writing it up, so the release of chapter 9 should
not be too far away. And then the following chapters will be much easier, since they will cover more
straightfoward topics.</p>

<p>And by scaling back my speaking activity next year, I hope to get the rest of the book out quickly.</p>

<h2 id="new-job">New job</h2>

<p>After leaving LinkedIn in 2014, I took a 14-month sabbatical/funemployment to focus full-time on
book writing, speaking, blogging and open source. Since October 2015, I am once again employed,
namely as a postdoc on a research project at the
<a href="http://www.cl.cam.ac.uk/">University of Cambridge Computer Laboratory</a>.</p>

<p>The fact that I got a postdoc position may be surprising, since I actually don’t have a PhD.
However, the university was happy to regard my industrial experience as “equivalent to a PhD”,
whatever that means. Thumbs up to them for recognising my degree from the School of Hard Knocks.</p>

<p>So far I’m really enjoying being in academia, and the research project I’m on is super interesting.
I’ll be blogging and talking about it lots more in future, but for now I’ll just say this: my area
of focus is moving away from stream processing, and towards information security. There are big and
important problems to be solved at the intersection of data systems, distributed systems, infosec
and cryptography. Watch this space!</p>

<h2 id="non-book-writing">Non-book writing</h2>

<p>As a way of procrastinating from book-writing, I published three research papers in 2015, in three
different areas of computer science (namely distributed systems, cryptographic protocols, and data
management):</p>

<ul>
  <li>“<a href="http://arxiv.org/abs/1509.05393">A Critique of the CAP Theorem</a>” explains in detail why I think
the CAP theorem is terrible, and why we should all stop quoting it. In this paper I catalogue
various common misunderstandings and ambiguities, prove a more precisely formulated theorem, and
suggest a saner alternative to CAP.</li>
  <li>“<a href="/papers/mrsa-pass15.pdf">Strengthening public key authentication against key theft</a>” (with
<a href="http://cirw.in/">Conrad Irwin</a>) describes a cryptographic authentication protocol that is
somewhat robust against someone stealing your private key. We presented this paper at the
<a href="/2015/12/08/preventing-key-theft-at-passwords15.html">9th International Conference on Passwords</a>.</li>
  <li>“<a href="/papers/kafka-debull15.pdf">Kafka, Samza and the Unix philosophy of distributed data</a>” (with
<a href="https://twitter.com/jaykreps">Jay Kreps</a>) describes the design philosophy of Apache Kafka and
stream processing framework Apache Samza by analogy to Unix pipes. This article is a refined
version of my <a href="/2015/08/05/kafka-samza-unix-philosophy-distributed-data.html">prior blog post</a>
on the topic. It was invited to appear in the December 2015 issue of the
<a href="http://sites.computer.org/debull/bull_issues.html">IEEE Data Engineering Bulletin</a>.</li>
</ul>

<p>I also wrote up transcripts of several of my conference talks as oversized blog posts:</p>

<ul>
  <li><a href="/2015/01/29/stream-processing-event-sourcing-reactive-cep.html">Stream processing, Event sourcing, Reactive, CEP… and making sense of it all</a></li>
  <li><a href="/2015/03/04/turning-the-database-inside-out.html">Turning the database inside-out with Apache Samza</a></li>
  <li><a href="/2015/04/13/real-time-full-text-search-luwak-samza.html">Real-time full-text search with Luwak and Samza</a></li>
  <li><a href="/2015/04/23/bottled-water-real-time-postgresql-kafka.html">Bottled Water: Real-time integration of PostgreSQL and Kafka</a></li>
  <li><a href="/2015/05/27/logs-for-data-infrastructure.html">Using logs to build a solid data infrastructure (or: why dual writes are a bad idea)</a></li>
  <li><a href="/2015/08/05/kafka-samza-unix-philosophy-distributed-data.html">Kafka, Samza, and the Unix philosophy of distributed data</a></li>
</ul>

<p>An edited and updated version of those blog posts is planned to be published as a report by O’Reilly
in 2016. Complete with hand-drawn slides, of course.</p>

<p>Besides those transcripts, I published a few blog posts on topics other than stream processing:</p>

<ul>
  <li><a href="/2015/05/11/please-stop-calling-databases-cp-or-ap.html">Please stop calling databases CP or AP</a>
was a precursor to my <a href="http://arxiv.org/abs/1509.05393">CAP theorem paper</a>. It was a bit
contentious, but I stand by my point that the CP/AP categorisation is more confusing than helpful.</li>
  <li><a href="/2015/10/11/recurse-center-joy-of-learning.html">The Recurse Center and the joy of learning</a>
describes my experience of spending two weeks as a resident at the
<a href="https://www.recurse.com/">Recurse Center</a>, an educational retreat for programmers in NYC.</li>
  <li><a href="/2015/11/10/investigatory-powers-bill.html">The Investigatory Powers Bill would increase cybercrime</a> –
see politics section below. Apologies for the “cyber”, but I decided it was important to speak the
language of my target audience.</li>
</ul>

<h2 id="interviews">Interviews</h2>

<ul>
  <li>Werner Schuster <a href="http://www.infoq.com/interviews/kleppmann-data-infrastructure-logs-crdt">interviewed me for InfoQ</a>
about my <a href="/2015/05/27/logs-for-data-infrastructure.html">talk at Craft</a>, and about the future of
data systems design.</li>
  <li>Jim Brikman interviewed me for his book “<a href="http://www.hello-startup.net/">Hello, Startup</a>”
regarding the pros and cons of startup life.</li>
  <li>PwC Technology Forecast used some quotes from me in their report on “<a href="http://www.pwc.com/us/en/technology-forecast/2015/remapping-database-landscape/immutable-data-stores--rise.html">Remapping the database landscape: 
the rise of immutable data stores</a>”.</li>
  <li>The September 2015 issue of <a href="http://www.linuxjournal.com/">Linux Journal</a> republished some of my
older work on <a href="/2013/05/24/improving-security-of-ssh-private-keys.html">SSH private key encryption</a>.</li>
</ul>

<h2 id="politics">Politics</h2>

<p>The UK government has been pursuing some disastrous information security policies. I have
participated in the political process by adding a technical voice to the debate:</p>

<ul>
  <li>Early in 2015, I warned members of the House of Lords about the draft
<a href="https://en.wikipedia.org/wiki/Draft_Communications_Data_Bill">Communications Data Bill</a>
(“Snoopers’ Charter”), which was eventually withdrawn. However, an even worse proposal called the
<a href="https://en.wikipedia.org/wiki/Draft_Investigatory_Powers_Bill">Investigatory Powers Bill</a>
was introduced in November 2015, about which I contacted my MP.</li>
  <li>I wrote a <a href="/2015/11/10/investigatory-powers-bill.html">blog post</a> explaining why you should be
worried about the Investigatory Powers Bill even if you don’t care about privacy, and you are
happy with government services scanning all your communication. In a nutshell: the bill
mandates backdoors and security holes in software. It is a bad idea to make systems insecure by
design.</li>
  <li>I submitted <a href="http://data.parliament.uk/writtenevidence/committeeevidence.svc/evidencedocument/science-and-technology-committee/investigatory-powers-bill-technology-issues/written/25146.html">written evidence</a>
to the <a href="http://www.parliament.uk/business/committees/committees-a-z/commons-select/science-and-technology-committee/inquiries/parliament-2015/investigatory-powers-bill-technology-issues-inquiry-launch-15-16/publications/">House of Commons Science and Technology Committee</a>,
explaining the problems I see with the draft Investigatory Powers Bill.</li>
  <li>I also submitted similar <a href="http://data.parliament.uk/writtenevidence/committeeevidence.svc/evidencedocument/draft-investigatory-powers-bill-committee/draft-investigatory-powers-bill/written/26275.html">written evidence</a>
to the <a href="http://www.parliament.uk/documents/joint-committees/draft-investigatory-powers-bill/ipb-call-for-evidence.pdf">House of Commons and House of Lords joint committee on the draft
Investigatory Powers Bill</a>.</li>
</ul>

<p>The debate is still ongoing, but I hope the warnings from computer scientists and software
developers will be heard.</p>

<h2 id="open-source">Open Source</h2>

<p>This year I released <a href="https://github.com/confluentinc/bottledwater-pg">Bottled Water</a>, a tool for
Postgres that captures all the data written to a database, and replicates it to Kafka for use in
other systems (e.g. building search indexes or maintaining caches). Lots of people have started
playing with Bottled Water, and I have received several good pull requests and bug reports.</p>

<p>Bottled Water is still a very early-stage project, but I am optimistic about it. I plan to integrate
it with the new <a href="http://www.confluent.io/blog/how-to-build-a-scalable-etl-pipeline-with-kafka-connect">Kafka Connect</a>
framework (pull requests welcome!), which will make it easier to deploy. Similar ideas have also been
popping up for other databases, for example
<a href="https://developer.zendesk.com/blog/introducing-maxwell-a-mysql-to-kafka-binlog-processor">Maxwell for MySQL</a>.</p>

<p>Besides Bottled Water, I have made some small open source contributions to
<a href="http://avro.apache.org/">Apache Avro</a> and a few other projects. My
<a href="https://github.com/ept/warc-hadoop">WARC library</a> is now
<a href="https://issues.apache.org/jira/browse/NUTCH-2102">used in Apache Nutch</a>.
There are probably some more things that I have forgotten.</p>

<h2 id="conclusion">Conclusion</h2>

<p>Another year has passed, and it’s easy to forget everything one has done. So I find it motivating
to write down the major things that have happened, and see that actually it was quite a lot.</p>

<p>If anything, I have spread myself too thin, trying to do too many different things. Certainly my
progress with the book has not been as fast as I (or my editor!) would have liked. I am honoured
that so many conferences want me to speak, but unfortunately I will have to dial down my number of
talks, otherwise I’ll never get the book finished.</p>

<p>Thank you to <a href="http://confluent.io/">Confluent</a>, who sponsored a lot of my work in the last year on
a freelance basis. Thank you also to all the great people I’ve met, who have shared their ideas and
helped me understand things. Happy new year!</p>

<p><em>Update 7 Jan 2016</em>: added link to
<a href="http://data.parliament.uk/writtenevidence/committeeevidence.svc/evidencedocument/draft-investigatory-powers-bill-committee/draft-investigatory-powers-bill/written/26275.html">evidence I submitted to parliament</a>, which has now bene published.</p>

                ]]></content:encoded>
            </item>
        
            <item>
                <title>The Investigatory Powers Bill would increase cybercrime</title>
                <link>http://martin.kleppmann.com/2015/11/10/investigatory-powers-bill.html</link>
                <comments>http://martin.kleppmann.com/2015/11/10/investigatory-powers-bill.html#disqus_thread</comments>
                <pubDate>Tue, 10 Nov 2015 00:00:00 +0000</pubDate>
                <dc:creator>Martin Kleppmann</dc:creator>
                
                    <guid isPermaLink="true">http://martin.kleppmann.com/2015/11/10/investigatory-powers-bill.html</guid>
                
                <description><![CDATA[ As widely reported, the UK government proposed the draft of a new Investigatory Powers Bill last week. There has been much discussion of the bill already, but there are some important questions which I have not yet seen addressed. These open questions raise serious concerns about the effects that the... ]]></description>
                <content:encoded><![CDATA[
                    <p>As widely reported, the UK government proposed the draft of a new <a href="https://www.gov.uk/government/collections/draft-investigatory-powers-bill">Investigatory Powers
Bill</a> last week. There has been much discussion of the bill already, but there are some
important questions which I have not yet seen addressed.</p>

<p>These open questions raise serious concerns about the effects that the proposed law would have on
ordinary citizens. In this article I argue that if the bill is passed in its current form, its
effect will be exactly the opposite of that intended: it would leave citizens <em>more</em> exposed to the
risks of crime and terrorism, and not reduce those risks as intended.</p>

<h2 id="background-of-the-bill">Background of the bill</h2>

<p>The stated purpose of the Investigatory Powers Bill is to help fight crime and terrorism – in other
words, to uphold the rule of law and to keep us safe from aggression. If people break the law or
harm citizens (for example through paedophilia, murder, or acts of terrorism), our law enforcement
services require the means to find the perpetrators and hold them responsible. This is an important
public service: a country with ineffective law enforcement would quickly become dysfunctional.</p>

<p>There is a well-known tension between the ability of law enforcement services to get the information
they require to find and convict criminals, and the basic <a href="http://gilc.org/privacy/survey/intro.html">human right of privacy</a> for
innocent citizens. In order to do their job, the police and the intelligence services must
necessarily intrude on individuals’ privacy to some degree. For us as a society, it is important
that we decide how much privacy we are collectively <a href="http://www.theguardian.com/commentisfree/2015/nov/08/surveillance-bill-snoopers-charter-george-orwell">willing to sacrifice</a> for the
sake of helping law enforcement services do their job.</p>

<p>However, this article is not about the tension between security and privacy – it is only about
security. The Investigatory Powers Bill is supposed to increase our security by giving law
enforcement services the tools they need to catch criminals and terrorists. I will argue that, in
fact, the proposed bill <em>harms our security</em> by making us <em>more vulnerable</em> to attacks by criminals
and terrorists.</p>

<p>People who defend surveillance often say that a loss of privacy is <a href="http://www.theyworkforyou.com/debates/?id=2015-11-04a.969.0#g991.4">a price that many people are
willing to pay</a> for the sake of increased safety. However, what if the surveillance
measures actually make people <em>less</em> safe? Even if you do not care about privacy, you should be
strongly opposed to this bill, because it vastly increases the risks of cybercrime, as explained
below.</p>

<h2 id="encrypted-communication">Encrypted communication</h2>

<p>Encryption ensures that your information can only be read by the correct recipient, and not by any
random bystander. Whenever you send information over the internet, if that information has any
value whatsoever, it should be encrypted – otherwise anyone (e.g. people using the same coffee shop
wifi as you) can trivially steal it. Thus, almost all websites use encryption whenever passwords,
credit card numbers, online banking information, or similar sensitive information is involved. If we
didn’t encrypt this information, fraud and identity theft would be rampant. Encryption is a basic
necessity for the internet.</p>

<p>However, encryption can be applied at different levels. For example, in a chat or telephony app,
there are two options:</p>

<ul>
  <li><strong>Encryption in transit:</strong> In this case, the data is encrypted as it is transmitted between your
device and the service provider (e.g. a mobile network operator or a social network), but the
service provider handles the data in unencrypted form. This is illustrated in Figure 1a. In this
case, the service provider is able to read all of the communication, and you need to trust them to
safeguard your information appropriately.</li>
  <li><strong>End-to-end encryption:</strong> In this case, data is encrypted all the way between you and the person
you’re talking to. The service provider only passes on the messages, but it cannot see what you
are talking about (Figure 1b). There may not even <em>be</em> a service provider, because anybody can
write their own app that communicates over the internet, without requiring a centrally managed
service (Figure 1c).</li>
</ul>

<p>If law enforcement decides to investigate you, and encryption in transit is used, that makes life
very easy for law enforcement: they can serve a warrant to the service provider, and get them to
wiretap your communication without you ever knowing. With end-to-end encryption, surveillance is
still possible, but it’s more expensive: since the service provider cannot access the messages,
a warrant to the service provider is of no use. Instead, law enforcement must go to the people
communicating. For example, they can obtain a court order to seize your phone, or they can point
<a href="http://www.cl.cam.ac.uk/~mgk25/pet2004-fpd.pdf">microphones and antennas</a> at your house to listen to your communications, or they
can infiltrate the suspected gang.</p>

<p><img src="/2015/11/end-to-end.png" width="550" height="357" alt="Figure 1: The difference between encryption in transit and end-to-end encryption." /></p>

<p style="text-align: center;"><em>Figure 1: The difference between encryption in transit and end-to-end encryption.</em></p>

<h2 id="the-value-of-end-to-end-encryption">The value of end-to-end encryption</h2>

<p>Although encryption in transit is widely used, it has serious security problems. For example, the
service provider could be hacked by an adversary, or compromised by an insider, causing sensitive
information can be leaked. A fault in the service provider could cause data to be corrupted. For
these reasons, security experts are pushing towards widespread use of end-to-end encryption, which
reduces the exposure to such attacks.</p>

<p>The goal of end-to-end encryption is <em>not</em> to prevent legitimate access by law enforcement in cases
where it is justified for a criminal investigation. Rather, the goal is to defend systems against
adversaries who want to steal sensitive data or cause systems to malfunction. Such defence is
particularly critical in cases where human life is at stake, such as <a href="http://www.wired.com/2015/01/german-steel-mill-hack-destruction/">industrial control
systems</a>, <a href="http://www.wired.com/2015/07/hackers-remotely-kill-jeep-highway/">internet-connected cars</a>, or <a href="http://www.slideshare.net/MarieGMoe/2015-1021keynotehacklumariemoe">medical data and devices</a>. But
even in other domains, such as trade secrets and financial information in enterprises, in journalism
or in legal professions, it is crucial that sensitive information is adequately protected.</p>

<p>End-to-end encryption helps protect our own information against theft and manipulation by
adversaries – ranging from an individual disgruntled employee, to hostile foreign intelligence
services who may be spying or sabotaging for economic, political or military reasons. As more and
more aspects of the world are <a href="http://www.wsj.com/articles/SB10001424053111903480904576512250915629460">controlled by software</a>, and as increasingly many devices
are connected to the internet, a cyberattack against weakly secured systems could have catastrophic
consequences.  We will need all the defences we can get, and end-to-end encryption is going to be an
indispensable part of our security infrastructure.</p>

<h2 id="exceptional-access-to-encrypted-communication">Exceptional access to encrypted communication</h2>

<p>On the other hand, end-to-end encryption makes life harder for law enforcement services, because
they cannot simply serve a warrant to the service provider in order to obtain the content of the
communication. For this reason, politicians have recently attacked end-to-end encryption; for
example, in a speech in January 2015, <a href="https://embed.theguardian.com/embed/video/uk-news/video/2015/jan/12/david-cameron-spy-agencies-britain-video">PM David Cameron said</a>:</p>

<blockquote>
  <p>In our country, do we want to allow a means of communication between people which, even in
extremis, with a signed warrant from the home secretary personally, that we cannot read? […] The
question remains: are we going to allow a means of communications where it simply is not possible
to do that? My answer to that question is: no, we must not. The first duty of any government is to
keep our country and our people safe.</p>
</blockquote>

<p>The proposed Investigatory Powers Bill is an attempt to cast into law this principle outlined by the
Prime Minister. In particular, it places a duty on communications service providers to allow law
enforcement agencies to intercept communication when served with a warrant – even if the service
provider is outside the UK (§31). It also requires service providers to assist with hacking devices
and removing encryption if compelled by an order from the home secretary (§189).</p>

<p>The bill and related guidance notes from the government do not explain how these rules might be put
into practice, but they have <a href="http://www.telegraph.co.uk/news/uknews/terrorism-in-the-uk/11970391/Internet-firms-to-be-banned-from-offering-out-of-reach-communications-under-new-laws.html">been interpreted</a> as requiring services with end-to-end
encryption to have some kind of “backdoor” by which they <a href="http://www.independent.co.uk/life-style/gadgets-and-tech/news/investigatory-powers-bill-could-allow-government-to-ban-end-to-end-encryption-technology-powering-a6725311.html">can be broken</a>, if required.
Other sources say that the government <a href="http://uk.businessinsider.com/investigatory-powers-bill-wont-ban-end-to-end-encryption-2015-11">does not wish to ban end-to-end encryption</a>,
but in that case it is not clear what they <em>do</em> want, since the Prime Minister has reiterated his
plea that terrorists, paedophiles and criminals must not be allowed a “safe space” online.</p>

<p>Security professionals have no interest in making life easy for terrorists, paedophiles and
criminals. However, <em>any</em> technology can be used for both good and bad purposes. The government has
not explained which technologies would comply with the new rules, and which technologies would
violate them – and the text of the bill itself is very vague and ambiguous.</p>

<p>If the bill requires communication services to have some mechanism of obtaining the content of the
communication in response to a warrant, that means the service must somehow retain the ability to
decrypt the data when required. Provisions for such <em>exceptional access</em> (e.g. <a href="https://www.schneier.com/paper-key-escrow.html">key
escrow</a> or backdoors) are normally avoided in encryption products, because they
introduce serious security problems.</p>

<h2 id="exceptional-access-is-insecure">Exceptional access is insecure</h2>

<p>At first glance, it may seem reasonable to require that all encryption products must include
a provision for data to be decrypted by law enforcement agencies, as long as the decryption order is
protected with sufficient oversight to prevent abuse. However, on closer inspection, it turns out
that this proposal is deeply flawed.</p>

<p>The problem is laid out very clearly in <a href="http://dspace.mit.edu/handle/1721.1/97690">a recent report</a>, written by some of the biggest
names in cryptographic research and security engineering worldwide. To quote from the report:</p>

<blockquote>
  <p>Political and law enforcement leaders in the United States and United Kingdom … propose that
data storage and communications systems must be designed for exceptional access by law enforcement
agencies. These proposals are unworkable in practice, raise enormous legal and ethical questions,
and would undo progress on security at a time when Internet vulnerabilities are causing extreme
economic harm. As computer scientists with extensive security and systems experience, we believe
that law enforcement has failed to account for the risks inherent in exceptional access systems.</p>
</blockquote>

<p>The report lays out in <a href="http://dspace.mit.edu/handle/1721.1/97690">no uncertain terms</a>: there is no known method for securely
providing law enforcement with exceptional access to systems with end-to-end encryption. Any method
that provides exceptional access immediately exposes the system to attacks by malicious parties,
rendering the protection of encryption essentially worthless.</p>

<p>Exceptional access would probably require that government departments have some kind of master keys
that allowed them to decrypt any communication if required. Those master keys would obviously have
to be kept extremely secret: if they were to become public, the entire security infrastructure of
the internet would crumble into dust.</p>

<p>How good are government agencies at keeping secrets? Even just in the last few months, the OPM
failed to protect <a href="http://www.theguardian.com/technology/2015/jul/09/opm-hack-21-million-personal-information-stolen">millions of their own personnel records</a> from hackers, the email
account of <a href="http://motherboard.vice.com/read/hackers-release-alleged-ssn-numbers-stolen-from-cia-directors-aol-account">CIA Director John Brennan was hacked</a>, and the <a href="https://theintercept.com/2015/09/17/tsa-doesnt-really-care-luggage-locks-hacked/">master keys for TSA
locks</a> were accidentally posted on the internet. The US Air Force has been accused of
<a href="http://thebulletin.org/okinawa-missiles-october8826">accidentally broadcasting the launch codes for nuclear missiles</a> over radio in the 1960s.
These incidents do not fill me with confidence that any government would be able to handle
cryptographic master keys securely.</p>

<p>If the law enforcement services can remotely break into the device of a suspect, then sooner or
later criminals will find ways to use the same mechanism to break into devices and steal or destroy
your personal data. They will know your location, and the PIN for your burglar alarm, so they will
have an easy time breaking into your house. There is simply no technical mechanism that will allow
legitimate access by law enforcement, and which is also unbreakable by people who want to do you
harm.</p>

<p>I’ll say it again, to be absolutely clear: <em>any</em> mechanism that can allow law enforcement legitimate
access to data can <em>inevitably</em> be abused by hostile foreign intelligence services, and even
technically sophisticated individuals, to break into systems and gain unauthorised access to the
same data. There is <em>no known method</em> for making this secure. If we add provisions for exceptional
access to encryption products, we are simply shooting ourselves in the foot.</p>

<h2 id="retention-of-browsing-history">Retention of browsing history</h2>

<p>Another provision of the proposed Investigatory Powers Bill is that internet service providers
(ISPs) must retain a record of all the websites you visit (more specifically, all the IP addresses
you connect to) for one year. This appears to be another measure to weaken privacy while
strengthening security – but in fact, it is harmful to <em>both</em> privacy and security.</p>

<p>In order to maintain a record of every website you have visited in the last year, the ISP must store
that information somewhere accessible. Information that is stored somewhere accessible will sooner
or later be stolen by attackers. For example, just a few weeks ago, <a href="http://www.theguardian.com/business/2015/oct/23/talktalk-hacking-crisis-deepens-as-more-details-emerge">records of millions of TalkTalk
customers were stolen</a> due to a <a href="https://tommorris.org/posts/9396">SQL injection</a> (one of the most easily
preventable security issues – the fact that TalkTalk was vulnerable to such a simple bug casts
serious doubt on their competence in elementary software engineering practices). If ISPs are
required to store your browsing history, it is only a matter of time before it is stolen.</p>

<p>And stolen browsing history is a security problem. After the website <a href="http://www.theguardian.com/technology/2015/aug/19/ashley-madison-hackers-release-10gb-database-of-33m-infidelity-site-accounts">Ashley
Madison</a> (which helps married people have an affair) was hacked, millions of its
users found their real name, home address, email address and credit card numbers spewed all over the
internet. It did not take long before <a href="http://www.csoonline.com/article/2980631/data-breach/blackmail-rising-from-ashley-madison-breach.html">blackmailers started using the data</a>, threatening
users that <a href="http://www.zdnet.com/article/in-ashley-madisons-wake-heres-one-mans-story-of-sex-sorrow-and-extortion/">their spouse would be informed</a> unless a ransom was paid. Browsing history
retained by an ISP would carry the same blackmail risk.</p>

<p>The problem is not only extortion of money from the victims of blackmail, but also a security
problem. What if someone succeeds in blackmailing employees of an intelligence agency, or senior
civil servants, or a government official? If the victim fears for their reputation or repercussions
from the release of the sensitive information, the attacker gains power over the victim, which is
worrisome if the victim is in a position of power.</p>

<p>Increasingly, stolen personal information is being used for <a href="https://www.schneier.com/blog/archives/2015/11/the_rise_of_pol.html">politically motivated blackmail and
intimidation</a>. Even if you personally have never done anything embarrassing, and you have
nothing to hide, the fact that other people can be blackmailed is a risk to you if those people have
power over you.</p>

<p>“Give me six lines written by the most honest man in the world, and I will find enough in them to
hang him.” (Origin uncertain, attributed to <a href="https://en.wikiquote.org/wiki/Cardinal_Richelieu">Cardinal Richelieu</a>.) Or, to give a more
modern equivalent: “We kill people based on metadata.” (Former CIA and NSA director <a href="https://www.rt.com/usa/158460-cia-director-metadata-kill-people/">Michael
Hayden</a>.)</p>

<h2 id="we-must-make-systems-more-secure-not-less">We must make systems more secure, not less</h2>

<p>As <a href="https://embed.theguardian.com/embed/video/uk-news/video/2015/jan/12/david-cameron-spy-agencies-britain-video">David Cameron said</a>, “the first duty of any government is to keep our country
and our people safe.” The proposed Investigatory Powers Bill is supposed to make us more safe by
giving great powers to the law enforcement services. However, in this article I have argued that the
bill would in fact make us <em>significantly less safe</em>, by making internet security systems vulnerable
to cyberattacks, and by increasing the risk of blackmail.</p>

<p>The proposed bill, as it stands now, is too vague to allow any serious technical analysis to take
place. With regard to encryption technologies, it fails to specify what is allowed and what is not.
But the Prime Minister’s repeated assertion that we “make sure we do not allow terrorists safe
spaces to communicate with each other” implies a <a href="http://www.theguardian.com/world/2015/nov/10/surveillance-bill-dire-consequences-apple-tim-cook">worrisome weakening of security
technologies</a>.</p>

<p>Nobody wants to give criminals a safe space in which they can operate. However, the technologies
that help protect industrial control systems, cars, medical devices, lawyers, journalists and
businesses against attacks by malicious parties are <em>the same</em> as the technologies behind which
criminals can hide. Any technology can be used for good and bad.</p>

<p>It is not possible to eliminate “safe spaces” for criminals without also eliminating security from
the computer systems that our daily lives depend on. I am worried that the Investigatory Powers Bill
would effectively <a href="http://www.theguardian.com/technology/2015/nov/09/tech-firms-snoopers-charter-end-strong-encryption-britain-ip-bill">mandate computer systems to be insecure</a>, and thus leave our
infrastructure vulnerable to cyberattacks from people who want to do us harm.</p>

<p>According to the <a href="https://www.gov.uk/government/uploads/system/uploads/attachment_data/file/60943/the-cost-of-cyber-crime-full-report.pdf">government’s own report</a>, cybercrime is a Tier One risk to national
security, and already costs the UK £27bn per year. This is only going to get worse if we do not
improve the security of our computer systems. As internet-connected devices are increasingly used
for matters of life and death, the security of those devices becomes paramount, and breaches could
have catastrophic consequences. We need to do everything we can to <em>strengthen</em> the security of
those systems, not to weaken them.</p>

<p>I recognise that as systems become more secure, surveillance becomes more difficult for the
intelligence services. I acknowledge that secure communication systems may allow a terrorist plot or
a crime to succeed which may have been thwarted if surveillance was easy for law enforcement
services. But I argue that this risk is <a href="http://motherboard.vice.com/blog/youll-never-guess-how-many-terrorist-plots-the-nsas-domestic-spy-program-has-foiled">tiny</a> compared to the risk of an
insecure, vulnerable infrastructure in which terrorist cyberattackers could wreak havoc.</p>

<p>Aside from the proposed bill’s disregard for civil liberties, even if we consider only the security
implications of the bill, it is deeply worrisome. As more technical details of the proposal become
clear, we must carefully examine to what extent they leave us less secure than we were before.</p>

<p><em>Thank you to Alastair Beresford and Diana Vasile for reviewing a draft of this article.</em></p>

                ]]></content:encoded>
            </item>
        
            <item>
                <title>The Recurse Center and the joy of learning</title>
                <link>http://martin.kleppmann.com/2015/10/11/recurse-center-joy-of-learning.html</link>
                <comments>http://martin.kleppmann.com/2015/10/11/recurse-center-joy-of-learning.html#disqus_thread</comments>
                <pubDate>Sun, 11 Oct 2015 00:00:00 +0000</pubDate>
                <dc:creator>Martin Kleppmann</dc:creator>
                
                    <guid isPermaLink="true">http://martin.kleppmann.com/2015/10/11/recurse-center-joy-of-learning.html</guid>
                
                <description><![CDATA[ I have just spent an excellent two weeks as a resident at the Recurse Center (RC). As RC is a somewhat unusual institution, and several people have asked me about it, I would like to take this opportunity to reflect on what I have learnt. Recurse Center describes itself as... ]]></description>
                <content:encoded><![CDATA[
                    <p>I have just spent an excellent two weeks as a <a href="https://www.recurse.com/blog/85-more-new-residents-for-2015">resident</a> at the <a href="https://www.recurse.com/">Recurse Center</a> (RC).
As RC is a somewhat unusual institution, and several people have asked me about it, I would like to
take this opportunity to reflect on what I have learnt.</p>

<p>Recurse Center describes itself as an <em>educational retreat for programmers</em>. Anyone can apply, and
if you are accepted, you get to spend three months at the Recurse Center in New York. It is free to
attend. You are part of a <em>batch</em>, a group of people who all start at the same time. A new batch
starts every six weeks, so the first and second half of each batch overlaps with the preceding and
following batch, respectively. A batch consists of about 15 to 30 people.</p>

<p>What do you do during those three months? That is largely up to you. There is no curriculum, no exam
or dissertation. The goal is simply for you to become a better programmer during that time, through
self-directed study and practice. How you go about that goal is your choice, but RC provides support
structures to help you achieve the goal.</p>

<p>The three months give you time to focus, without the distraction of a job or project deadlines. This
is your chance to learn challenging new things: three months is long enough that you can build up
a significant amount of new skills, but short enough that great focus is needed.</p>

<p>At the end of the three months, RC will help you find a job at a good company, if you want one, but
there is no obligation to take a job. Alumni often stay involved with the RC community long after
their three months have come to an end.</p>

<h2 id="the-culture-of-recurse-center">The culture of Recurse Center</h2>

<p>The most striking thing about RC is the care with which the atmosphere and culture is designed.
Everything is geared towards providing a supportive environment in which everyone feels welcome, no
matter who they are or what background they come from.</p>

<p>Most technology companies and events I’ve seen are dominated by straight middle-class white and
Asian men. I count myself towards that group, and I don’t have any problem with white men <em>per se</em>,
but it’s an extremely biased sample of humanity, and such bias is not healthy for a community.
Fortunately, the Recurse Center is different. The people who go to RC are a much more representative
cross-section of humanity (in terms of ethnicity, gender, nationality, sexuality, social background,
etc) than most other venues in technology, which is wonderful and heartwarming.</p>

<p>I attribute this diversity directly to RC’s conscious efforts to create a safe space in which
everyone feels welcome. This includes explicit social rules to avoid bigoted (sexist, racist,
homophobic, transphobic, ageist, ableist, etc) language and ideas, and encouraging everybody to ask
questions and discuss technical topics without fear of being seen as stupid or clueless. Not knowing
something is celebrated as an opportunity to learn, not a fault to be embarrassed about.</p>

<p>In the batches I met at RC, some people had been programming for decades, and some only for a few
months. Some had built websites or mobile apps, some had built machine learning systems, others had
built low-level communication systems for satellites. Some people had non-computing backgrounds such
as oceanography, microbiology, religious studies, linguistics and poetry. But all had an
enthusiastic desire to learn.</p>

<h2 id="everyone-is-a-beginner-at-something">Everyone is a beginner at something</h2>

<p>One thing I had wondered about before coming to RC: if people have such vastly differing amounts of
programming experience, how can people productively help each other? If they are all in the same
room, won’t the beginners get intimidated and overwhelmed, while the experienced software engineers
get bored?</p>

<p>After working with a few members of the batch, I realised why the varying levels of experience are
not a problem for RC: everyone is a beginner at the thing they are exploring at RC. If you are an
experienced software developer, you don’t go to RC in order to keep doing the same things as you did
at your last job, using the same languages to build the same kind of application. No, you probably
join RC because you want to learn something completely different.</p>

<p>Your time at RC is an opportunity to “level up” your craft. If you’re an experienced web developer,
how about learning a functional language like <a href="https://www.haskell.org/">Haskell</a> or <a href="http://www.idris-lang.org/">Idris</a>? If you’re a veteran C++
hacker, maybe you want to learn about formal methods like <a href="https://coq.inria.fr/">Coq</a> or <a href="http://research.microsoft.com/en-us/um/people/lamport/tla/tla.html">TLA+</a>? Even if you’ve
been writing software for 20 years, you’re a complete newbie when you move so far out of your
comfort zone – so you’re actually not that different from someone who is trying to get their first
small piece of Python code to work. You’re a beginner too.</p>

<p>At RC, I learnt to appreciate that each individual person has a rich and interesting background, and
that it’s actually not particularly important what percentage of that background is in programming
activities versus non-programming activities. There is nothing about programming that makes it in
any way superior to, say, oceanography or poetry. All of us are just trying to learn something new,
and occasionally one person can help out another because they have more experience in one particular
area, but in another area those roles may well be reversed.</p>

<h2 id="the-joy-of-learning-for-its-own-sake">The joy of learning for its own sake</h2>

<p>Another thing that struck me about RC was the breadth of different things that people wanted to
learn: from fairly mainstream things like game programming in JavaScript, to obscure experimental
programming languages that I hadn’t even heard of.</p>

<p>Each person has their own reasons for their choice of technology to work on, and no justification is
required. Outside of the Recurse Center, people tend to focus on <em>marketable</em> skills, i.e. learning
things that are likely to help them in a job. Within RC, although getting a job is certainly a goal
for some people, the marketability of a technology does not seem to be a dominant factor in
Recursers’ choice of technology to work on. RC is not a training program for web developers or
mobile developers!</p>

<p>In practical terms, this means that some people will choose to work on things that are unlikely to
directly help them in a job, but which are simply <em>interesting</em> (e.g. creating a new programming
language). I find this non-utilitarian approach wonderful. Nobody will criticize your project for
not being “useful”: what matters is what you learn from doing it.</p>

<p>Recurse Center is not about startups. It is specifically about sharpening your technical skills, not
about building a product. During RC you will probably work on projects, because the best way of
deeply understanding an idea is to apply it to a concrete problem. However, the focus is on your
learning, not on the artifacts that you create as a side-effect of learning.</p>

<p>Only at the Recurse Center have I seen people genuinely giddy, bouncing up and down with excitement,
at the prospect of trying out some new language or technology. I don’t think I ever saw that while
studying computer science at university. This joy of learning something simply because it is
interesting, this intellectual curiosity, this childlike sense of wonder, is something we must
treasure.</p>

<h2 id="levelling-up-for-experienced-programmers">“Levelling up” for experienced programmers</h2>

<p>There is no doubt that the Recurse Center is ideal for beginner programmers: it is an incredibly
welcoming, supportive, encouraging environment where you can learn a lot in a short time. But is it
also worthwhile for experienced software engineers?</p>

<p>For the last year I have taken a “sabbatical” (read: been unemployed), doing only occasional bits of
consulting work, and otherwise focussing on on writing my <a href="http://dataintensive.net/">book</a>, doing <a href="http://arxiv.org/abs/1509.05393">research</a>, and giving
<a href="http://martin.kleppmann.com/talks.html">talks</a> about my work. It has been incredibly rewarding, because I have had the opportunity to
think through and understand topics in much greater depth than I could have done whilst working
a job and meeting project deadlines. I have worked hard during this year, and I feel that I have
grown substantially in my craft.</p>

<p>Now, I understand if such a completely self-organised sabbatical is not for everyone. But when
I look at RC, I see an opportunity for other experienced programmers to have a similar personal
growth experience as I have enjoyed. In the supportive environment of RC it is easier to leave your
comfortable software engineering job and set sail into uncharted waters.</p>

<p>If you want to help <em>create</em> the future of software development, rather than just coming along for
the ride, you will need to bring together ideas from very different areas of computer science, and
even incorporate ideas from outside of computer science. You will need to learn about things that do
not have much commercial value today, but which will make programming better for everyone in ten
years’ time.</p>

<p>It is unlikely that you will have the time to learn those things and take such a long-term view on
the side during your day job – you will need to make dedicated time. A sabbatical is a good option,
but it requires a lot of self-discipline to stay on track. The support structures of RC can help you
take your craft to the next level.</p>

<h2 id="conclusion">Conclusion</h2>

<p>If you’re a programmer (no matter what your level of experience), do take a look at the Recurse
Center <a href="https://www.recurse.com/about">about page</a>, <a href="https://www.recurse.com/faq">FAQ</a> and <a href="https://www.recurse.com/manual">user’s manual</a>. If the ideas there resonate with
you, I can warmly encourage you to apply.</p>

<p>If you’re at a company that hires software engineers, you should totally work with Recurse Center to
recruit Recursers. They are people who are intellectually curious, approach their work with joy, and
treat others with respect. The companies I have spoken to which recruit through RC have been very
happy with the people they hired.</p>

<p>I personally spent only two weeks at RC. I would have loved to stay for longer, but (a) my partner
can’t get three months off her job to stay in New York, and (b) I’ve just started a new dream job,
in which I will continue to have great freedom to explore, learn and teach (more on that in a future
blog post).</p>

<p>The thing that impressed me most about the Recurse Center is how they have succeeded in creating
a respectful, thoughtful, encouraging atmosphere that is welcoming to people from a very diverse set
of backgrounds. The tech industry would be vastly improved if more people would learn from RC’s
example. I, for my part, will try to carry that spirit of RC with me.</p>

                ]]></content:encoded>
            </item>
        
    </channel>
</rss>
