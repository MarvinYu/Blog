<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0"
    xmlns:content="http://purl.org/rss/1.0/modules/content/"
    xmlns:dc="http://purl.org/dc/elements/1.1/"
    xmlns:atom="http://www.w3.org/2005/Atom"
    xmlns:sy="http://purl.org/rss/1.0/modules/syndication/">

    <channel>
        <title>Martin Kleppmann's blog</title>
        <atom:link href="http://martin.kleppmann.com/feed.rss" rel="self" type="application/rss+xml" />
        <link>http://martin.kleppmann.com/</link>
        <description>Entrepreneurship, web technology and the user experience</description>
        <lastBuildDate>Mon, 01 Jun 2015 09:32:27 CEST</lastBuildDate>
        <language>en</language>
        <sy:updatePeriod>hourly</sy:updatePeriod>
        <sy:updateFrequency>1</sy:updateFrequency>

        
        
            <item>
                <title>Please stop calling databases CP or AP</title>
                <link>http://martin.kleppmann.com/2015/05/11/please-stop-calling-databases-cp-or-ap.html</link>
                <comments>http://martin.kleppmann.com/2015/05/11/please-stop-calling-databases-cp-or-ap.html#disqus_thread</comments>
                <pubDate>Mon, 11 May 2015 00:00:00 +0000</pubDate>
                <dc:creator>Martin Kleppmann</dc:creator>
                
                    <guid isPermaLink="true">http://martin.kleppmann.com/2015/05/11/please-stop-calling-databases-cp-or-ap.html</guid>
                
                <description><![CDATA[ In his excellent blog post Notes on Distributed Systems for Young Bloods, Jeff Hodges recommends that you use the CAP theorem to critique systems. A lot of people have taken that advice to heart, describing their systems as “CP” (consistent but not available under network partitions), “AP” (available but not... ]]></description>
                <content:encoded><![CDATA[
                    <p>In his excellent blog post <a href="http://www.somethingsimilar.com/2013/01/14/notes-on-distributed-systems-for-young-bloods/">Notes on Distributed Systems for Young Bloods</a>, Jeff Hodges
recommends that you use the <a href="http://henryr.github.io/cap-faq/">CAP theorem</a> to critique systems. A lot of people have taken
that advice to heart, describing their systems as “CP” (consistent but not available under network
partitions), “AP” (available but not consistent under network partitions), or sometimes “CA”
(meaning “I still haven’t read <a href="http://codahale.com/you-cant-sacrifice-partition-tolerance/">Coda’s post from almost 5 years ago</a>”).</p>

<p>I agree with all of Jeff’s other points, but with regard to the CAP theorem, I must disagree. The
CAP theorem is too simplistic and too widely misunderstood to be of much use for characterizing
systems. Therefore I ask that we retire all references to the CAP theorem, stop talking about the
CAP theorem, and put the poor thing to rest. Instead, we should use more precise terminology to
reason about our trade-offs.</p>

<p>(Yes, I realize the irony of writing a blog post about the very topic that I am asking people to
stop writing about. But at least it gives me a URL that I can give to people when they ask why
I don’t like them talking about the CAP theorem. Also, apologies if this is a bit of a rant, but
at least it’s a rant with lots of literature references.)</p>

<h2 id="cap-uses-very-narrow-definitions">CAP uses very narrow definitions</h2>

<p>If you want to refer to CAP as a <em>theorem</em> (as opposed to a vague hand-wavy concept in your
database’s marketing materials), you have to be precise. Mathematics requires precision. The proof
only holds if you use the words with the same meaning as they are used in <a href="http://webpages.cs.luc.edu/~pld/353/gilbert_lynch_brewer_proof.pdf">the proof</a>.
And the proof uses very particular definitions:</p>

<ul>
  <li>
    <p><em>Consistency</em> in CAP actually means <a href="http://cs.brown.edu/~mph/HerlihyW90/p463-herlihy.pdf">linearizability</a>, which is a very specific (and very
strong) notion of consistency. In particular it has got nothing to do with the C in ACID, even
though that C also stands for “consistency”. I explain the meaning of linearizability below.</p>
  </li>
  <li>
    <p><em>Availability</em> in CAP is defined as “every request received by a non-failing [database] node in
the system must result in a [non-error] response”. It’s not sufficient for <em>some</em> node to be able
to handle the request: <em>any</em> non-failing node needs to be able to handle it. Many so-called
“highly available” (i.e. low downtime) systems actually do not meet this definition of
availability.</p>
  </li>
  <li>
    <p><em>Partition Tolerance</em> (terribly mis-named) basically means that you’re communicating over an
<a href="http://henryr.github.io/cap-faq/">asynchronous network</a> that may delay or drop messages. The internet and all our
datacenters <a href="https://aphyr.com/posts/288-the-network-is-reliable">have this property</a>, so you don’t really have any choice in this
matter.</p>
  </li>
</ul>

<p>Also note that the CAP theorem doesn’t just describe any old system, but a very specific model of
a system:</p>

<ul>
  <li>
    <p>The CAP system model is a single, read-write register – that’s all. For example, the CAP theorem
says nothing about transactions that touch multiple objects: they are simply out of scope of the
theorem, unless you can somehow reduce them down to a single register.</p>
  </li>
  <li>
    <p>The only fault considered by the CAP theorem is a network partition (i.e. nodes remain up, but
the network between some of them is not working). That kind of fault absolutely
<a href="https://aphyr.com/posts/288-the-network-is-reliable">does happen</a>, but it’s not the only kind of thing that can go wrong: nodes can
crash or be rebooted, you can run out of disk space, you can hit a bug in the software, etc. In
building distributed systems, you need to consider a much wider range of trade-offs, and focussing
too much on the CAP theorem leads to ignoring other important issues.</p>
  </li>
  <li>
    <p>Also, the CAP theorem says nothing about latency, which people <a href="http://dbmsmusings.blogspot.co.uk/2010/04/problems-with-cap-and-yahoos-little.html">tend to care about more</a>
than availability. In fact, CAP-available systems are allowed to be arbitrarily slow to respond,
and can still be called “available”. Going out on a limb, I’d guess that your users wouldn’t call
your system “available” if it takes 2 minutes to load a page. </p>
  </li>
</ul>

<p>If your use of words matches the precise definitions of the proof, then the CAP theorem applies to
you. But if you’re using some other notion of consistency or availability, you can’t expect the CAP
theorem to still apply. Of course, that doesn’t mean you can suddenly do impossible things, just by
redefining some words! It just means that you can’t turn to the CAP theorem for guidance, and you
cannot use the CAP theorem to justify your point of view.</p>

<p>If the CAP theorem doesn’t apply, that means you have to think through the trade-offs yourself. You
can reason about consistency and availability using your own definitions of those words, and you’re
welcome to prove your own theorem. But please don’t call it CAP theorem, because that name is
already taken.</p>

<h2 id="linearizability">Linearizability</h2>

<p>In case you’re not familiar with linearizability (i.e. “consistency” in the CAP sense), let me
explain it briefly. The <a href="http://cs.brown.edu/~mph/HerlihyW90/p463-herlihy.pdf">formal definition</a> is not entirely straightforward, but
the key idea, stated informally, is this:</p>

<blockquote>
  <p>If operation B started after operation A successfully completed, then operation B must see the
the system in the same state as it was on completion of operation A, or a newer state.</p>
</blockquote>

<p>To make this more tangible, consider an example of a system that is <em>not</em> linearizable. See the
following diagram (sneak preview from an unreleased chapter of <a href="http://dataintensive.net/">my book</a>):</p>

<p><img src="/2015/05/linearizability.png" width="550" height="391" alt="Illustration of a non-linearizable sequence of events" /></p>

<p>This diagram shows Alice and Bob, who are in the same room, both checking their phones to see the
outcome of the <a href="http://www.bbc.co.uk/sport/0/football/28181689">2014 football world cup final</a>. Just after the final score is announced,
Alice refreshes the page, sees the winner announced, and excitedly tells Bob about it. Bob
incredulously hits <em>reload</em> on his own phone, but his request goes to a database replica that is
lagging, and so his phone shows that the game is still ongoing.</p>

<p>If Alice and Bob had hit reload at the same time, it wouldn’t have been surprising if they had got
two different query results, because they don’t know at exactly what time their respective requests
were processed by the server. However, Bob knows that he hit the reload button (initiated his query)
<em>after</em> he heard Alice exclaim the final score, and therefore he expects his query result to be at
least as recent as Alice’s. The fact that he got a stale query result is a violation of
linearizability.</p>

<p>Knowing that Bob’s request happened strictly after Alice’s request (i.e. that they were not
concurrent) depends on the fact that Bob heard about Alice’s query result through a separate
communication channel (in this case, IRL audio). If Bob hadn’t heard from Alice that the game was
over, he wouldn’t have known that the result of his query was stale.</p>

<p>If you’re building a database, you don’t know what kinds of backchannel your clients may have. Thus,
if you want to provide linearizable semantics (CAP-consistency) in your database, you need to make
it appear as though there is only a single copy of the data, even though there may be copies
(replicas, caches) of the data in multiple places.</p>

<p>This is a fairly expensive guarantee to provide, because it requires a lot of coordination. Even the
CPU in your computer <a href="http://www.cl.cam.ac.uk/~pes20/weakmemory/x86tso-paper.tphols.pdf">doesn’t provide linearizable access to your local RAM</a>! On modern
CPUs, you need to use an explicit <a href="http://mechanical-sympathy.blogspot.co.uk/2011/07/memory-barriersfences.html">memory barrier instruction</a> in order to get
linearizability. And even testing whether a system provides linearizability is <a href="https://github.com/aphyr/knossos">tricky</a>.</p>

<h2 id="cap-availability">CAP-Availability</h2>

<p>Let’s talk briefly about the need to give up either linearizability or availability in the case of
a network partition.</p>

<p>Let’s say you have replicas of your database in two different datacenters. The exact method of
replication doesn’t matter for now – it may be single-leader (master/slave), multi-leader
(master/master) or quorum-based replication (<a href="http://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf">Dynamo-style</a>). The requirement of replication
is that whenever data is written to data in one datacenter, it also has to be written to the replica
in the other datacenter. Assuming that clients only connect to one datacenter, there must be
a network link between the two datacenters over which the replication happens.</p>

<p>Now assume that network link is interrupted – that’s what we mean with a <em>network partition</em>. What
happens?</p>

<p><img src="/2015/05/cap-availability.png" width="550" height="251" alt="A network partition between two datacenters" /></p>

<p>Clearly you can choose one of two things:</p>

<ol>
  <li>
    <p>The application continues to be allowed to write to the database, so it remains fully available
in both datacenters. However, as long as the replication link is interrupted, any changes that
are written in one datacenter will not appear in the other datacenter. This violates
linearizability (in terms of the previous example, Alice could be connected to DC 1 and Bob
could be connected to DC 2).</p>
  </li>
  <li>
    <p>If you don’t want to lose linearizability, you have to make sure you do all your reads and
writes in one datacenter, which you may call the <em>leader</em>. In the other datacenter (which cannot
be up-to-date, due to the failed replication link), the database must stop accepting reads and
writes until the network partition is healed and the database is in sync again. Thus, although
the non-leader database has not failed, it cannot process requests, so it is not CAP-available.</p>
  </li>
</ol>

<p>(And this, by the way, is essentially the proof of the CAP theorem. That’s all there is to it. This
example uses two datacenters, but it applies equally to network problems within a single datacenter.
I just find it easier to think about when I imagine it as two datacenters.)</p>

<p>Note that in our notionally “unavailable” situation in option 2, we’re still happily processing
requests in one of the datacenters. So if a system chooses linearizability (i.e. it is not
CAP-available), that doesn’t necessarily mean that a network partition automatically leads to an
outage of the application. If you can shift all clients to using the leader datacenter, the clients
will in fact see no downtime at all.</p>

<p>Availability in practice <a href="http://blog.thislongrun.com/2015/04/cap-availability-high-availability-and_16.html">does not quite correspond</a> to CAP-availability. Your
application’s availability is probably measured with some SLA (e.g. 99.9% of well-formed requests
must return a successful response within 1 second), but such an SLA can be met both with
CAP-available and CAP-unavailable systems.</p>

<p>In practice, multi-datacenter systems <em>are</em> often designed with asynchronous replication, and thus
non-linearizable. However, the reason for that choice is often the latency of wide-area networks,
not just wanting to tolerate datacenter and network failures.</p>

<h2 id="many-systems-are-neither-linearizable-nor-cap-available">Many systems are neither linearizable nor CAP-available</h2>

<p>Under the CAP theorem’s strict definitions of consistency (linearizability) and availability, how
do systems fare?</p>

<p>For example, take any replicated database with a single leader, which is the standard way of setting
up replication in most relational databases. In this configuration, if a client is partitioned from
the leader, it cannot write to the database. Even though it may be able to read from a follower (a
read-only replica), the fact that it cannot write means any single-leader setup is not
CAP-available. Never mind that such configurations are often marketed as “high availability”.</p>

<p>If single-leader replication is not CAP-available, does that make it “CP”? Wait, not so fast. If you
allow the application to make reads from a follower, and the replication is asynchronous (the
default in most databases), then a follower may be a little behind the leader when you read from it.
In this case, your reads will not be linearizable, i.e. not CAP-consistent.</p>

<p>Moreover, databases with <a href="http://research.microsoft.com/pubs/69541/tr-95-51.pdf">snapshot isolation</a>/MVCC are intentionally non-linearizable,
because enforcing linearizability would reduce the level of concurrency that the database can offer.
For example, <a href="http://drkp.net/papers/ssi-vldb12.pdf">PostgreSQL’s SSI</a> provides <em>serializability</em> but not
<em>linearizability</em>, and <a href="http://www.researchgate.net/publication/220225203_Making_snapshot_isolation_serializable/file/e0b49520567eace81f.pdf">Oracle provides neither</a>. Just because a database is branded “ACID”
doesn’t mean it meets the CAP theorem’s definition of consistency.</p>

<p>So these systems are neither CAP-consistent nor CAP-available. They are neither “CP” nor “AP”, they
are just “P”, whatever that means. (Yes, the “two out of three” formulation <em>does</em> allow you to pick
only one out of three, or even none out of three!)</p>

<p>What about “NoSQL”? Take MongoDB, for example: it has a single leader per shard (or at least it’s
supposed to, if it’s not in split-brain mode), so it’s not CAP-available by the argument above. And
Kyle <a href="https://aphyr.com/posts/322-call-me-maybe-mongodb-stale-reads">recently showed</a> that it allows non-linearizable reads even at the highest
consistency setting, so it’s not CAP-consistent either.</p>

<p>And the <a href="http://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf">Dynamo</a> derivatives like Riak, Cassandra and Voldemort, which are often called “AP” since
they optimize for high availability? It depends on your settings. If you accept a single replica for
reads and writes (R=W=1), they are indeed CAP-available. However, if you require quorum reads and
writes (R+W&gt;N), and you have a network partition, clients on the minority side of the partition
cannot reach a quorum, so quorum operations are not CAP-available (at least temporarily, until the
database sets up additional replicas on the minority side).</p>

<p>You sometimes see people people claiming that quorum reads and writes guarantee linearizability, but
I think it would be unwise to rely on it – subtle combinations of features such as sloppy quorums
and read repair can lead to <a href="http://basho.com/riaks-config-behaviors-part-3/">tricky edge cases</a> in which deleted data is resurrected,
or the number of replicas of a value falls below the original W (violating the quorum condition), or
the number of replica nodes increases above the original N (again violating the quorum condition).
All of these lead to non-linearizable outcomes.</p>

<p>These are not bad systems: people successfully use them in production all the time. However, so far
we haven’t been able to rigorously classify them as “AP” or “CP”, either because it depends on the
particular operation or configuration, or because the system meets neither of the CAP theorem’s
strict definitions of consistency or availability.</p>

<h2 id="case-study-zookeeper">Case study: ZooKeeper</h2>

<p>What about ZooKeeper? It uses a <a href="http://web.stanford.edu/class/cs347/reading/zab.pdf">consensus algorithm</a>, so people generally regard it as a
<a href="http://www.knewton.com/tech/blog/2014/12/eureka-shouldnt-use-zookeeper-service-discovery/">clear-cut case of choosing consistency over availability</a> (i.e. a “CP system”).</p>

<p>However, if you look at the <a href="http://zookeeper.apache.org/doc/r3.4.6/zookeeperProgrammers.html#ch_zkGuarantees">ZooKeeper docs</a>, they make quite clear that ZooKeeper
by default <em>does not</em> provide linearizable reads. Each client is connected to one of the server
nodes, and when you make a read, you see only the data on that node, even if there are more
up-to-date writes on another node. This makes reads much faster than if you had to assemble a quorum
or contact the leader for every read, but it also means that ZooKeeper by default <em>does not</em> meet
the CAP theorem’s definition of consistency.</p>

<p>It is possible to make linearizable reads in ZooKeeper by <a href="http://mail-archives.apache.org/mod_mbox/zookeeper-user/201303.mbox/%3CCAJwFCa0Hoekc14Zy6i0LyLj=eraF8JimqMZadohoKQJNTMtYSg@mail.gmail.com%3E">preceding a read with a <code>sync</code>
command</a>. That isn’t the default though, because it comes with a performance penalty.
People do use <code>sync</code>, but usually not all the time.</p>

<p>What about ZooKeeper availability? Well, ZK requires a <a href="http://www.tcs.hut.fi/Studies/T-79.5001/reports/2012-deSouzaMedeiros.pdf">majority quorum</a> in
order to reach consensus, i.e. in order to process writes. If you have a partition with a majority
of the nodes on one side and a minority on the other, then the majority side continues to function,
but the nodes on the minority side can’t process writes, even though the nodes are up. Thus, writes
in ZK are not CAP-available under a partition (even though the majority side can continue to process
writes).</p>

<p>To add to the fun, ZooKeeper 3.4.0 added a <a href="http://zookeeper.apache.org/doc/r3.4.6/zookeeperAdmin.html#Experimental+Options%2FFeatures">read-only mode</a>, in which nodes on the
minority side of a partition can continue serving read requests – no quorum needed! This read-only
mode <em>is</em> CAP-available. Thus, ZooKeeper by default is neither CAP-consistent (CP) nor CAP-available
(AP) – it’s really just “P”. However, you can optionally make it CP by calling <code>sync</code> if you want,
and for reads (but not for writes) it’s actually AP, if you turn on the right option.</p>

<p>But this is irritating. Calling ZooKeeper “not consistent”, just because it’s not linearizable by
default, really badly misrepresents its features. It actually provides an excellent level of
consistency! It provides <a href="http://web.stanford.edu/class/cs347/reading/zab.pdf">atomic broadcast</a> (which is <a href="http://courses.csail.mit.edu/6.852/08/papers/CT96-JACM.pdf">reducible to consensus</a>)
combined with the session guarantee of <a href="http://www-i2.informatik.rwth-aachen.de/i2/fileadmin/user_upload/documents/Seminar_MCMM11/Causal_memory_1996.pdf">causal consistency</a> – which is <a href="http://arxiv.org/pdf/1302.0309.pdf">stronger</a>
than <a href="http://www.researchgate.net/profile/Douglas_Terry3/publication/3561300_Session_guarantees_for_weakly_consistent_replicated_data/links/02e7e52cdbe60a6cb4000000.pdf">read your writes, monotonic reads</a> and <a href="http://research.microsoft.com/pubs/157411/ConsistencyAndBaseballReport.pdf">consistent prefix reads</a> combined.
The documentation says that it provides <a href="http://research-srv.microsoft.com/en-us/um/people/lamport/pubs/multi.pdf">sequential consistency</a>, but it’s under-selling
itself, because ZooKeeper’s guarantees are in fact much stronger than sequential consistency.</p>

<p>As ZooKeeper demonstrates, it is quite reasonable to have a system that is neither CAP-consistent
nor CAP-available in the presence of partitions, and by default isn’t even linearizable in the
<em>absence</em> of partitions. (I guess that would be PC/EL in <a href="http://dbmsmusings.blogspot.co.uk/2010/04/problems-with-cap-and-yahoos-little.html">Abadi’s PACELC framework</a>, but
I don’t find that any more enlightening than CAP.)</p>

<h2 id="cpap-a-false-dichotomy">CP/AP: a false dichotomy</h2>

<p>The fact that we haven’t been able to classify even one datastore as unambiguously “AP” or “CP”
should be telling us something: those are simply not the right labels to describe systems.</p>

<p>I believe that we should stop putting datastores into the “AP” or “CP” buckets, because:</p>

<ul>
  <li>
    <p>Within one piece of software, you may well have various operations with <a href="http://groups.csail.mit.edu/tds/papers/Gilbert/Brewer2.pdf">different consistency
characteristics</a>.</p>
  </li>
  <li>
    <p>Many systems are neither consistent nor available under the CAP theorem’s definitions. However,
I’ve never heard anyone call their system just “P”, presumably because it looks bad. But it’s not
bad – it may be a perfectly reasonable design, it just doesn’t fit one of the two CP/AP buckets.</p>
  </li>
  <li>
    <p>Even though most software doesn’t neatly fit one of those two buckets, people try to shoehorn
software into one of the two buckets anyway, thereby inevitably changing the meaning of
“consistency” or “availability” to whatever definition suits them. Unfortunately, if the meaning
of the words is changed, the CAP theorem no longer applies, and thus the CP/AP distinction is
rendered completely meaningless.</p>
  </li>
  <li>
    <p>A huge amount of subtlety is lost by putting a system in one of two buckets. There are many
considerations of fault-tolerance, latency, simplicity of programming model, operability, etc.
that feed into the design of a distributed systems. It is simply not possible to encode this
subtlety in one bit of information. For example, even though ZooKeeper has an “AP” read-only mode,
this mode still provides a total ordering of historical writes, which is a vastly stronger
guarantee than the “AP” in a system like Riak or Cassandra – so it’s ridiculous to throw them
into the same bucket.</p>
  </li>
  <li>
    <p>Even Eric Brewer <a href="http://cs609.cs.ua.edu/CAP12.pdf">admits</a> that CAP is misleading and oversimplified. In 2000, it was meant
to start a discussion about trade-offs in distributed data systems, and it did that very well. It
wasn’t intended to be a breakthrough formal result, nor was it meant to be a rigorous
classification scheme for data systems. 15 years later, we now have a much greater range of tools
with different consistency and fault-tolerance models to choose from. CAP has served its purpose,
and now it’s time to move on.</p>
  </li>
</ul>

<h2 id="learning-to-think-for-yourself">Learning to think for yourself</h2>

<p>If CP and AP are unsuitable to describe and critique systems, what should you use instead? I don’t
think there is one right answer. Many people have thought hard about these problems, and proposed
terminology and models to help us understand problems. To learn about those ideas, you’ll have to go
deeper into the literature.</p>

<ul>
  <li>
    <p>A good starting point is Doug Terry’s paper in which he <a href="http://research.microsoft.com/pubs/157411/ConsistencyAndBaseballReport.pdf">explains various different levels of
eventual consistency using Baseball examples</a>. It’s very readable and clear, even if
(like me) you’re not American and have no clue about Baseball.</p>
  </li>
  <li>
    <p>If you’re interested in transaction isolation models (which is not the same as consistency of
distributed replicas, but somewhat related), my little project <a href="http://martin.kleppmann.com/2014/11/25/hermitage-testing-the-i-in-acid.html">Hermitage</a> may be relevant.</p>
  </li>
  <li>
    <p><a href="http://arxiv.org/pdf/1302.0309.pdf"><img src="/2014/11/isolation-levels.png" width="250" height="101" style="float: right; margin: 0 0 1em 1em;" /></a>The connections between
replica consistency, transaction isolation and availability are explored by <a href="http://arxiv.org/pdf/1302.0309.pdf">Peter Bailis et
al.</a> (That paper also explains the meaning of that hierarchy of consistency levels which
Kyle Kingsbury <a href="https://aphyr.com/posts/322-call-me-maybe-mongodb-stale-reads">likes to show</a>.)</p>
  </li>
  <li>
    <p>When you’ve read those, you should be ready to dive deeper into the literature. I’ve scattered
a ton of links to papers throughout this post. Do take a look at them: a number of experts have
already figured out a lot of stuff for you.</p>
  </li>
  <li>
    <p>As a last resort, if you can’t face reading the original papers, I suggest you take a look at
<a href="http://dataintensive.net/">my book</a>, which summarizes the most important ideas in an approachable manner. (See,
I tried <em>very hard</em> not to make this post a sales pitch.)</p>
  </li>
  <li>
    <p>If you want to know more specifically about using ZooKeeper correctly, 
<a href="http://shop.oreilly.com/product/0636920028901.do">Flavio Junqueira and Benjamin Reed’s book</a> is good.</p>
  </li>
</ul>

<p>Whatever way you choose to learn, I encourage you to be curious and patient – this stuff doesn’t
come easy. But it’s rewarding, because you learn to reason about trade-offs, and thus figure out
what kind of architecture works best for your particular application. But whatever you do, please
stop talking about CP and AP, because they just don’t make any sense.</p>

<p><em>Thank you to <a href="https://aphyr.com/">Kyle Kingsbury</a> and <a href="https://twitter.com/skamille">Camille Fournier</a>
for comments on a draft of this post. Any errors or unpalatable opinions are mine, of course.</em></p>

                ]]></content:encoded>
            </item>
        
            <item>
                <title>Bottled Water: Real-time integration of PostgreSQL and Kafka</title>
                <link>http://martin.kleppmann.com/2015/04/23/bottled-water-real-time-postgresql-kafka.html</link>
                <comments>http://martin.kleppmann.com/2015/04/23/bottled-water-real-time-postgresql-kafka.html#disqus_thread</comments>
                <pubDate>Thu, 23 Apr 2015 00:00:00 +0000</pubDate>
                <dc:creator>Martin Kleppmann</dc:creator>
                
                    <guid isPermaLink="true">http://martin.kleppmann.com/2015/04/23/bottled-water-real-time-postgresql-kafka.html</guid>
                
                <description><![CDATA[ This post was originally published on the Confluent blog. Writing to a database is easy, but getting the data out again is surprisingly hard. Of course, if you just want to query the database and get some results, that’s fine. But what if you want a copy of your database... ]]></description>
                <content:encoded><![CDATA[
                    <p><em>This post was originally published
<a href="http://blog.confluent.io/2015/04/23/bottled-water-real-time-integration-of-postgresql-and-kafka/">on the Confluent blog</a>.</em></p>

<p>Writing to a database is easy, but getting the data out again is surprisingly hard.</p>

<p>Of course, if you just want to query the database and get some results, that’s fine. But what if you
want a copy of your database contents in some other system — for example, to make it searchable in
Elasticsearch, or to pre-fill caches so that they’re nice and fast, or to load it into a data
warehouse for analytics, or if you want to migrate to a different database technology?</p>

<p>If your data never changed, it would be easy. You could just take a snapshot of the database (a full
dump, e.g. a backup), copy it over, and load it into the other system. The problem is that the data
in the database is constantly changing, and so the snapshot is already out-of-date by the time
you’ve loaded it. Even if you take a snapshot once a day, you still have one-day-old data in the
downstream system, and on a large database those snapshots and bulk loads can become very expensive.
Not really great.</p>

<p>So what do you do if you want a copy of your data in several different systems?</p>

<p>One option is for your application to do so-called “dual writes”. That is, every time your
application code writes to the database, it also updates/invalidates the appropriate cache entries,
reindexes the data in your search engine, sends it to your analytics system, and so on:</p>

<p><img src="/2015/04/bottledwater-01.png" alt="Application-managed dual writes" width="550" height="412" /></p>

<p>However, as I explain in <a href="http://martin.kleppmann.com/2014/10/28/staying-agile-at-span.html">one of my talks</a>,
the dual-writes approach is really problematic. It suffers from race conditions and reliability
problems. If slightly different data gets written to two different datastores (perhaps due to a bug
or a race condition), the contents of the datastores will gradually drift apart — they will become
more and more inconsistent over time. Recovering from such gradual data corruption is difficult.</p>

<p>If you rebuild a cache or index from a snapshot of a database, that has the advantage that any
inconsistencies get <a href="http://nathanmarz.com/blog/how-to-beat-the-cap-theorem.html">blown away</a> when
you rebuild from a new database dump. However, on a large database, it’s slow and inefficient to
process the entire database dump once a day (or more frequently). How could we make it fast?</p>

<p>Typically, only a small part of the database changes between one snapshot and the next. What if you
could process only a “diff” of what changed in the database since the last snapshot? That would also
be a smaller amount of data, so you could take such diffs more frequently. What if you could take
such a “diff” every minute? Every second? 100 times a second?</p>

<p>When you take it to the extreme, the changes to a database become a stream. Every time someone
writes to the database, that is a message in the stream. If you apply those messages to a database
in exactly the same order as the original database committed them, you end up with an exact copy of
the database. And if you think about it, this is exactly
<a href="http://blog.confluent.io/2015/03/04/turning-the-database-inside-out-with-apache-samza/">how database replication works</a>.</p>

<p>The replication approach to data synchronization works much better than dual writes. First, you
write all your data to one database (which is probably what you’re already doing anyway). Next, you
extract two things from that database:</p>

<ul>
  <li>a <strong>consistent snapshot</strong> at one point in time, and</li>
  <li>a <strong>real-time stream of changes</strong> from that point onwards.</li>
</ul>

<p>You can load the snapshot into the other systems (for example your search indexes or caches), and
then apply the real-time changes on an ongoing basis. If this pipeline is well tuned, you can
probably get a latency of less than a second, so your downstream systems remain very almost
up-to-date. And since the stream of changes provides ordering of writes, race conditions are
<a href="https://martin.kleppmann.com/2015/04/24/logs-for-data-infrastructure-at-craft.html">much less of a problem</a>.</p>

<p>This approach to building systems is sometimes called
<a href="http://en.wikipedia.org/wiki/Change_data_capture">Change Data Capture</a> (CDC), though the tools for
doing it are currently not very good. However, at some companies, CDC has become a key building
block for applications — for example, LinkedIn built <a href="http://www.socc2012.org/s18-das.pdf">Databus</a>
and Facebook built
<a href="https://code.facebook.com/posts/188966771280871/wormhole-pub-sub-system-moving-data-through-space-and-time/">Wormhole</a>
for this purpose.</p>

<p>I am excited about change capture because it allows you to unlock the value in the data you already
have. You can feed the data into a
<a href="http://blog.confluent.io/2015/02/25/stream-data-platform-1/">central hub of data streams</a>, where it
can readily be combined with event streams and data from other databases in real-time. This approach
makes it much easier to experiment with new kinds of analysis or data format, it allows gradual
migration from one system to another with minimal risk, and it is much more robust to data
corruption: if something goes wrong, you can always rebuild a datastore from the snapshot and the
stream.</p>

<p><img src="/2015/04/bottledwater-02.png" alt="Using change capture to drive derived data stores" width="550" height="412" /></p>

<h2 id="getting-the-real-time-stream-of-changes">Getting the real-time stream of changes</h2>

<p>Getting a consistent snapshot of a database is a common feature, because you need it in order to
take backups. But getting a real-time stream of changes has traditionally been an overlooked feature
of databases. Oracle
<a href="http://www.oracle.com/us/products/middleware/data-integration/goldengate/overview/index.html">GoldenGate</a>, the
<a href="https://dev.mysql.com/doc/refman/5.7/en/binary-log.html">MySQL binlog</a>, the
<a href="http://www.manuel-schoebel.com/blog/meteorjs-and-mongodb-replica-set-for-oplog-tailing">MongoDB oplog</a> or the
<a href="http://guide.couchdb.org/draft/notifications.html">CouchDB changes feed</a> do something like
this, but they’re not exactly easy to use correctly. More recently, a few databases such as
<a href="http://rethinkdb.com/blog/realtime-web/">RethinkDB</a> or
<a href="https://www.firebase.com/docs/web/guide/retrieving-data.html">Firebase</a> have oriented themselves
towards real-time change streams.</p>

<p>However, today we will talk about <strong>PostgreSQL</strong>. It’s an old-school database, but it’s good. It is
very stable, has good performance, and is <a href="https://vimeo.com/61044807">surprisingly full-featured</a>.</p>

<p>Until recently, if you wanted to get a stream of changes from Postgres, you had to use triggers.
This is possible (see below), but it is fiddly, requires schema changes and doesn’t perform very
well. However, Postgres 9.4 (released in December 2014) introduced a new feature that changes
everything: <a href="http://www.postgresql.org/docs/9.4/static/logicaldecoding.html">logical decoding</a>
(which I explain in more detail below).</p>

<p>With logical decoding, change data capture for Postgres suddenly becomes much more appealing. So,
when this feature was released, I set out to build a change data capture tool for Postgres that
would take advantage of the new facilities. <a href="http://confluent.io/">Confluent</a> sponsored me to work
on it (thank you Confluent!), and today we are releasing an alpha version of this tool as open
source. It is called <a href="https://github.com/confluentinc/bottledwater-pg">Bottled Water</a>.</p>

<p><img src="/2015/04/bottledwater-03.png" alt="Bottled Water: Data streams freshly bottled at source" width="550" height="412" /></p>

<h2 id="introducing-bottled-water">Introducing Bottled Water</h2>

<p>Logical decoding takes the database’s write-ahead log (WAL), and gives us access to row-level change
events: every time a row in a table is inserted, updated or deleted, that’s an event. Those events
are grouped by transaction, and appear in the order in which they were committed to the database.
Aborted/rolled-back transactions do not appear in the stream. Thus, if you apply the change events
in the same order, you end up with an exact, transactionally consistent copy of the database.</p>

<p>The Postgres logical decoding is well designed: it even creates a consistent snapshot that is
coordinated with the change stream. You can use this snapshot to make a point-in-time copy of the
entire database (without locking — you can continue writing to the database while the copy is being
made), and then use the change stream to get all writes that happened since the snapshot.</p>

<p>Bottled Water uses these features to copy all the data in a database, and encodes it in the
efficient binary <a href="http://avro.apache.org/">Avro format</a>. The encoded data is sent to
<a href="http://kafka.apache.org/">Kafka</a> — each table in the database becomes a Kafka topic, and each row
in the database becomes a message in Kafka.</p>

<p>Once the data is in Kafka, you can easily write a Kafka consumer that does whatever you need: send
it to Elasticsearch, or populate a cache, or process it in a <a href="http://samza.apache.org/">Samza</a> job,
or load it into HDFS with <a href="http://confluent.io/docs/current/camus/docs/intro.html">Camus</a>… the
possibilities are endless.</p>

<h2 id="why-kafka">Why Kafka?</h2>

<p>Kafka is a messaging system, best known for transporting high-volume activity events, such as web
server logs and user click events. In Kafka, such events are typically retained for a certain time
period and then discarded. Is Kafka really a good fit for database change events? We don’t want
database data to be discarded!</p>

<p>In fact, Kafka is a perfect fit — the key is Kafka’s
<a href="http://kafka.apache.org/documentation.html#compaction">log compaction feature</a>, which was designed
precisely for this purpose. If you enable log compaction, there is no time-based expiry of data.
Instead, every message has a key, and Kafka retains the latest message for a given key indefinitely.
Earlier messages for a given key are eventually garbage-collected. This is quite similar to new
values overwriting old values in a key-value store.</p>

<p>Bottled Water identifies the primary key (or
<a href="http://michael.otacoo.com/postgresql-2/postgres-9-4-feature-highlight-replica-identity-logical-replication/">replica identity</a>)
of each table in Postgres, and uses that as the key of the messages sent to Kafka. The value of the
message depends on the kind of event:</p>

<ul>
  <li>For inserts and updates, the message value contains all of the row’s fields, encoded as Avro.</li>
  <li>For deletes, the message value is set to null. This causes Kafka to remove the message during log
compaction, so its disk space is freed up.</li>
</ul>

<p>With log compaction, you don’t need one system to store the snapshot of the entire database and
another system for the real-time messages — they can live perfectly well within the same system.
Bottled Water writes the initial snapshot to Kafka by turning every single row in the database into
a message, keyed by primary key, and sending them all to the Kafka brokers. When the snapshot is
done, every row that is inserted, updated or deleted similarly turns into a message.</p>

<p>If a row frequently gets updated, there will be many messages with the same key (because each update
turns into a message). Fortunately, Kafka’s log compaction will sort this out, and garbage-collect
the old values, so that we don’t waste disk space. On the other hand, if a row never gets updated or
deleted, it just stays unchanged in Kafka forever — it never gets garbage-collected.</p>

<p>Having the full database dump and the real-time stream in the same system is tremendously powerful.
If you want to rebuild a downstream database from scratch, you can start with an empty database,
start consuming the Kafka topic from the beginning, and scan through the whole topic, writing each
message to your database. When you reach the end, you have an up-to-date copy of the entire
database. What’s more, you can continue keeping it up-to-date by simply continuing to consume the
stream. Building alternative views onto your data was never easier!</p>

<p>The idea maintaining a copy of your database in Kafka surprises people who are more familiar with
traditional enterprise messaging and its limitations. Actually, this use case is exactly why Kafka
is built around a
<a href="https://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying">replicated log abstraction</a>:
it makes this kind of large-scale data retention and distribution possible. Downstream systems can
reload and re-process data at will, without impacting the performance of the upstream database that
is serving low-latency queries.</p>

<h2 id="why-avro">Why Avro?</h2>

<p>The data extracted from Postgres could be encoded as JSON, or Protobuf, or Thrift, or any number of
formats. However, I believe Avro is the best choice. Gwen Shapira has written about the
<a href="http://radar.oreilly.com/2014/11/the-problem-of-managing-schemas.html">advantages of Avro</a> for
schema management, and I’ve got a
<a href="http://martin.kleppmann.com/2012/12/05/schema-evolution-in-avro-protocol-buffers-thrift.html">blog post</a>
comparing it to Protobuf and Thrift. The
<a href="http://blog.confluent.io/2015/02/25/stream-data-platform-2/">Confluent stream data platform guide</a>
gives some more reasons why Avro is good for data integration.</p>

<p>Bottled Water inspects the schema of your database tables, and automatically generates an Avro
schema for each table. The schemas are automatically registered with
<a href="http://confluent.io/docs/current/schema-registry/docs/index.html">Confluent’s schema registry</a>,
and the schema version is embedded in the messages sent to Kafka. This means it “just works” with
the stream data platform’s
<a href="http://confluent.io/docs/current/schema-registry/docs/serializer-formatter.html">serializers</a>: you
can work with the data from Postgres as meaningful application objects and rich datatypes, without
writing a lot of tedious parsing code.</p>

<p>The translation of Postgres datatypes into Avro is already fairly comprehensive, covering all the
common datatypes, and providing a lossless and sensibly typed conversion. I intend to extend it to
support all of Postgres’ built-in datatypes (of which there are many!) — it’s some effort, but it’s
worth it, because good schemas for your data are tremendously important.</p>

<p><img src="/2015/04/bottledwater-04.png" alt="Inside the bottle factory" width="550" height="316" /></p>

<h2 id="the-logical-decoding-output-plugin">The logical decoding output plugin</h2>

<p>An interesting property of Postgres’ logical decoding feature is that it does not define a wire
format in which change data is sent over the network to a consumer. Instead, it defines an
<a href="http://www.postgresql.org/docs/9.4/static/logicaldecoding-output-plugin.html">output plugin API</a>,
which receives a function call for every insert, update or delete. Bottled Water uses this API to
read data in the database’s internal format, and serializes it to Avro.</p>

<p>The output plugin must be written in C using the Postgres extension mechanism, and loaded into the
database server as a shared library. This requires superuser privileges and filesystem access on the
database server, so it’s not something to be undertaken lightly. I understand that many a database
administrator will be scared by the prospect of running custom code inside the database server.
Unfortunately, this is the only way logical decoding can currently be used.</p>

<p>Fortunately, you don’t have to install the plugin on your leader database — you can just use
a follower (replica, hot standby) for that purpose. That way you can be sure that the plugin cannot
corrupt your data, crash the main database instance or affect its performance.</p>

<p><img src="/2015/04/bottledwater-05.png" alt="Bottled Water architecture" width="550" height="412" /></p>

<h2 id="the-client-daemon">The client daemon</h2>

<p>Besides the plugin (which runs inside the database server), Bottled Water consists of a client
program which you can run anywhere. It connects to the Postgres server and to the Kafka brokers,
receives the Avro-encoded data from the database, and forwards it to Kafka.</p>

<p>The client is also written in C, because it’s easiest to use the Postgres client libraries that way,
and because some code is shared between the plugin and the client. It’s fairly lightweight and
doesn’t need to write to disk.</p>

<p>What happens if the client crashes, or gets disconnected from either Postgres or Kafka? No problem.
It keeps track of which messages have been published and acknowledged by the Kafka brokers. When the
client restarts after an error, it replays all messages that haven’t been acknowledged. Thus, some
messages could appear twice in Kafka, but no data should be lost.</p>

<h2 id="related-work">Related work</h2>

<p>Various other people are working on similar problems:</p>

<ul>
  <li><a href="https://github.com/xstevens/decoderbufs">Decoderbufs</a> is an experimental Postgres plugin by
<a href="https://twitter.com/xstevens">Xavier Stevens</a> that decodes the change stream into a Protocol
Buffers format. It only provides the logical decoding plugin part of the story — it doesn’t have
the consistent snapshot or client parts (Xavier mentions he has written a client which reads from
Postgres and writes to Kafka, but it’s not open source).</li>
  <li><a href="https://github.com/xstevens/pg_kafka">pg_kafka</a> (also from Xavier) is a Kafka producer client in
a Postgres function, so you could potentially produce to Kafka from a trigger.</li>
  <li><a href="https://wiki.postgresql.org/wiki/PGQ_Tutorial">PGQ</a> is a Postgres-based queue implementation, and
<a href="https://wiki.postgresql.org/wiki/SkyTools">Skytools Londiste</a> (developed at Skype) uses it to
provide trigger-based replication. <a href="https://bucardo.org/wiki/Bucardo">Bucardo</a> is another
trigger-based replicator. I get the impression that trigger-based replication is somewhat of
a hack, requiring schema changes and fiddly configuration, and incurring significant overhead.
Also, none of these projects seems to be endorsed by the PostgreSQL core team, whereas logical
decoding is fully supported.</li>
  <li><a href="http://sqoop.apache.org/">Sqoop</a> recently added support for
<a href="https://issues.apache.org/jira/browse/SQOOP-1852">writing to Kafka</a>. To my knowledge, Sqoop can
only take full snapshots of a database, and not capture an ongoing stream of changes. Also, I’m
unsure about the transactional consistency of its snapshots.</li>
  <li>For those using MySQL, <a href="https://twitter.com/lorax_james">James Cheng</a> has put together a list of
<a href="https://github.com/wushujames/mysql-cdc-projects/wiki">change capture projects</a> that get data
from MySQL into Kafka. AFAIK, they all focus on the binlog parsing piece and don’t do the
consistent snapshot piece.</li>
</ul>

<h2 id="status-of-bottled-water">Status of Bottled Water</h2>

<p>At present, Bottled Water is alpha-quality software. It’s more than a proof of concept — quite a bit
of care has gone into its design and implementation — but it hasn’t yet been tested in any
real-world scenarios. It’s definitely not ready for production use right now, but with some testing
and tweaking it will hopefully become production-ready in future.</p>

<p>We’re releasing it as open source now in the hope of getting feedback from the community. Also,
a few people who heard I was working on this have been bugging me to release it :-)</p>

<p>The <a href="https://github.com/confluentinc/bottledwater-pg/blob/master/README.md">README</a> has more
information on how to get started. Please let us know how you get on! Also, I’ll be talking more
about Bottled Water at
<a href="http://berlinbuzzwords.de/session/change-data-capture-magic-wand-we-forgot">Berlin Buzzwords</a> in
June — hope to see you there.</p>

                ]]></content:encoded>
            </item>
        
            <item>
                <title>Real-time full-text search with Luwak and Samza</title>
                <link>http://martin.kleppmann.com/2015/04/13/real-time-full-text-search-luwak-samza.html</link>
                <comments>http://martin.kleppmann.com/2015/04/13/real-time-full-text-search-luwak-samza.html#disqus_thread</comments>
                <pubDate>Mon, 13 Apr 2015 00:00:00 +0000</pubDate>
                <dc:creator>Martin Kleppmann</dc:creator>
                
                    <guid isPermaLink="true">http://martin.kleppmann.com/2015/04/13/real-time-full-text-search-luwak-samza.html</guid>
                
                <description><![CDATA[ This is an edited transcript of a talk given by Alan Woodward and Martin Kleppmann at FOSDEM 2015. It was originally published on the Confluent blog. Traditionally, search works like this: you have a large corpus of documents, and users write ad-hoc queries to find documents within that corpus. Documents... ]]></description>
                <content:encoded><![CDATA[
                    <p>This is an edited transcript of a
<a href="https://fosdem.org/2015/schedule/event/searching_over_streams_with_luwak_and_apache_samza/">talk</a>
given by <a href="https://twitter.com/romseygeek">Alan Woodward</a> and
<a href="https://martin.kleppmann.com/">Martin Kleppmann</a> at <a href="https://fosdem.org/2015/">FOSDEM 2015</a>.
It was originally published
<a href="http://blog.confluent.io/2015/04/13/real-time-full-text-search-with-luwak-and-samza/">on the Confluent blog</a>.</p>

<p><em>Traditionally, search works like this: you have a large corpus of documents, and users write ad-hoc
queries to find documents within that corpus. Documents may change from time to time, but on the
whole, the corpus is fairly stable.</em></p>

<p><em>However, with fast-changing data, it can be useful to turn this model on its head, and search over
a stream of documents as they appear. For example, companies may want to detect whenever they are
mentioned in a feed of news articles, or a Twitter user may want to see a continuous stream of
tweets for a particular hashtag.</em></p>

<p><em>In this talk, we describe open source tools that enable search on streams:
<a href="https://github.com/flaxsearch/luwak">Luwak</a> is a <a href="http://lucene.apache.org/">Lucene</a>-based library
for running many thousands of queries over a single document, with optimizations that make this
process efficient. <a href="http://samza.apache.org/">Samza</a> is a stream processing framework based on
<a href="http://kafka.apache.org/">Kafka</a>, allowing real-time computations to be distributed across
a cluster of machines. We show how Luwak and Samza can be combined into an efficient and scalable
streaming search engine.</em></p>

<p><img src="/2015/04/streamsearch-01.png" alt="Searching over streams with Luwak &amp; Samza" width="550" height="412" /></p>

<p>In this talk we’re going to discuss some work that we’ve been doing in the area of full-text search
on streams. Perhaps you already know about normal search engines like Elasticsearch and Solr, but as
we’ll see, searching on streams is quite a different problem, with some interesting challenges.</p>

<p>Searching on streams becomes important when you’re dealing with real-time data that is rapidly
changing. We’ll see some examples later of when you might need it.</p>

<p><img src="/2015/04/streamsearch-02.png" alt="What is a stream?" width="550" height="197" /></p>

<p>But first of all, we should define what we mean with a stream. For our purposes, we’ll say that
a stream is an append-only, totally ordered sequence of records (also called events or messages).
For example, a log file is a stream: each record is a line of text with some structure, perhaps some
metadata like a timestamp or severity, perhaps an exception stack trace. Every log record is
appended to the end of the file.</p>

<p><img src="/2015/04/streamsearch-03.png" alt="Appending to a log, and tailing it" width="550" height="243" /></p>

<p>There are a few ways you can read the content of a stream. For example, you can start at the
beginning of the file and read the entire file sequentially. Or you can use <code>tail -f</code> to watch the
file for any new records that are appended, and be notified when new data appears.</p>

<p>We call a process that writes to a stream a <em>“producer”</em>, and a process that reads from the stream
a <em>“consumer”</em>.</p>

<p><img src="/2015/04/streamsearch-04.png" alt="How do you search a stream?" width="550" height="239" /></p>

<p>Now say you’ve got some data in a stream, such as a log file, and you want to do full-text search on
it. How do you go about doing that?</p>

<p><img src="/2015/04/streamsearch-05.png" alt="Put the contents of a log file in an index" width="550" height="412" /></p>

<p>The traditional approach is to load everything into a big search index, perhaps something like
Elasticsearch or Solr. <a href="http://www.elasticsearch.org/overview/elkdownloads/">ELK</a> (Elasticsearch,
Logstash and Kibana) is a currently trendy way of setting this up. That way you have the entire
history of the stream searchable, and people can write any queries they want to search the index.</p>

<p><img src="/2015/04/streamsearch-06.png" alt="Partitioned indexes for different time periods" width="550" height="412" /></p>

<p>But what happens as new records are appended to the stream? You need to add them to an index in
order to make them searchable. For example, you could imagine creating different indexes for
different time periods: one for historical data, one for yesterday, one for the last hour, one for
the last minute…</p>

<p>And this is basically what people mean when they talk about “near-real-time” search: create an index
for documents that appeared very recently, and send any queries to that index as well as the older
historical indexes.</p>

<p>Let’s talk about some examples of this kind of search in practice.</p>

<p><img src="/2015/04/streamsearch-07.png" alt="Example: Twitter search" width="550" height="431" /></p>

<p>Take Twitter, for example. If you type something in the search box, you’ll see a list of tweets that
match your search query, ranked by recency. The index includes
<a href="https://blog.twitter.com/2014/building-a-complete-tweet-index">all public tweets ever written</a>,
broken down my time period, similar to the diagram above.</p>

<p>But if you stay on that page for a while, notice that something happens: a bar appears at the top of
the page, saying that there are new results for your query. What happened here? When you typed your
search query, it seems that Twitter didn’t forget about the query the moment they returned the
results to you. Rather, they must have <em>remembered</em> the query, and <em>continued to search the stream</em>
of tweets for any new matches for your query. When new matches appear in the stream, they send
a notification to your browser.</p>

<p>In this case, the stream we’re searching is Twitter’s so-called <em>firehose</em> of Tweets. I don’t know
how they’ve implemented that. Perhaps they group tweets into batches — say, create an index for 10
seconds worth of tweets, and then run the queries from all open search sessions against that index.
But somehow they are doing full-text search on a stream.</p>

<p><img src="/2015/04/streamsearch-08.png" alt="Example: Google alerts" width="550" height="277" /></p>

<p>Another example is Google Alerts. This is a feature of Google where you can register some search
queries with their system, and they send you an email notification when new web pages matching your
query are published. For example, you might set up an alert for your name or company name, so that
you find out when people write stuff about you.</p>

<p>Google internally has a stream of new web pages being discovered by its crawler, and Google Alerts
allows you to register a query against that stream. Google remembers the query, and runs the query
against every new document that is discovered and added to its index.</p>

<p><img src="/2015/04/streamsearch-09.png" alt="Comparing after-the-fact search and streaming search" width="550" height="412" /></p>

<p>So it seems that we can divide search into two categories:</p>

<ul>
  <li>In one case, you put all the documents in a big index, and people can search that index by writing
ad-hoc queries. We could call that <em>“after-the-fact search”</em>, because it’s searching a repository
of historical documents that we received at some point in the past.</li>
  <li>In the other case, you register the queries in advance, and then the system checks each document
that appears on a stream, to see whether it matches any of the registered queries. This is
<em>streaming search</em>.</li>
</ul>

<p>It often makes sense to combine these two: for example, in the Twitter case, both types of search
are happening. You first get after-the-fact search results from the last 7 days, but while you have
the search page open, your query is also registered for a stream search, so that you can follow the
stream of tweets that match your query.</p>

<p>You might have seen this pattern in
<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-percolate.html">Elasticsearch Percolator</a>,
for example. The streaming search approach we’re describing here is similar to Percolator, but we
think that it will scale better.</p>

<p><img src="/2015/04/streamsearch-10.png" alt="One document, many queries" width="550" height="412" /></p>

<p>So, how do you actually implement streaming search? Well, we saw earlier that for near-real-time
search, you construct a small index for recently added documents. We can take that approach to the
extreme: for each new document that appears on a stream, we create a new index, containing just that
one document. Then we can run through all of the registered queries, test each query against the
one-document index, and output a list of all the queries that match the new document. A downstream
system can then take those matches and notify the owners of the queries.</p>

<p><img src="/2015/04/streamsearch-11.png" alt="Does it scale?" width="550" height="412" /></p>

<p>However, the question is: how efficient is this going to be when you have lots of queries, or very
complex queries? If you only have a few hundred registered queries, you can pretty quickly run
through all of those queries for every new document on the stream — but if you have hundreds of
thousands of queries, it can get quite slow.</p>

<p>Also, if you have very complex queries, even just executing a single query can take a long time.
I (Alan) work with some clients who provide
<a href="http://en.wikipedia.org/wiki/Media_monitoring">media monitoring</a> services (also known as <em>clipping
companies</em>). They collect feeds of newspaper articles and other news from around the world, and
their clients are companies who want to know whenever they are mentioned in the news. The media
monitoring companies construct one big query for each client, and those queries can become really
huge — a query might be hundreds of kilobytes long! They contain a large number of terms, have lots
of nested boolean operators, and lots of exclusions (negated search terms).</p>

<p>To give just one example, in the UK there’s a magazine called “<a href="http://www.which.co.uk/">Which?</a>”.
If you simply search for the term “which”, you match a huge number of documents, since that’s such
a common word in English. They have to construct really complex queries to filter out most of the
noise.</p>

<p>So, if you have a large number of queries, or very complex queries, the streaming search becomes
slow. We need to find ways of optimizing that. Observe this: the fastest query is a query that you
never execute. So, if we can figure out which queries are definitely <em>not</em> going to match a given
document, we can skip those queries entirely, and potentially save ourselves a lot of work.</p>

<p><img src="/2015/04/streamsearch-12.png" alt="Flax Luwak" width="550" height="281" /></p>

<p>Which brings us to <a href="https://github.com/flaxsearch/luwak">Luwak</a>, a library that
<a href="http://www.flax.co.uk/">we (Flax)</a> wrote in order to do efficient streaming search. Luwak is open
source and builds upon Apache Lucene. It works the other way round from a normal search index: with
a normal index, you first add documents to the index, and then you query it. Luwak turns this on its
head: you first register queries with Luwak, and then match documents against them. Luwak tells you
which of the registered queries match the document.</p>

<p>Let’s go a bit into the detail of how Luwak optimizes this matching process.</p>

<p><img src="/2015/04/streamsearch-13.png" alt="Index of queries" width="550" height="412" /></p>

<p>As I said, we want some way of figuring out which queries are definitely <em>not</em> going to match
a document, so that we don’t need to bother executing those queries. In order to do this, we can do
something unusual: we can index the <em>queries</em>! In search engines you normally index documents, but
in this case we’re going to index the queries.</p>

<p>Let’s say we have three registered queries: Q1 is <code>“WHEELS” NEAR “BUS”</code>, Q2 is <code>“WHEELS” NEAR
“CAR”</code>, and Q3 is <code>“WHEELS” OR “BUMPERS”</code>. First observe this: in a conjunction query (that is,
a query like A AND B AND C), <em>all</em> the search terms must appear in the document for the query to
match. (An operator like NEAR is a specialized form of AND that has an additional proximity
restriction.) For example, a document must contain both “wheels” and “bus” in order to match Q1; if
a document doesn’t contain the word “bus”, there’s no chance it can match Q1.</p>

<p>That means, we can arbitrarily pick one term from a conjunction and check whether the document
contains that term. If the document doesn’t contain the term, we can be sure that the document won’t
match the conjunction either.</p>

<p>On the other hand, a disjunction query (with an OR operator, like Q3 for example) matches if <em>any</em>
of the search terms appear in the document. For example, if a document doesn’t contain “wheels”, it
may nevertheless match Q3 if it contains “bumpers”. In this case, we must extract <em>all</em> of the terms
from the disjunction; if any one of those terms appears in the document, we have to test the full
query.</p>

<p>We can now take those terms that we extracted from queries (for example “bus” from Q1, “car” from
Q2, and “bumpers” and “wheels” from Q3), and build an index of those terms. As I said, this is an
index of <em>queries</em>, not of documents. The index is a dictionary from terms to queries: it maps terms
to queries containing that term.</p>

<p><img src="/2015/04/streamsearch-14.png" alt="Document disjunction" width="550" height="412" /></p>

<p>Now that we’ve taken all our registered queries and indexed them, we can move on to the next step:
processing the stream of documents. For each document, we want to find all the queries that match.
How do we go about doing this?</p>

<p>The trick is to take each document, and turn it into a <em>query</em>. (Previously we created an index of
queries. Now we’re turning a document into a query. How upside-down!) Namely, we take all the words
(terms) that appear in the document, and construct a disjunction (OR) query from all of those words.
Intuitively, this is saying: “find me all the queries that match any of the words in this document”.
 Creating an inverted index from a single document automatically gives us this list of terms.</p>

<p><img src="/2015/04/streamsearch-15.png" alt="Selecting candidate queries" width="550" height="412" /></p>

<p>Now that we have created an index of queries, and turned a document into a query, we can figure out
which queries match the document. First, we run the document disjunction query against the index of
queries. This will tell us which queries <em>may</em> match the document.</p>

<p>In our example, we created the query index by extracting the term “bus” from Q1, the term “car” from
Q2, and the terms “bumpers” and “wheels” from Q3. Also, we turned the document “The wheels on the
bus go round and round” into the disjunction query:</p>

<pre><code>“and” OR “bus” OR “go” OR “on” OR “round” OR “the” OR “wheels”
</code></pre>

<p>Running that disjunction query against the query index, we get a hit on the terms “bus” (Q1) and
“wheels” (Q3), but the terms “bumpers” (Q3) and “car” (Q2) don’t appear in the document. Therefore
we can conclude that Q1 and Q3 <em>might</em> match the document, but Q2 definitely <em>doesn’t</em> match the
document.</p>

<p>The next step is then to run queries Q1 and Q3 against the document index, to see whether they
really do match. But we don’t need to run Q2, because we’ve already established that it definitely
doesn’t match.</p>

<p>This whole process of indexing queries may seem a bit complicated, but it is a really powerful
optimization if you have a large number of queries. It can cut out 99% of the queries you would
otherwise have to execute, and thus massively speed up searching on streams. As I said, the fastest
query is one that you never even execute.</p>

<p><img src="/2015/04/streamsearch-16.png" alt="Query decomposition" width="550" height="412" /></p>

<p>Besides indexing queries, there are other optimizations we can make. One particular optimization
that we’ve found useful: if you have a big query that contains an OR operator at the top level, you
can break that big query into smaller ones. That is especially useful if one of the subqueries is
simple (fast to execute), and another one is complex (slow to execute).</p>

<p>Say we have a document that will match on the simple subquery, but not on the complex one.  In the
normal case, the whole query is run against the document, so we still pay the price for executing
the complex subquery, even though it doesn’t match. If we decompose it into its constituent parts,
however, then only the simple subquery will be selected, and we can avoid the performance hit of
running the complex one.</p>

<p><img src="/2015/04/streamsearch-17.png" alt="Term frequency analysis and phrase query analysis" width="550" height="260" /></p>

<p>We said earlier that when you’re indexing the queries, you can make some arbitrary choices about
which terms to extract. For example, for the query <em>“car” AND “bumpers”</em>, you could choose either
“car” or “bumpers” as the term to use in the query index. Which one should you choose?</p>

<p>It’s helpful to know how often each term occurs in your documents. For example, perhaps “car” is
quite a common term, but “bumpers” is much more rare. In that case, it would be better to use
“bumpers” in the query index, because it’s less likely to appear in documents. Only the small number
of documents containing the term “bumpers” would then need to be matched against the query <em>“car”
AND “bumpers”</em>, and you save yourself the effort of executing the query for the large number of
documents that contain “car” but not “bumpers”.</p>

<p><img src="/2015/04/streamsearch-18.png" alt="Query term extraction" width="550" height="412" /></p>

<p>Another, more advanced optimization technique considers several different possibilities of
extracting terms from queries. Take the query tree above, containing five terms and four boolean
operators. Remember the rule for extracting terms that we established earlier: extract any one of
the children of a conjunction (AND), but extract all of the children of a disjunction (OR). This
means there are three different combinations of terms that you could extract from the above query
tree:</p>

<p><img src="/2015/04/streamsearch-19.png" alt="Three ways of extracting terms from a query" width="550" height="107" /></p>

<p>Say you have a document “term1 term2 term3”: this document does not match the query (because neither
of the required term4 or term5 appears). However, in the first two combinations above (term1 and
term2 extracted, or term1 and term3 extracted), the document would nevertheless be selected to be
matched against the query. In the third combination above (term4 and term5 extracted), the document
wouldn’t be selected, because we can tell from the query index that it is definitely not going to
match the query.</p>

<p>Can we make the query pre-selection more precise? Yes, we can! Rather than just extracting one
(arbitrary) set of terms from the query, we can extract <em>several</em> sets of terms from the same query,
like the three above, and index them into separate fields (let’s call them _a, _b and _c). You then
run your document disjunction against <em>all</em> those fields, and the set of queries you need to run is
the intersection of those results — a conjunction of disjunctions, if you like.</p>

<p>The document “term1 term2 term3”, which we previously turned into a simple disjunction of terms, now
turns into something like this:</p>

<pre><code>_a:(term1 OR term2 OR term3) AND
_b:(term1 OR term2 OR term3) AND
_c:(term1 OR term2 OR term3)
</code></pre>

<p>The first two terms of the conjunction match, but the third doesn’t, and so we don’t select this
query. It’s still an approximation — you still need to execute the full query to be sure whether it
matches or not — but with these optimizations you can further reduce the number of queries you need
to execute.</p>

<p>Fortunately, Luwak has implemented all of these optimizations already, and they’re available for you
to use today.</p>

<p>However, as described so far, Luwak runs on a single machine. At some point, you may have so many
queries or such high throughput of documents that a single machine is not enough, even after you
have applied all of these optimizations. Which brings us to the second half of this talk: scaling
stream search across a cluster of machines.</p>

<p><em>(At this point, <a href="http://martin.kleppmann.com/">Martin</a> took over from
<a href="https://twitter.com/romseygeek">Alan</a>)</em></p>

<p><img src="/2015/04/streamsearch-20.png" alt="Kafka and Samza" width="550" height="412" /></p>

<p>Rather than inventing our own distributed computing framework — which would be likely to go wrong,
because distributed systems are hard — we’re going to build on a robust foundation. We’re going to
use <a href="http://kafka.apache.org/">Apache Kafka</a> and <a href="http://samza.apache.org/">Apache Samza</a>, two open
source projects that originated at LinkedIn.</p>

<p>I’ll start by giving a bit of background about Kafka, and then talk about how we embedded Luwak
inside Samza in order to scale out search on streams.</p>

<p><img src="/2015/04/streamsearch-21.png" alt="A partitioned stream in Kafka" width="550" height="412" /></p>

<p>Kafka is a kind of <em>message broker</em> or <em>message queue</em> — that is, it takes messages that originate
in one process (a <em>producer</em>), and delivers them to another process (a <em>consumer</em>). It does so in
a scalable, fault-tolerant manner.</p>

<p>The way it works is simple but remarkably effective. You can imagine Kafka as one big, append-only
file. Whenever a producer wants to send a new message to a stream, it simply appends it to the end
of the file. That’s the only way how you can write to Kafka: by appending to the end of a file.</p>

<p>A consumer is like a <code>tail -f</code> on that file, just like what we described at the beginning of this
talk. Each consumer reads the messages in the file sequentially, and each consumer has a current
<em>position</em> or <em>offset</em> in this file. Thus, it knows that all messages before that position have
already been read, and all messages after that position have not yet been read. This makes Kafka
very efficient: the brokers don’t have to keep track of which consumer has seen which messages,
because the consumers themselves just need to store their current position.</p>

<p>In order to scale across multiple machines, a Kafka stream is also <em>partitioned</em>. That means,
there’s not just one append-only file, but several. Each partition is completely independent from
the others, so different partitions can live on different machines.</p>

<p><img src="/2015/04/streamsearch-22.png" alt="Replication over Kafka broker nodes" width="550" height="412" /></p>

<p>In addition, Kafka provides replication, i.e. maintaining a copy of the same data on multiple
machines. This is important for fault tolerance — so that if one machine dies, you don’t lose any
data, and everything keeps running.</p>

<p>Kafka does this using a leader/follower model. Each partition of a stream has a leader on one broker
node, and a configurable number of followers on other broker nodes. All the new messages for
a partition go to its leader, and Kafka replicates them from the leader to the followers. If
a broker node goes down, and it was the leader for some partition, then one of the followers for
that partition becomes the new leader.</p>

<p>Every message in Kafka has a <em>key</em> and a <em>value</em> (which can be arbitrary byte strings). The key is
used for two different things: firstly, it determines which partition the message should go to (we
make sure that all the messages with the same key go to the same partition, by choosing the
partition based on a hash of the key). Secondly, it is used for <em>compaction</em>.</p>

<p><img src="/2015/04/streamsearch-23.png" alt="Kafka changelog compaction" width="550" height="412" /></p>

<p>Compaction is an exception to Kafka’s otherwise strict append-only model. You don’t have to use
compaction, but if you do turn it on, then Kafka keeps track of the keys in the stream. And if there
are several messages with the same key, then Kafka is allowed to throw away older messages with that
key — only the newest message for a given key is guaranteed to be retained.</p>

<p>In the picture above, there are originally three messages with key A, and compaction discards two of
them. In effect, this means that later messages can “overwrite” earlier messages with the same key.
This overwriting doesn’t happen immediately, but at some later time: compaction is done in
a background thread, a bit like garbage collection.</p>

<p>A Kafka stream with compaction is thus similar to a database with a key-value data model. If a key
is never overwritten, it is never discarded, so it stays in the stream forever. With compaction, we
can thus keep a complete history of key-value pairs:</p>

<p><img src="/2015/04/streamsearch-24.png" alt="Keeping complete history of events in Kafka" width="550" height="412" /></p>

<p>Without compaction, the stream would keep growing forever (the size of the stream is proportional to
the number of messages ever sent). But with compaction, the size of the stream is proportional to
the number of distinct keys — just like a database. If you can imagine storing all the keys and
values in a database, you can equally well store all the keys and values in a Kafka stream.</p>

<p>Why is this useful? Well, if we want to use Luwak in a reliable manner, there is a problem we need
to solve: when you register a query with Luwak, it is only kept in memory. Thus, whenever the Luwak
process is restarted, it needs to reload its list of queries from stable storage into memory.</p>

<p>You could use a database for this, but using a Kafka stream has a big advantage: Luwak can consume
the stream of queries, so it gets notified whenever someone registers a new query, modifies
a registered query, or unregisters (deletes) a query. We simply use the query ID as the message key,
and the query string as the message value (or a null value when a query is unregistered). And stream
compaction ensures that the query stream doesn’t get too big.</p>

<p><img src="/2015/04/streamsearch-25.png" alt="Re-processing historical events from stream" width="550" height="412" /></p>

<p>Now, whenever Luwak starts up, it can jump to the very beginning of the queries stream, and consume
it from beginning to end. All queries in that stream are loaded into memory, so that Luwak knows
what queries it should apply to documents. Only once it has finished consuming the stream, Luwaks
starts processing documents and matching them against queries. We call the query stream a
<a href="http://samza.apache.org/learn/documentation/0.8/container/streams.html#bootstrapping">bootstrap stream</a>,
because it’s used to bootstrap (initialize) the in-memory state of the stream consumer.</p>

<p>This brings us to <a href="http://samza.apache.org/">Samza</a>, a framework for writing stream processing jobs
on top of Kafka. A basic Samza job is very simple: you write a bit of code (implementing a Java
interface called <a href="http://samza.apache.org/learn/documentation/0.8/api/overview.html">StreamTask</a>)
and tell it which stream you want to consume, and Samza calls the process() method on your code for
every message that is consumed from the input stream. The code can do whatever it wants, including
sending messages to an output stream.</p>

<p><img src="/2015/04/streamsearch-26.png" alt="Samza takes a Kafka stream an input, produces another as output" width="550" height="412" /></p>

<p>As the input stream from Kafka is split into partitions, Samza creates a separate StreamTask for
each partition, and each StreamTask processes the messages in its corresponding partition
sequentially. Although a StreamTask only processes input from one partition, it can send output to
any partition of its output streams.</p>

<p>This partitioning model allows a Samza job to have two or more input streams, and “join” them
together:</p>

<p><img src="/2015/04/streamsearch-27.png" alt="Joining streams in Samza" width="550" height="412" /></p>

<p>By default, if a job has two input streams (say A and B), Samza sends partition 1 of stream A and
partition 1 of stream B to the same StreamTask 1; it sends partition 2 of A and partition 2 of B to
the same StreamTask 2; and so on. This is illustrated in the picture above. Note this only really
works if both input streams have the same number of partitions.</p>

<p>This allows the stream join to scale, by partitioning both input streams in the same way. For
example, if both streams are partitioned by user ID (i.e. using the user ID as the Kafka message
key), then you can be sure that all the messages for a particular user ID on both input streams are
routed to the same StreamTask. That StreamTask can then keep whatever state it needs in order to
join the streams together.</p>

<p>How do we bring full-text search on streams into this processing model?</p>

<p><img src="/2015/04/streamsearch-28.png" alt="Integration of Luwak with Samza" width="550" height="412" /></p>

<p>Alan and I hacked together a proof-of-concept called
<a href="https://github.com/romseygeek/samza-luwak">Samza-Luwak</a> to test using Luwak inside a Samza job. It
works as follows:</p>

<p>There are two input streams (Kafka topics): one for queries, and one for documents. The query stream
is a bootstrap stream with compaction, as described above. Whenever a user wants to register, modify
or unregister a query, they send a message to the queries stream. The Samza job consumes this
stream, and whenever a message appears, it updates Luwak’s in-memory roster of queries.</p>

<p>The documents stream contains the things that should be matched by the queries (the tweets if you’re
building Twitter search, the web pages from the crawler if you’re building Google Alerts, etc). The
Samza job also consumes the documents stream, and whenever a document appears, it is matched against
the index of registered queries, as described previously. It then sends a message to an output
stream, indicating which queries matched.</p>

<p>How do we distribute this matching process across multiple machines? The problem is that Samza’s
default partitioning model actually doesn’t do what we need. As I said previously, Samza by default
sends partition 1 of all the input streams to task 1, partition 2 of all the input streams to task
2, and so on:</p>

<p><img src="/2015/04/streamsearch-29.png" alt="Joining by co-partitioning two streams" width="550" height="412" /></p>

<p>This is a good partitioning model if you’re doing an
<a href="https://en.wikipedia.org/wiki/Join_(SQL)#Equi-join">equi-join</a>, because you can set up the
partitioning of the input streams such that each tasks only needs to know about its own input
partitions, and can ignore all the other partitions. This allows you to increase parallel processing
and scale the computation simply by creating more partitions.</p>

<p>However, full-text search is different. We’re not doing an equi-join on a particular field, we’re
trying to find matches involving arbitrarily complicated boolean expressions. In general, we don’t
know in advance which documents are going to match which queries. (The query index tells us
<em>approximately</em> which queries might match, but it’s not precise enough to use for partitioning.)</p>

<p>We still want to partition the query and document streams, because that will allow the system to
scale. But we also want to be able to match every document against every possible query. In other
words, we need to make sure that every query partition is joined with every document partition:</p>

<p><img src="/2015/04/streamsearch-31.png" alt="Cartesian product join" width="550" height="412" /></p>

<p>If you think about it, what we need is a
<a href="https://en.wikipedia.org/wiki/Cartesian_product">cartesian product</a> of query partitions and
document partitions. We want to create a separate StreamTask for every possible combination of
a query partition and a document partition. For example, in the picture above, StreamTask 8 is
responsible for handling query partition 4 and document partition 2.</p>

<p>This gives us exactly the semantics we need: every query is sent to multiple tasks (one task per
document partition), and conversely, every document is sent to multiple tasks (one task per query
partition). Each task can independently do its work on its own partitions, and afterwards you just
need to combine all the matches for each document. The dataflow is similar to the scatter-gather
approach you get in distributed search engines.</p>

<p>Unfortunately, this mode of streaming joins is not yet supported in Samza, but it’s being worked on
(you can track it under <a href="https://issues.apache.org/jira/browse/SAMZA-353">SAMZA-353</a> if you’re
interested). Once this feature is in place, you’ll be able to perform full-text search on streams at
arbitrary scale, simply by adding new partitions and adding more machines to the cluster. Combining
the clever indexing of Luwak with the scalability of Kafka and Samza — isn’t that cool?</p>

<p><em>If you want to play with Samza, there’s a quickstart project
“<a href="http://samza.apache.org/startup/hello-samza/0.8/">hello-samza</a>”, and you can find our
proof-of-concept integration of Samza and Luwak
<a href="https://github.com/romseygeek/samza-luwak">on Github</a>.</em></p>

<p><em><a href="https://twitter.com/romseygeek">Alan Woodward</a> is a director of <a href="http://www.flax.co.uk/">Flax</a>,
a consultancy specializing in open source search engines. He is a committer on
<a href="http://lucene.apache.org/">Lucene/Solr</a>, and developer of
<a href="https://github.com/flaxsearch/luwak">Luwak</a>. He previously worked on the enterprise search product
Fast ESP.</em></p>

<p><em><a href="https://martin.kleppmann.com/">Martin Kleppmann</a> is a committer on
<a href="http://samza.apache.org/">Apache Samza</a> and author of the upcoming O’Reilly book
<a href="http://dataintensive.net/">Designing Data-Intensive Applications</a>. He previously co-founded
<a href="https://rapportive.com/">Rapportive</a>, which was acquired by <a href="https://www.linkedin.com/">LinkedIn</a>.</em></p>

                ]]></content:encoded>
            </item>
        
            <item>
                <title>Turning the database inside-out with Apache Samza</title>
                <link>http://martin.kleppmann.com/2015/03/04/turning-the-database-inside-out.html</link>
                <comments>http://martin.kleppmann.com/2015/03/04/turning-the-database-inside-out.html#disqus_thread</comments>
                <pubDate>Wed, 04 Mar 2015 00:00:00 +0000</pubDate>
                <dc:creator>Martin Kleppmann</dc:creator>
                
                    <guid isPermaLink="true">http://martin.kleppmann.com/2015/03/04/turning-the-database-inside-out.html</guid>
                
                <description><![CDATA[ This is an edited and expanded transcript of a talk I gave at Strange Loop 2014. This transcript was originally published on the Confluent blog. The video recording (embedded below) has been watched over 8,000 times. For those of you who prefer reading, I thought it would be worth writing... ]]></description>
                <content:encoded><![CDATA[
                    <p>This is an edited and expanded transcript of a
<a href="/2014/09/18/turning-database-inside-out-at-strange-loop.html">talk</a>
I gave at <a href="https://thestrangeloop.com/archive/2014">Strange Loop 2014</a>.
This transcript was originally published on the
<a href="http://blog.confluent.io/2015/03/04/turning-the-database-inside-out-with-apache-samza/">Confluent blog</a>. The
<a href="https://www.youtube.com/watch?v=fU9hR3kiOK0&amp;list=PLeKd45zvjcDHJxge6VtYUAbYnvd_VNQCx">video recording</a>
(embedded below) has been watched over 8,000 times. For those of you who prefer reading, I thought
it would be worth writing down the talk.</p>

<iframe width="550" height="309" src="https://www.youtube.com/embed/fU9hR3kiOK0?rel=0" frameborder="0" allowfullscreen=""></iframe>

<p><em>Databases are global, shared, mutable state. That’s the way it has been since the 1960s, and no
amount of NoSQL has changed that. However, most self-respecting developers have got rid of mutable
global variables in their code long ago. So why do we tolerate databases as they are?</em></p>

<p><em>A more promising model, used in some systems, is to think of a database as an always-growing
collection of immutable facts. You can query it at some point in time — but that’s still old,
imperative style thinking. A more fruitful approach is to take the streams of facts as they come in,
and functionally process them in real-time.</em></p>

<p><em>This talk introduces Apache Samza, a distributed stream processing framework developed at LinkedIn.
At first it looks like yet another tool for computing real-time analytics, but it’s more than that.
Really it’s a surreptitious attempt to take the database architecture we know, and turn it inside
out.</em></p>

<p><em>At its core is a distributed, durable commit log, implemented by Apache Kafka. Layered on top are
simple but powerful tools for joining streams and managing large amounts of data reliably.</em></p>

<p><em>What do we have to gain from turning the database inside out? Simpler code, better scalability,
better robustness, lower latency, and more flexibility for doing interesting things with data. After
this talk, you’ll see the architecture of your own applications in a new light.</em></p>

<p><img src="/2015/03/insideout-01.png" alt="Turning the database inside-out with Apache Samza" width="550" height="363" /></p>

<p>This talk is about database architecture and application architecture. It’s somewhat related to an
open source project I’ve been working on, called <a href="http://samza.apache.org/">Apache Samza</a>. I’m
<a href="http://martin.kleppmann.com/">Martin Kleppmann</a>, and I was until recently at LinkedIn working on
Samza. At the moment I’m taking a sabbatical to write a book for O’Reilly, called
<a href="http://dataintensive.net/">Designing Data-Intensive Applications</a>.</p>

<p>Let’s talk about databases. What I mean is not any particular brand of database — I don’t mind
whether you’re using relational, or NoSQL, or whatever. I’m really talking about the general concept
of a database, as we use it when building applications.</p>

<p>Take, for example, the stereotypical web application architecture:</p>

<p><img src="/2015/03/insideout-02.png" alt="Three-tier architecture: client, backend, database" width="550" height="412" /></p>

<p>You have a client, which may be a web browser or a mobile app, and that client talks to some kind of
server-side system (which you may call a “backend” or whatever you like). The backend typically
implements some kind of business logic, performs access control, accepts input, produces output.
When the backend needs to remember something for the future, it stores that data in a database, and
when it needs to look something up, it queries a database. That’s all very familiar stuff.</p>

<p>The way we typically build these sorts of applications is that we make the backend layer
<em>stateless</em>. That has a lot of advantages: you can scale out the backend by just running more
processes in parallel, and you can route any request to any backend instance (they are all equally
well qualified to handle the request), so it’s easy to spread the load across multiple machines. Any
state that is required to handle a request will be looked up from the database on each request. That
also works nicely with HTTP, since HTTP is a stateless protocol.</p>

<p>However, the big problem with this approach is: the state has to go <em>somewhere</em>, and so we have to
put it in the database. We are now using the database as a kind of gigantic, global, shared, mutable
state. It’s like a global variable that’s shared between all your application servers. It’s exactly
the kind of horrendous thing that, in shared-memory concurrency, we’ve been trying to get rid of for
ages. <a href="https://en.wikipedia.org/wiki/Actor_model">Actors</a>,
<a href="http://clojure.com/blog/2013/06/28/clojure-core-async-channels.html">channels</a>,
<a href="https://gobyexample.com/goroutines">goroutines</a>, etc. are all attempts to get away from
shared-memory concurrency, avoiding the problems of locking, deadlock, concurrent modifications,
race conditions, and so on.</p>

<p>We’re trying to get away from shared-memory concurrency, but with databases we’re still stuck with
this big, shared, mutable state. So it’s worth thinking about this: if we’re trying to get rid of
shared memory in our single-process application architecture, what would happen if we tried to get
rid of this shared mutable state on a whole-system level?</p>

<p><img src="/2015/03/insideout-03.png" alt="It's always been that way" width="550" height="202" /></p>

<p>At the moment, it seems to me that the main reason why systems are still being built with mutable
databases is just inertia: that’s the way we’ve building applications for decades, and we don’t
really have good tools to do it differently. So, let’s think about what other possibilities we have
for building stateful systems.</p>

<p>In order to try to figure out what routes we could take, I’d like to look at four different examples
of things that databases currently do, and things that we do with databases. And these four examples
might give us an indicator of the directions in which we could take these systems forward in future.</p>

<p><img src="/2015/03/insideout-04.png" alt="Title: 1. Replication" width="550" height="172" /></p>

<p>The first example I’d like to look at is <em>replication</em>. You probably know about the basics of
replication: the idea is that you have a copy of the same data on multiple machines (nodes), so that
you can serve reads in parallel, and so that the system keeps running if you lose a machine.</p>

<p>It’s the database’s job to keep those replicas in sync. A common architecture for replication is
that you send your writes to one designated node (which you may call the <em>leader</em>, <em>master</em> or
<em>primary</em>), and it’s the leader’s responsibility to ensure that the writes are copied to the other
nodes (which you may call <em>followers</em>, <em>slaves</em> or <em>standbys</em>). There are also other ways of doing
it, but leader-based replication is familiar — many systems are built that way.</p>

<p>Let’s look at an example of replication to see what’s actually happening under the hood. Take
a shopping cart, for instance.</p>

<p><img src="/2015/03/insideout-05.png" alt="Shopping cart example" width="550" height="412" /></p>

<p>This is using a relational data model, but the same principles apply with other data models too. Say
you have a table with three columns: customers, products, and quantity. Each row indicates that
a particular customer has a particular quantity of a particular product in their shopping cart.</p>

<p>Now say customer 123 changes their mind, and instead of wanting quantity 1 of product 999, they
actually want quantity 3 of that product. So they issue an <em>update</em> query to the database, which
matches the row for customer 123 and product 999, and it changes the value of the quantity column
from 1 to 3.</p>

<p><img src="/2015/03/insideout-06.png" alt="Updating quantity of item in the shopping cart" width="550" height="412" /></p>

<p>The result is that the database overwrites the quantity value with the new value, i.e. it applies
the update in the appropriate place.</p>

<p>Now, I was talking about replication. What does this update do in the context of replication? Well,
first of all, you send this update query to your leader, and it executes the query, figures out
which rows match the condition, and applies the write locally:</p>

<p><img src="/2015/03/insideout-07.png" alt="Replicating a write from a leader to a follower" width="550" height="412" /></p>

<p>Now, how does this write get applied to the other replicas? There are several different ways how you
can implement replication. One option is to send the same update query to the follower, and it
executes the same statement on its own copy of the database. Another option is to ship the
write-ahead log from the leader to the follower.</p>

<p>A third option for replication, which I’ll focus on here, is called a <em>logical log</em>. In this case,
the leader writes out the effect that the query had — i.e. which rows were inserted, updated or
deleted — like a kind of diff. For an update, like in this example, the logical log identifies the
row that was changed (using a primary key or some kind of internal tuple identifier), gives the new
value of that row, and perhaps also the old value.</p>

<p>This might seem like nothing special, but notice that something interesting has happened here:</p>

<p><img src="/2015/03/insideout-08.png" alt="Update statement is imperative, replication event is immutable" width="550" height="412" /></p>

<p>At the top we have the update statement, an imperative statement describing the state mutation. It
is an instruction to the database, telling it to modify certain rows in the database that match
certain conditions.</p>

<p>On the other hand, when the write is replicated from the leader to the follower as part of the
logical log, it takes a different form: it becomes an event, stating that at a particular point in
time, a particular customer changed the quantity of a particular product in their cart from 1 to 3.
And this is a <em>fact</em> — even if the customer later removes the item from their cart, or changes the
quantity again, or goes away and never comes back, that doesn’t change the fact that this state
change occurred. The fact always remains true.</p>

<p>This distinction between an imperative modification and an immutable fact is something you may have
seen in the context of
<a href="http://blog.confluent.io/2015/01/29/making-sense-of-stream-processing/">event sourcing</a>. That’s
a method of database design that says you should structure all of your data as immutable facts, and
it’s an interesting idea.</p>

<p>However, what I’m saying here is: even if you use your database in the traditional way, overwriting
old state with new state, the database’s internal replication mechanism may still be translating
those imperative statements into a stream of immutable events.</p>

<p>Hold that thought for now: I’m going to talk about some completely different things, and return to
this idea later.</p>

<p><img src="/2015/03/insideout-09.png" alt="Title: 2. Secondary indexes" width="550" height="302" /></p>

<p>The second one of the four things I want to talk about is <em>secondary indexing</em>. You’re probably
familiar with secondary indexes — they are the bread and butter of relational databases. Using the
shopping cart example again:</p>

<p><img src="/2015/03/insideout-10.png" alt="Two indexes on one table" width="550" height="412" /></p>

<p>You have a table with different columns, and you may have several different indexes on that table in
order to be able to efficiently find rows that match a particular query. For example, you may run
some SQL to create two indexes: one on the customer_id column, and a separate index on the
product_id column.</p>

<p>Using the index on customer_id you can then efficiently find all the items that a particular
customer has in their cart. Using the index on product_id you can efficiently find all the carts
that contain a particular product.</p>

<p>What does the database do when you run one of these CREATE INDEX queries?</p>

<p><img src="/2015/03/insideout-11.png" alt="An index is a data structure derived from table data" width="550" height="412" /></p>

<p>The database scans over the entire table, and it creates an auxiliary data structure for each index.
An index is a data structure that represents the information in the base table in some different
way. In this case, the index is a key-value-like structure: the keys are the contents of the column
that you’re indexing, and the values are the rows that contain this particular key.</p>

<p><img src="/2015/03/insideout-12.png" alt="Values in table cells become keys in the index" width="550" height="412" /></p>

<p>Put another way: to build the index for the customer_id column, the database takes all the values
that appear in that column, and uses them as keys in a dictionary. A value points at all of the
occurrences of that value — for example, the index entry 123 points at all of the rows which have
a customer_id of 123. Similarly for the other index.</p>

<p>The important point here is that the process of going from the base table to the indexes is
completely mechanical. You simply tell the database that you want a particular index to exist, and
it goes away and builds that index for you.</p>

<p><img src="/2015/03/insideout-13.png" alt="Index is generated from table through a derivation function" width="550" height="412" /></p>

<p>The index doesn’t add any new information to the database — it just represents the existing data in
a different structure. (Put another way, if you drop the index, that doesn’t delete any data from
your database.) It’s a redundant data structure that only exists to make certain queries faster. And
that data structure can be entirely <em>derived</em> from the original table.</p>

<p>Creating an index is essentially a transformation which takes a database table as input, and
produces an index as output. The transformation consists of going through all the rows in the table,
picking out the field that you want to index, and restructuring the data so that you can look up by
that field. That transformation is built into the database, so you don’t need to implement it
yourself. You just tell the database that you want an index on a particular field to exist, and it
does all the work of building it.</p>

<p>Another great thing about indexes: whenever the data in the underlying table changes, the database
automatically updates the indexes to be consistent with the new data in the table. In other words,
this transformation function which derives the index from the original table is not just applied
once when you create the index, but applied continuously.</p>

<p>With many databases, these index updates are even done in a transactionally consistent way. This
means that any later transactions will see the data in the index in the same state as it is in the
underlying table. If a transaction aborts and rolls back, the index modifications are also rolled
back. That’s a really great feature which we often don’t appreciate!</p>

<p><img src="/2015/03/insideout-14.png" alt="Create index concurrently" width="550" height="362" /></p>

<p>What’s even better is that some databases let you build an index at the same time as continuing to
process write queries. In PostgreSQL, for example, you can say
<a href="http://www.postgresql.org/docs/9.4/static/sql-createindex.html#SQL-CREATEINDEX-CONCURRENTLY">CREATE INDEX CONCURRENTLY</a>.
On a large table, creating an index could take several hours, and on a production database you
wouldn’t want to have to stop writing to the table while the index is being built. The index builder
really needs to be a background process which can run while your application is simultaneously
reading and writing to the database as usual.</p>

<p>The fact that databases can do this is quite impressive. After all, to build an index, the database
has to scan the entire table contents, but those contents are changing at the same time as the scan
is happening. The index builder is tracking a moving target. At the end, the database ends up with
a transactionally consistent index, despite the fact that the data was changing concurrently.</p>

<p>In order to do this, the database needs to build the index from a consistent snapshot at one point
in time, and also keep track of all the changes that occurred since that snapshot while the index
build was in progress. That’s a really cool feature.</p>

<p>So far we’ve discussed two aspects of databases: replication and secondary indexing. Let’s move on
to number 3: caching.</p>

<p><img src="/2015/03/insideout-15.png" alt="Title: 3. Caching" width="550" height="194" /></p>

<p>What I’m talking about here is caching that is explicitly done by the application. (You also get
caching happening automatically at various levels, such as the operating system’s page cache and the
CPU caches, but that’s not what I’m talking about here.)</p>

<p>Say you have a website that becomes popular, and it becomes too expensive or too slow to hit the
database for every web request, so you introduce a caching layer — often using
<a href="http://memcached.org/">memcached</a> or <a href="http://redis.io/">Redis</a> or something of that sort. And often
this cache is managed in application code, which typically looks something like this:</p>

<p><img src="/2015/03/insideout-16.png" alt="Reading from a cache; filling cache from DB on miss" width="550" height="412" /></p>

<p>When a request arrives at the application, you first look in a cache to see whether the data you
want is already there. The cache lookup is typically by some key that describes the data you want.
If the data is in the cache, you can return it straight to the client.</p>

<p>If the data you want isn’t in the cache, that’s a cache miss. You then go to the underlying
database, and query the data that you want. On the way out, the application also writes that data to
the cache, so that it’s there for the next request that needs it. The thing it writes to the cache
is whatever the application would have wanted to see there in the first place. Then the application
returns the data to the client.</p>

<p>This is a very common pattern, but there are several big problems with it.</p>

<p><img src="/2015/03/insideout-17.png" alt="Problems with read-through caching" width="550" height="412" /></p>

<p>The first problem is that clichéd quote about there being only
<a href="http://martinfowler.com/bliki/TwoHardThings.html">two hard problems in computer science</a> (which
I can’t stand any more). But seriously, if you’re managing a cache like this, then cache
invalidation really is tricky. When data in the underlying database changes, how do you know what
entries in the cache to expire or update?  One option is to have an expiry algorithm which figures
out which database change affects which cache entries, but those algorithms are brittle and
error-prone. Alternatively, you can just have a time-to-live (expiry time) and accept that you
sometimes read stale data from the cache, but such staleness is often unacceptable.</p>

<p>Another problem is that this architecture is very prone to race conditions. For example, say you
have two processes concurrently writing to the database and also updating the cache. They might
update the database in one order, and the cache in the other order, and now the two are
inconsistent. Or if you fill the cache on read, you may read and write concurrently, and so the
cache is updated with a stale value while the concurrent write is occurring. I suspect that most of
us building these systems just pretend that the race conditions don’t exist, because they are just
too much to think about.</p>

<p>A third problem is cold start. If you reboot your memcached servers and they lose all their cached
contents, suddenly every request is a cache miss, the database is overloaded because of the sudden
surge in requests, and you’re in a world of pain. If you want to create a new cache, you need some
way of bootstrapping its contents without overloading other parts of the system.</p>

<p><img src="/2015/03/insideout-18.png" alt="Creating an index is simple, maintaining a cache is a mess" width="550" height="361" /></p>

<p>So, here we have a contrast: on the one hand, creating a secondary index in a database is
beautifully simple, one line of SQL — the database handles it automatically, keeping everything
up-to-date, and even making the index transactionally consistent. On the other hand,
application-level cache maintenance is a complete mess of complicated invalidation logic, race
conditions and operational problems.</p>

<p>Why should it be that way? Secondary indexes and caches are not fundamentally different. We said
earlier that a secondary index is just a redundant data structure on the side, which structures the
same data in a different way, in order to speed up read queries. A cache is just the same.</p>

<p><img src="/2015/03/insideout-19.png" alt="But a cache is also generated from a database through a derivation function" width="550" height="323" /></p>

<p>If you think about it, a cache is also the result of taking your data in one form (the form in which
it’s stored in the database) and transforming it into a different form for faster reads. In other
words, the contents of the cache are derived from the contents of the database.</p>

<p>We said that a secondary index is built by picking out one field from every record, and using that
as the key in a dictionary. In the case of a cache, we may apply an arbitrary function to the data:
the data from the database may have gone through some kind of business logic or rendering before
it’s put in the cache, and it may be the result of joining several records from different tables.
But the end result is similar: if you lose your cache, you can rebuild it from the underlying
database; thus, the contents of the cache are derived from the database.</p>

<p>In a read-through cache, this transformation happens on the fly, when there is a cache miss. But we
could perhaps imagine making the process of building and updating a cache more systematic, and more
similar to secondary indexes. Let’s return to that idea later.</p>

<p>I said I was going to talk about four different aspects of database. Let’s move on to the fourth:
<em>materialized views</em>.</p>

<p><img src="/2015/03/insideout-20.png" alt="Title 4. Materialized views" width="550" height="313" /></p>

<p>You may already know what materialized views are, but let me explain them briefly in case you’ve not
previously come across them. You may be more familiar with “normal” views — non-materialized views,
or virtual views, or whatever you want to call them. They work like this:</p>

<p><img src="/2015/03/insideout-21.png" alt="How a normal (non-materialized) view works" width="550" height="412" /></p>

<p>In a relational database, where views are common, you would create a view by saying “CREATE VIEW
viewname…” followed by a SELECT query. When you now look at this view in the database, it looks
somewhat like a table — you can use it in read queries like any other table. And when you do this,
say you SELECT * from that view, the database’s query planner actually rewrites the query into the
underlying query that you used in the definition of the view.</p>

<p>So you can think of a view as a kind of convenient alias, a wrapper that allows you to create an
abstraction, hiding a complicated query behind a simpler interface.</p>

<p>Contrast that with a <em>materialized</em> view, which is defined using almost identical syntax:</p>

<p><img src="/2015/03/insideout-22.png" alt="Creating a materialized views: copy of data" width="550" height="412" /></p>

<p>You also define a materialized view in terms of a SELECT query; the only syntactic difference is
that you say CREATE MATERIALIZED VIEW instead of CREATE VIEW. However, the implementation is totally
different.</p>

<p>When you create a materialized view, the database starts with the underlying tables — that is, the
tables you’re querying in the SELECT statement of the view (“bar” in the example). The database
scans over the entire contents of those tables, executes that SELECT query on all of the data, and
copies the results of that query into something like a temporary table.</p>

<p>The results of this query are actually written to disk, in a form that’s very similar to a a normal
table. And that’s really what “materialized” means in this context: it just means that the view’s
query has been executed and the results written to disk.</p>

<p>Remember that with the non-materialized view, the database would expand the view into the underlying
query at query time. On the other hand, when you query a materialized view, the database can read
its contents directly from disk, just like a table. The view’s underlying query has already been
executed ahead of time, so the database now just needs to read the result. This is especially useful
if the view’s underlying query is expensive.</p>

<p><img src="/2015/03/insideout-23.png" alt="Derivation function for materialized view" width="550" height="316" /></p>

<p>If you’re thinking “this seems like a cache of query results”, you would be right — that’s exactly
what it is. However, the big difference between a materialized view and application-managed caches
is the responsibility for keeping it up-to-date.</p>

<p>With a materialized view, you declare once how you want the materialized view to be defined, and the
database takes care of building that view from a consistent snapshot of the underlying tables (much
like building a secondary index). Moreover, when the data in the underlying tables changes, the
database takes responsibility for maintaining the materialized view, keeping it up-to-date. Some
databases do this materialized view maintenance on an ongoing basis, and some require you to
periodically refresh the view so that changes take effect. But you certainly don’t have to do cache
invalidation in your application code.</p>

<p>Another feature of application-managed caches is that you can apply arbitrary business logic to the
data before storing it in the cache, so that you can do less work at query time, or reduce the
amount of data you need to cache. Could a materialized view do something similar?</p>

<p><img src="/2015/03/insideout-24.png" alt="Use JavaScript stored procedure in derivation function" width="550" height="412" /></p>

<p>In a relational database, materialized views are defined using SQL, so the transformations they can
apply to the data are limited to the operations that are built into SQL (which are very restricted
compared to a general-purpose programming language). However, many databases can also be extended
using stored procedures — code that runs inside the database and can be called from SQL. For
example, you can <a href="http://pgxn.org/dist/plv8/">use JavaScript</a> to write PostgreSQL stored procedures.
This would let you implement something like an application-level cache, including arbitrary business
logic, running as a materialized view inside a database.</p>

<p>I am not convinced that this is necessarily a good idea: with code running inside your database,
it’s much harder to reason about monitoring, versioning, deployments, performance impact,
multi-tenant resource isolation, and so on. I don’t think I would advocate stored procedures as an
application development platform. However, the idea of materialized views is nevertheless
interesting.</p>

<p><img src="/2015/03/insideout-25.png" alt="Comparison of replication, secondary indexing, caching and materialized views" width="550" height="412" /></p>

<p>Let’s recap the four aspects of databases that we discussed: replication, secondary indexing,
caching, and materialized views. What they all have in common is that they are dealing with <em>derived
data</em> in some way: some secondary data structure is derived from an underlying, primary dataset, via
a transformation process.</p>

<ul>
  <li>We first discussed replication, i.e. keeping a copy of the same data on multiple machines. It
generally works very well, so we’ll give it a green smiley. There are some operational quirks with
some databases, and some of the tooling is a bit weird, but on the whole it’s mature
well-understood, and well-supported.</li>
  <li>Similarly, secondary indexing works very well. You can build a secondary index concurrently with
processing write queries, and the database somehow manages to do this in a transactionally
consistent way.</li>
  <li>On the other hand, application-level caching is a complete mess. Red frowny face.</li>
  <li>And materialized views are so-so: the idea is good, but the way they’re implemented is not what
you’d want from a modern application development platform. Maintaining the materialized view puts
additional load on the database, while actually the whole point of a cache is to <em>reduce</em> load on
the database!</li>
</ul>

<p><img src="/2015/03/insideout-26.png" alt="Magically self-updating cache..." width="550" height="412" /></p>

<p>However, there’s something really compelling about this idea of materialized views. I see
a materialized view almost as a kind of cache that magically keeps itself up-to-date. Instead of
putting all of the complexity of cache invalidation in the application (risking race conditions and
all the discussed problems), materialized views say that cache maintenance should be the
responsibility of the data infrastructure.</p>

<p><img src="/2015/03/insideout-27.png" alt="Let's rethink materialized views!" width="550" height="412" /></p>

<p>So let’s think about this: can we reinvent materialized views, implement them in a modern and
scalable way, and use them as a general mechanism for cache maintenance?</p>

<p>If we started with a clean slate, without the historical baggage of existing databases, what would
the ideal architecture for applications look like?</p>

<p><img src="/2015/03/insideout-28.png" alt="Traditionally, the replication stream is an implementation detail" width="550" height="412" /></p>

<p>Think back to leader-based replication which we discussed earlier. You make your writes to a leader,
which first applies the writes locally, and then sends those writes over the network to follower
nodes. In other words, the leader sends a stream of data changes to the followers. We discussed
a logical log as one way of implementing this.</p>

<p>In a traditional database architecture, application developers are not supposed to think about that
replication stream. It’s an implementation detail that is hidden by the database abstraction. SQL
queries and responses are the database’s public interface — and the replication stream is not part
of the public interface. You’re not supposed to go and parse that stream, and use it for your own
purposes. (Yes, there are <a href="http://tungsten-replicator.org/">tools that do this</a>, but in traditional
databases they are on the periphery of what is supported, whereas the SQL interface is the dominant
access method.)</p>

<p>And in some ways this is reasonable — the relational model is a pretty good abstraction, which is
why it has been so popular for several decades. But SQL is not the last word in databases.</p>

<p>What if we took that replication stream, and made it a first-class citizen in our data architecture?
What if we changed our infrastructure so that the replication stream was not an implementation
detail, but a key part of the public interface of the database? What if we <em>turn the database
inside-out</em>, take the implementation detail that was previously hidden, and make it a top-level
concern?  What would that look like?</p>

<p><img src="/2015/03/insideout-29.png" alt="Unbundle the database: make the transaction log a first-class component" width="550" height="412" /></p>

<p>Well, you could call that replication stream a <em>“transaction log”</em> or an <em>“event stream”</em>. You can
format all your writes as immutable events (facts), like we saw earlier in the context of a logical
log. Now each write is just an immutable event that you can append to the end of the transaction
log. The transaction log is a really simple, append-only data structure.</p>

<p>There are various ways of implementing this, but one good choice for the transaction log is to use
<a href="http://kafka.apache.org/">Apache Kafka</a>. It provides an append-only log data structure, but it does
so in a reliable and scalable manner — it durably writes everything to disk, it replicates data
across multiple machines (so that you don’t lose any data if you lose a machine), and it partitions
the stream across multiple machines for horizontal scalability. It easily handles
<a href="https://engineering.linkedin.com/kafka/benchmarking-apache-kafka-2-million-writes-second-three-cheap-machines">millions of writes</a>
per second on very modest hardware.</p>

<p>When you do this, you don’t need to necessarily make your writes through a leader database — you
could also imagine directly appending your writes to the log. (Going through a leader would still be
useful if you want to validate that writes meet certain constraints before writing them to the log.)</p>

<p>Writing to this system is now super fast and scalable, because the only thing you’re doing is
appending an event to a log. But what about reads? Reading data that has been written to the log is
now really inconvenient, because you have to scan the entire log to find the thing that you want.</p>

<p>The solution is to build materialized views from the writes in the transaction log. The materialized
views are just like the secondary indexes we talked about earlier: data structures that are derived
from the data in the log, and optimized for fast reading. A materialized view is just a
<a href="http://www.cidrdb.org/cidr2015/Papers/CIDR15_Paper16.pdf">cached subset of the log</a>, and you could
rebuild it from the log at any time. There could be many different materialized views onto the same
data: a key-value store, a full-text search index, a graph index, an analytics system, and so on.</p>

<p>You can think of this as <em>“unbundling”</em> the database. All the stuff that was previously packed into
a single monolithic software package is being broken out into modular components that can be
composed in flexible ways.</p>

<p><img src="/2015/03/insideout-30.png" alt="Derive materialized views from the transaction log using Samza" width="550" height="412" /></p>

<p>If you use Kafka to implement the log, how do you implement these materialized views? That’s where
<a href="http://samza.apache.org/">Apache Samza</a> comes in. It’s a stream processing framework that is
designed to go well with Kafka. With Samza, you write jobs that consume the events in a log, and
build cached views of the data in the log. When a job first starts up, it can build up its state by
consuming all the events in the log. And on an ongoing basis, whenever a new event appears in the
stream, it can update the view accordingly. The view can be any existing database or index — Samza
just provides the framework for processing the stream.</p>

<p>Anyone who wants to read data can now query those materialized views that are maintained by the
Samza jobs. Those views are just databases, indexes or caches, and you can send read-only requests
to them in the usual way. The difference to traditional database architecture is that if you want to
write to the system, you don’t write directly to the same databases that you read from. Instead, you
write to the log, and there is an explicit transformation process which takes the data on the log
and applies it to the materialized views.</p>

<p><img src="/2015/03/insideout-31.png" alt="Make writes an append-only stream of immutable facts" width="550" height="412" /></p>

<p>This separation of reads and writes is really the key idea here. By putting writes only in the log,
we can make them much simpler: we don’t need to update state in place, so we move away from the
problems of concurrent mutation of global shared state. Instead, we just keep an append-only log of
immutable events. This gives excellent performance (appending to a file is sequential I/O, which is
much faster than random-access I/O), is easily scalable (independent events can be put in separate
partitions), and is much easier to make reliable.</p>

<p>If it’s too expensive for you to keep the entire history of every change that ever happened to your
data, Kafka supports compaction, which is a kind of garbage collection process that runs in the
background. It’s very similar to the log compaction that databases do internally. But that doesn’t
change the basic principle of working with streams of immutable events.</p>

<p>These ideas are nothing new. To mention just a few examples,
<a href="http://blog.confluent.io/2015/01/29/making-sense-of-stream-processing/">Event Sourcing</a> is a data
modelling technique based on the same principle; query languages like
<a href="http://www.researchgate.net/profile/Letizia_Tanca/publication/3296132_What_you_always_wanted_to_know_about_Datalog_(and_never_dared_toask)/links/0fcfd50ca2d20473ca000000.pdf">Datalog</a>
have been based on immutable facts for decades; databases like <a href="http://www.datomic.com/">Datomic</a>
are built on immutability, enabling neat features like point-in-time historical queries; and the
<a href="http://manning.com/marz/">Lambda Architecture</a> is one possible approach for dealing with immutable
datasets at scale. At many levels of the stack, immutability is being
<a href="http://www.cidrdb.org/cidr2015/Papers/CIDR15_Paper16.pdf">applied successfully</a>.</p>

<p><img src="/2015/03/insideout-32.png" alt="Make reads from materialized views" width="550" height="344" /></p>

<p>On the read side, we need to start thinking less about querying databases, and more about consuming
and joining streams, and maintaining materialized views of the data in the form in which we want to
read it.</p>

<p>To be clear, I think querying databases will continue to be important: for example, when an analyst
is running exploratory ad-hoc queries against a data warehouse of historical data, it doesn’t make
much sense to use materialized views — for those kinds of queries it’s better to just keep all the
raw events, and to build databases which can scan over them very quickly. Modern column stores have
become very good at that.</p>

<p>But in situations where you might use application-managed caches (namely, an OLTP context where the
queries are known in advance and predictable), materialized views are very helpful.</p>

<p><img src="/2015/03/insideout-33.png" alt="Precompute and maintain materialized views from the log" width="550" height="412" /></p>

<p>There are a few differences between a read-through cache (which gets invalidated or updated within
the application code) and a materialized view (which is maintained by consuming a log):</p>

<ul>
  <li>With the materialized view, there is a principled <em>translation process</em> from the write-optimized
data in the log into the read-optimized data in the view. That translation runs in a separate
process which you can monitor, debug, scale and maintain independently from the rest of your
application. By contrast, in the typical read-through caching approach, the cache management logic
is deeply intertwined with the rest of the application, it’s easy to introduce bugs, and it’s
difficult to understand what is happening.</li>
  <li>A cache is filled on demand when there is a cache miss (so the first request for a given object is
always slow). By contrast, a materialized view is <em>precomputed</em>, i.e. its entire contents are
computed before anyone asks for it — just like a secondary index. This means there is no such
thing as a cache miss: if an item doesn’t exist in the materialized view, it doesn’t exist in the
database. There is no need to fall back to some kind of underlying database.</li>
  <li>Once you have this process for translating logs into views, you have great flexibility to create
new views: if you want to present your existing data in some new way, you can simply create a new
stream processing job, consume the input log from the beginning, and thus build a completely new
view onto all the existing data. (If you think about it, this is pretty much what a database does
internally when you create a new secondary index on an existing table.) You can then maintain both
views in parallel, gradually move applications to the new view, and eventually discard the old
view.  No more scary stop-the-world schema migrations.</li>
</ul>

<p><img src="/2015/03/insideout-34.png" alt="Mechanics that need to be solved for practical adoption" width="550" height="412" /></p>

<p>Of course, such a big change in application architecture and database architecture means that many
practical details need to be figured out: how do you deploy and monitor these stream processing
jobs, how do you make the system robust to various kinds of fault, how do you integrate with
existing systems, and so on? But the good news is that all of these issues are being worked on. It’s
a fast-moving area with lots of activity, so if you find it interesting, we’d love your
contributions to the open source projects.</p>

<p><img src="/2015/03/insideout-35.png" alt="Happiness" width="550" height="412" /></p>

<p>We are still figuring out how to build large-scale applications well — what techniques we can use to
make our systems scalable, reliable and maintainable. Put more bluntly, we need to figure out ways
to stop our applications turning into <a href="http://www.laputan.org/pub/foote/mud.pdf">big balls of mud</a>.</p>

<p>However, to me, this approach of immutable events and materialized views seems like a very promising
route forwards. I am optimistic that this kind of application architecture will help us build better
(more powerful and more reliable) software faster.</p>

<p><img src="/2015/03/insideout-36.png" alt="Why?" width="550" height="412" /></p>

<p>The changes I’ve proposed are quite radical, and it’s going to be a lot of work to put them into
practice. If we are going to completely change the way we use databases, we had better have some
very good reasons. So let me give three reasons why I think it’s worth moving towards a log of
immutable events.</p>

<p><img src="/2015/03/insideout-37.png" alt="Reason 1: Better data" width="550" height="345" /></p>

<p>Firstly, I think that writing data as a log produces better-quality data than if you update
a database directly. For example, if someone adds an item to their shopping cart and then removes it
again, those actions have information value. If you delete that information from the database when
a customer removes an item from the cart, you’ve just thrown away information that would have been
valuable for analytics and recommendation systems.</p>

<p>The entire long-standing debate about normalization in databases is predicated on the assumption
that data is going to be written and read in the same schema. A normalized database (with no
redundancy) is optimized for writing, whereas a denormalized database is optimized for reading. If
you separate the writing side (the log) from the reading side (materialized views), you can
denormalize the reading side to your heart’s content, but still retain the ability to process writes
efficiently.</p>

<p>Another very nice feature of an append-only log is that it allows much easier recovery from errors.
If you deploy some bad code that writes incorrect data to the database, or if a human enters some
incorrect data, you can look at the log to see the exact history of what happened, and
<a href="http://nathanmarz.com/blog/how-to-beat-the-cap-theorem.html">undo it</a>. That kind of recovery is
much harder if you’ve overwritten old data with new data, or even deleted data incorrectly. Also,
any kind of audit is much easier if you only ever append to a log — that’s why
<a href="http://blogs.msdn.com/b/pathelland/archive/2007/06/14/accountants-don-t-use-erasers.aspx">accountants don’t use erasers</a>.</p>

<p><img src="/2015/03/insideout-38.png" alt="Reason 2: Fully precomputed caches" width="550" height="347" /></p>

<p>Secondly, we can fix all the problems of read-through caches that we discussed earlier. The cold
start problem goes away, because we can simply precompute the entire contents of the cache (which
also means there’s no such thing as a cache miss).</p>

<p>If materialized views are only ever updated via the log, then a whole class of race conditions goes
away: the log defines the order in which writes are applied, so all the views that are based on the
same log apply the changes in the same order, so they end up being consistent with each other. The log
<a href="http://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying">squeezes the non-determinism</a>
of concurrency out of the stream of writes.</p>

<p>What I particularly like is that this architecture helps enable agile, incremental software
development. If you want to experiment with a new product feature, for which you need to present
existing data in a new way, you can just build a new view onto your data without affecting any of
the existing views. You can then show that view to a subset of users, and test whether it’s better
than the old thing. If yes, you can gradually move users to the new view; if not, you can just drop
the view as if nothing had happened. This is much more flexible than schema migrations, which are
generally an all-or-nothing affair. Being able to experiment freely with new features, without
onerous migration processes, is a tremendous enabler.</p>

<p><img src="/2015/03/insideout-39.png" alt="Reason 3: Streams everywhere" width="550" height="300" /></p>

<p>My third reason for wanting to change database architecture is that it allows us to put streams
everywhere. This point needs a bit more explanation.</p>

<p>Imagine what happens when a user of your application views some data. In a traditional database
architecture, the data is loaded from a database, perhaps transformed with some business logic, and
perhaps written to a cache. Data in the cache is rendered into a user interface in some way — for
example, by rendering it to HTML on the server, or by transferring it to the client as JSON and
rendering it on the client.</p>

<p>The result of template rendering is some kind of structure describing the user interface layout: in
a web browser, this would be the HTML DOM, and in a native application this would be using the
operating system’s UI components. Either way, a rendering engine eventually turns this description
of UI components into pixels in video memory, and this is what the user actually sees.</p>

<p><img src="/2015/03/insideout-40.png" alt="Transformation pipeline of materialized views" width="550" height="412" /></p>

<p>When you look at it like this, it looks very much like a data transformation pipeline. In fact, you
can think of each lower layer as a materialized view of the upper layer: the cache is a materialized
view of the database (the cache contents are derived from the database contents); the HTML DOM is
a materialized view of the cache (the HTML is derived from the JSON stored in the cache); and the
pixels in video memory are a materialized view of the HTML DOM (the rendering engine derives the
pixels from the UI layout).</p>

<p>Now, how well does each of these transformation steps work? I would argue that web browser rendering
engines are brilliant feats of engineering. You can use JavaScript to change some CSS class, or have
some CSS rules conditional on mouse-over, and the rendering engine automatically figures out which
rectangle of the page needs to be re-rendered as a result of the changes. It does
hardware-accelerated animations and even 3D transformations. The pixels in video memory are
automatically kept up-to-date with the underlying DOM state, and this very complex transformation
process works remarkably well.</p>

<p>What about the transformation from data objects to user interface components? I’ve given it a yellow
“so-so” smiley for now, as the techniques for updating user interface based on data changes are
still quite new. However, they are rapidly maturing: on the web, frameworks like
<a href="http://facebook.github.io/react/">Facebook’s React</a>, <a href="https://angularjs.org/">Angular</a> and
<a href="http://emberjs.com/">Ember</a> are enabling user interfaces that can be updated from a stream, and
Functional Reactive Programming (FRP) languages like <a href="http://elm-lang.org/">Elm</a> are in the same
area. There is a lot of activity in this field, and it is rapidly maturing towards a green smiley.</p>

<p>However, the transformation from database writes to cache/materialized view updates is still mostly
stuck in the dark ages. That’s what this entire talk is about: database-driven backend services are
currently the weakest link in this entire data transformation pipeline. Even if the user interface
can dynamically update when the underlying data changes, that’s not much use if the application
can’t detect when data changes!</p>

<p><img src="/2015/03/insideout-41.png" alt="Clients subscribe to materialized view changes" width="550" height="412" /></p>

<p>If we move to an architecture where materialized views are updated from a stream of changes, that
opens up an exciting new prospect: when a client reads from one of these views, it can keep the
connection open. If that view is later updated, due to some change that appeared in the stream, the
server can use this connection to notify the client about the change (for example, using
a <a href="https://developer.mozilla.org/en/docs/WebSockets">WebSocket</a> or
<a href="https://developer.mozilla.org/en-US/docs/Server-sent_events">Server-Sent Events</a>). The client can
then update its user interface accordingly.</p>

<p>This means that the client is not just reading the view at one point in time, but actually
subscribing to the stream of changes that may subsequently happen. Provided that the client’s
internet connection remains active, the server can push any changes to the client. After all, why
would you ever want outdated information on your screen if more recent information is available? The
notion of static web pages, which are requested once and then never change, is looking increasingly
anachronistic.</p>

<p><img src="/2015/03/insideout-42.png" alt="Move from request/response to subscribe/notify" width="550" height="339" /></p>

<p>However, allowing clients to subscribe to changes in data requires a big rethink of the way we write
applications. The request-response model is very deeply engrained in our thinking, in our network
protocols and in our programming languages: whether it’s a request to a RESTful service, or a method
call on an object, the assumption is generally that you’re going to make one request, and get one
response. There’s generally no provision for an ongoing stream of responses. Basically, I’m saying
that in the future, REST is not going to cut the mustard, because it’s based on a request-response
model.</p>

<p>Instead of thinking of requests and responses, we need to start thinking of subscribing to streams
and notifying subscribers of new events. And this needs to happen through all the layers of the
stack — the databases, the client libraries, the application servers, the business logic, the
frontends, and so on. If you want the user interface to dynamically update in response to data
changes, that will only be possible if we systematically apply stream thinking everywhere, so that
data changes can propagate through all the layers. I think we’re going to see a lot more people
using stream-friendly programming models based on actors and channels, or <em>reactive</em> frameworks such
as <a href="http://reactivex.io/">RxJava</a>.</p>

<p>I’m glad to see that some people are already working on this. A few weeks ago RethinkDB
<a href="http://rethinkdb.com/blog/realtime-web/">announced</a> that they are going to support clients
subscribing to query results, and being notified if the query results change.
<a href="https://www.meteor.com/">Meteor</a> and <a href="https://www.firebase.com/">Firebase</a> are also worth
mentioning, as frameworks which integrate the database backend and user interface layers so as to be
able to push changes into the user interface. These are excellent efforts. We need many more like
them.</p>

<p><img src="/2015/03/insideout-43.png" alt="Streams everywhere!" width="550" height="346" /></p>

<p>This brings us to the end of this talk. We started by observing that traditional databases and
caches are like global variables, a kind of shared mutable state that becomes messy at scale. We
explored four aspects of databases — replication, secondary indexing, caching and materialized views
— which naturally led us to the idea of streams of immutable events. We then looked at how things
would get better if we oriented our database architecture around streams and materialized views, and
found some compelling directions for the future.</p>

<p>Fortunately, this is not science fiction — it’s happening now. People are working on various parts
of the problem and finding good solutions. The tools at our disposal are rapidly becoming better.
It’s an exciting time to be building software.</p>

<p><em>If this excessively long article was not enough, and you want to read even more on the topic, I would recommend:</em></p>

<ul>
  <li><em>My upcoming book, <a href="http://dataintensive.net/">Designing Data-Intensive Applications</a>,
systematically explores the architecture of data systems. If you enjoyed this article, you’ll enjoy
the book too.</em></li>
  <li><em>I previously wrote about similar ideas in 2012, from a different perspective, in a blog post
called “<a href="/2012/10/01/rethinking-caching-in-web-apps.html">Rethinking caching in web apps</a>.”</em></li>
  <li><em>Jay Kreps has written several highly relevant articles, in particular about
<a href="http://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying">logs as a fundamental data abstraction</a>,
about the <a href="http://radar.oreilly.com/2014/07/questioning-the-lambda-architecture.html">lambda architecture</a>,
and about <a href="http://radar.oreilly.com/2014/07/why-local-state-is-a-fundamental-primitive-in-stream-processing.html">stateful stream processing</a>.</em></li>
  <li><em>The most common question people ask is: “but what about transactions?” — This is a somewhat open
research problem, but I think a promising way forward would be to layer a transaction protocol on
top of the asynchronous log. <a href="http://research.microsoft.com/pubs/199947/Tango.pdf">Tango</a> (from
Microsoft Research) describes one way of doing that, and <a href="http://www.bailis.org/">Peter Bailis</a> et
al.’s work on <a href="http://www.bailis.org/papers/ramp-sigmod2014.pdf">highly available transactions</a> is
also relevant.</em></li>
  <li><em>Pat Helland has been preaching this gospel for ages. His latest CIDR paper
<a href="http://www.cidrdb.org/cidr2015/Papers/CIDR15_Paper16.pdf">Immutability Changes Everything</a> is
a good summary.</em></li>
  <li><em>Jay Kreps has also written an applied guide to
<a href="http://blog.confluent.io/2015/02/25/stream-data-platform-1/">putting stream data to work in a company</a>.</em></li>
</ul>

                ]]></content:encoded>
            </item>
        
            <item>
                <title>Stream processing, Event sourcing, Reactive, CEP… and making sense of it all</title>
                <link>http://martin.kleppmann.com/2015/01/29/stream-processing-event-sourcing-reactive-cep.html</link>
                <comments>http://martin.kleppmann.com/2015/01/29/stream-processing-event-sourcing-reactive-cep.html#disqus_thread</comments>
                <pubDate>Thu, 29 Jan 2015 00:00:00 +0000</pubDate>
                <dc:creator>Martin Kleppmann</dc:creator>
                
                    <guid isPermaLink="true">http://martin.kleppmann.com/2015/01/29/stream-processing-event-sourcing-reactive-cep.html</guid>
                
                <description><![CDATA[ This is an edited transcript of a talk I gave at /dev/winter 2015. It was originally published on the Confluent blog. Some people call it stream processing. Others call it Event Sourcing or CQRS. Some even call it Complex Event Processing. Sometimes, such self-important buzzwords are just smoke and mirrors,... ]]></description>
                <content:encoded><![CDATA[
                    <p><em>This is an edited transcript of a
<a href="https://martin.kleppmann.com/2015/01/24/stream-processing-at-dev-winter.html">talk</a> I gave at
<a href="http://devcycles.net/2015/winter/">/dev/winter 2015</a>. It was originally published
<a href="http://blog.confluent.io/2015/01/29/making-sense-of-stream-processing/">on the Confluent blog</a>.</em></p>

<p><em>Some people call it stream processing. Others call it Event Sourcing or CQRS. Some even call it
Complex Event Processing. Sometimes, such self-important buzzwords are just smoke and mirrors,
invented by companies who want to sell you stuff. But sometimes, they contain a kernel of wisdom
which can really help us design better systems.</em></p>

<p><em>In this talk, we will go in search of the wisdom behind the buzzwords. We will discuss how event
streams can help make your application more scalable, more reliable and more maintainable. Founded
in the experience of building large-scale data systems at LinkedIn, and implemented in open source
projects like Apache Kafka and Apache Samza, stream processing is finally coming of age.</em></p>

<p><img src="/2015/01/stream-01.png" alt="Title: making sense of stream processing" width="550" height="412" /></p>

<p>In this presentation, I’m going to discuss some of the ideas that people have about processing event
streams. The idea of structuring data as a stream of events is nothing new, but I’ve recently
noticed this idea reappearing in many different places, often with different terminology and
different use cases, but with the same underlying principles.</p>

<p>The problem when a technique becomes fashionable is that people start generating a lot of hype and
buzzwords around it, often reinventing ideas that are already commonplace in a different field, but
using different words to describe the same thing. In this talk I’m going to try to cut through some
of the hype and jargon in the general area of stream processing, and try to get down to the
underlying fundamental ideas.</p>

<p>Although the jargon can be off-putting when you first encounter it, there’s no need to be scared.
A lot of the ideas are quite simple when you get down to the core. Also, there are a lot of good
ideas which are worth learning about, because they can help us build applications better.</p>

<p><img src="/2015/01/stream-02.png" alt="Overview of event-based technologies" width="550" height="412" /></p>

<p>The notion of processing events appears in many different areas, which is really confusing at first.
People in different fields use different vocabulary to refer to the same thing. I think this is
mainly because the techniques originated in different communities of people, and people seem to
often stick within their own community and not look at what their neighbours are doing.</p>

<p>The current tools for distributed stream processing have come out of internet companies like
LinkedIn, with philosophical roots in database research of the early 2000s. On the other hand,
complex event processing (CEP) originated in
<a href="http://complexevents.com/stanford/rapide/">event simulation research</a> in the 1990s and is now used
for operational purposes in enterprises. Event sourcing has its roots in the domain-driven design
community, which deals with enterprise software development — people who have to work with very
complex data models, but often smaller datasets than the internet companies.</p>

<p>My background is in internet companies, but I’m doing my best to understand the jargon of the other
communities, and figure out the commonalities and differences. If I’ve misunderstood something,
please correct me.</p>

<p><img src="/2015/01/stream-03.png" alt="Stream processing (analytics)" width="550" height="202" /></p>

<p>To make this concrete, I’m going to start by giving an example from the field of stream processing,
specifically analytics. I’ll then draw parallels with other areas.</p>

<p><img src="/2015/01/stream-04.png" alt="Google Analytics screenshot" width="550" height="447" /></p>

<p>To start with, take something like Google Analytics. As you probably know, Google Analytics is a bit
of Javascript that you can put on your website, and that keeps track of which pages have been viewed
by which visitors. An administrator can then explore this data, breaking it down by time period, or
by URL, and so on.</p>

<p>How would you implement something like Google Analytics?</p>

<p>First take the input to the system. Every time a user views a page, we need to log an event to
record that fact. A page view event may look something like this (using a kind of pseudo-JSON):</p>

<p><img src="/2015/01/stream-05.png" alt="PageViewEvent example" width="550" height="323" /></p>

<p>A page view has an event type (PageViewEvent), a Unix timestamp that indicates when the event
happened, the IP address of the client, the session ID (this may be a unique identifier from
a cookie, which allows you to figure out what series of page views is from the same person), the URL
of the page that was viewed, how the user got to that page (for example, from a search engine, or
clicking a link from another site), the user’s browser and language settings, and so on.</p>

<p>Note that each page view event is a simple, immutable fact. It simply records that something
happened.</p>

<p>Now, how do you go from these page view events to the nice graphical dashboard on which you can
explore how people are using your website?</p>

<p><img src="/2015/01/stream-06.png" alt="Use cases for events vs. materialized aggregates" width="550" height="412" /></p>

<p>Broadly speaking, you have two options.</p>

<p><strong>Option (a):</strong> you can simply store every single event as it comes in, dump them all in a big
database, a data warehouse or a Hadoop cluster. Now, whenever you want to analyze this data in some
way, you run a big SELECT query against this dataset. For example, you might group by URL and by
time period, you might filter by some condition, and then COUNT(*) to get the number of page views
for each URL over time. This will scan over essentially all the events, or at least some large
subset, and do the aggregation on the fly.</p>

<p><strong>Option (b):</strong> if storing every single event is too much for you, you can instead store an
aggregated summary of the events. For example, if you’re counting things, you increment a few
counters every time an event comes in, and then you throw away the actual event. You might keep
several counters in something that’s called an <a href="http://arxiv.org/pdf/cs/0701155.pdf">OLAP cube</a>:
imagine a multi-dimensional cube, where one dimension is the URL, another dimension is the time of
the event, another dimension is the browser, and so on. For each event, you just need to increment
the counters for that particular URL, that particular time, etc.</p>

<p>With an OLAP cube, when you want to find the number of page views for a particular URL on
a particular day, you just need to read the counter for that combination of URL and date. You don’t
have to scan over a long list of events; it’s just a matter of reading a single value.</p>

<p><img src="/2015/01/stream-07.png" alt="Raw event data vs. materialized aggregates" width="550" height="412" /></p>

<p>Now option (a) may sound a bit crazy, but it actually works surprisingly well. I believe Google
Analytics actually does store the raw events, or at least a large sample of events, and performs
a big scan over those events when you look at the data. Modern analytic databases have become really
good at scanning quickly over large amounts of data.</p>

<p>The big advantage of storing raw event data is that you have maximum flexibility for analysis. For
example, you can trace the sequence of pages that one person visited over the course of their
session. You can’t do that if you’ve squashed all the events into counters. That sort of analysis is
really important for some offline processing tasks, such as training a recommender system (“people
who bought X also bought Y”, that sort of thing). For such use cases, it’s best to simply keep all
the raw events, so that you can later feed them all into your shiny new machine learning system.</p>

<p>However, option (b) also has its uses, especially when you need to make decisions or react to things
in real time. For example, if you want to prevent people from scraping your website, you can
introduce a rate limit, so that you only allow 100 requests per hour from any particular IP address;
if a client goes over the limit, you block it. Implementing that with raw event storage would be
incredibly inefficient, because you’d be continually re-scanning your history of events to determine
whether someone has gone over the limit. It’s much more efficient to just keep a counter of number
of page views per IP address per time window, and then you can check on every request whether that
number has crossed your threshold.</p>

<p>Similarly, for alerting purposes, you need to respond quickly to what the events are telling you.
For stock market trading, you also need to be quick.</p>

<p>The bottom line here is that raw event storage and aggregated summaries of events are both very
useful. They just have different use cases.</p>

<p><img src="/2015/01/stream-08.png" alt="Streaming aggregation by directly incrementing a counter from the app" width="550" height="277" /></p>

<p>Let’s focus on those aggregated summaries for now. How do you implement those?</p>

<p>Well, in the simplest case, you simply have the web server update the aggregates directly. Say you
want to count page views per IP address per hour, for rate limiting purposes. You can keep those
counters in something like memcached or Redis, which have an atomic increment operation. Every time
a web server processes a request, it directly sends an increment command to the store, with a key
that is constructed from the client IP address and the current time (truncated to the nearest hour).</p>

<p><img src="/2015/01/stream-09.png" alt="Streaming aggregation via event stream" width="550" height="412" /></p>

<p>If you want to get a bit more sophisticated, you can introduce an event stream, or a message queue,
or an event log, or whatever you want to call it. The messages on that stream are the PageViewEvent
records that we saw earlier: one message contains the properties of one particular page view.</p>

<p>The nice thing about this architecture is that you can now have multiple consumers for the same
event data. You can have one consumer which simply archives the raw events to some big storage; even
if you don’t yet have the capability to process the raw events, you might as well store them, since
storage is cheap and you can use them in future. Then you can have another consumer which does some
aggregation (for example, incrementing counters), and another consumer which does something else.
Those can all feed off the same event stream.</p>

<p><img src="/2015/01/stream-10.png" alt="Title: event sourcing" width="550" height="272" /></p>

<p>Let’s now change topic for a moment, and look at similar ideas from a different field. Event
sourcing is an idea that has come out of the domain-driven design community — it seems to be fairly
well known amongst enterprise software developers, but it’s totally unknown in internet companies.
It comes with a large amount of jargon that I find very confusing, but there seem to be some very
good ideas in event sourcing.</p>

<p>So I’m going to try to extract those good ideas without going into all the jargon, and we’ll see
that there are some surprising parallels with my last example from the field of stream processing
analytics.</p>

<p><img src="/2015/01/stream-11.png" alt="Shopping cart example" width="550" height="412" /></p>

<p>Event sourcing is concerned with how we structure data in databases. As example database I’m going
to use a shopping cart from an e-commerce website. Each customer may have some number of different
products in their cart at one time, and for each item in the cart there is a quantity. Nothing very
complicated here.</p>

<p><img src="/2015/01/stream-12.png" alt="Shopping cart example: updating quantity" width="550" height="412" /></p>

<p>Now say that customer 123 updates their cart: instead of quantity 1 of product 999, they now want
quantity 3 of that product. You can imagine this being recorded in the database using an UPDATE
query, which matches the row for customer 123 and product 999, and modifies that row, changing the
quantity from 1 to 3.</p>

<p>This example uses a relational data model, but that doesn’t really matter. With most non-relational
databases you’d do more or less the same thing: overwrite the old value with the new value when it
changes.</p>

<p>However, event sourcing says that this isn’t a good way to design databases. Instead, we should
individually record every change that happens to the database.</p>

<p><img src="/2015/01/stream-13.png" alt="Shopping cart example: change event log" width="550" height="412" /></p>

<p>For example, we first record an AddToCart event when customer 123 first adds product 888 to their
cart, with quantity 1. We then record a separate UpdateCartQuantity event when they change the
quantity to 3. Later, the customer changes their mind again, and reduces the quantity to 2, and
finally they go to the checkout. Each of these actions is recorded as a separate event, and appended
to the database. You can imagine having a timestamp on every event too.</p>

<p><img src="/2015/01/stream-14.png" alt="No to state mutation, yes to immutable commands" width="550" height="412" /></p>

<p>When you structure the data like this, every change to the shopping cart is an immutable event —
a fact. Even if the customer did change the quantity to 2, it is still true that at a previous point
in time, the selected quantity was 3. If you overwrite data in your database, you lose this historic
information. Keeping the list of all changes as a log of immutable events thus gives you strictly
richer information than if you overwrite things in the database.</p>

<p>And this is really the essence of event sourcing: rather than performing destructive state mutation
on a database when writing to it, we should record every write as a “command”, as an immutable
event.</p>

<p><img src="/2015/01/stream-15.png" alt="History of what happened vs. current state" width="550" height="412" /></p>

<p>And that brings us back to our stream processing example (Google Analytics). Remember we discussed
two options for storing data: (a) raw events, or (b) aggregated summaries.</p>

<p>What we have here with event sourcing is looking very similar. You can think of those event-sourced
commands (AddToCart, UpdateCartQuantity) as raw events: they comprise the history of what happened
over time. But when you’re looking at the contents of your shopping cart, you see the its current
state — the end result, which is what you get when you have applied the entire history of events and
squashed them together into one thing.</p>

<p>So the current state of the cart may say quantity 2. The history of raw events will tell you that at
some previous point in time the quantity was 3, but that the customer later changed their mind and
updated it to 2. The aggregated end result only tells you that the current quantity is 2.</p>

<p><img src="/2015/01/stream-16.png" alt="Events = writes, aggregates = reads" width="550" height="412" /></p>

<p>Thinking about it further, you can observe that the raw events are the form in which it’s ideal to
write the data: all the information in the database write is contained in a single blob. You don’t
need to go and update five different tables if you’re storing raw events — you only need to append
the event to the end of a log. That’s the simplest and fastest possible way of writing to
a database.</p>

<p>On the other hand, the aggregated data is the form in which it’s ideal to read data from the
database. If a customer is looking at the contents of their shopping cart, they are not interested
in the entire history of modifications that led to the current state — they only want to know what’s
in the cart right now. So when you’re reading, you can get the best performance if the history of
changes has already been squashed together into a single object representing the current state.</p>

<p><img src="/2015/01/stream-17.png" alt="Events = buttons, aggregates = screens" width="550" height="412" /></p>

<p>Going even further, think about the user interfaces that lead to database writes and database reads.
A database write typically happened because the user clicked some button: for example, they edited
some data, and now they click the save button. So, buttons in the user interface correspond to raw
events in the event sourcing history.</p>

<p>On the other hand, a database read typically happens because the user views some screen: they click
on some link or open some document, and now they need to read the contents. These reads typically
want to know the current state of the database. So screens in the user interface correspond to
aggregated state.</p>

<p>This is quite an abstract idea, so let me go through a few examples.</p>

<p><img src="/2015/01/stream-18.png" alt="Twitter example: input = tweet, output = timeline" width="550" height="412" /></p>

<p>Take Twitter, for example. The most common way of writing to Twitter’s database — i.e. to provide
input into the Twitter system — is to tweet something. A tweet is very simple: it consists of some
text, a timestamp, and the ID of the user who tweeted. (Perhaps optionally a location, or a photo,
or something.) The user then clicks that “Tweet” button, which causes a database write to happen,
i.e. an event is generated.</p>

<p>On the output side, the way how you read from Twitter’s database is by viewing your timeline. It
shows all the stuff that was written by people you follow. It’s a vastly more complicated structure:</p>

<p><img src="/2015/01/stream-19.png" alt="Twitter example: input and output data" width="550" height="412" /></p>

<p>For each tweet, you now have not just the text, timestamp and user ID, but also the name of the
user, their profile photo, and other information that has been joined with the tweet. Also, the list
of tweets has been selected based on the people you follow, which may itself change.</p>

<p>How would you go from the simple input to the more complex output? Well, you could try expressing it
in SQL, something like this:</p>

<p><img src="/2015/01/stream-20.png" alt="Twitter example: query to generate timeline" width="550" height="412" /></p>

<p>That is, find all of the users who $user is following, find all the tweets that they have written,
order them by time and pick the 100 most recent. Turns out this query really doesn’t scale very
well. Do you remember in the early days of Twitter, when they kept having the fail whale all the
time? <a href="http://www.infoq.com/presentations/Twitter-Timeline-Scalability">Apparently</a> that was
essentially because they were using something like the query above.</p>

<p>When a user views their timeline, it’s too expensive to iterate over all the people they are
following to get those users’ tweets. Instead, Twitter must compute a user’s timeline ahead of time,
and cache it, so that it’s fast when a user looks at it. To do that, they need a process that
translates from the write-optimized event (a single tweet) to the read-optimized aggregate (a
timeline). Twitter has such a process, and calls it the
<a href="http://www.infoq.com/presentations/Twitter-Timeline-Scalability">fanout service</a>.</p>

<p><img src="/2015/01/stream-21.png" alt="Facebook example: input = like button, output = post" width="550" height="412" /></p>

<p>Another example: take Facebook. It has many buttons that enable you to write something to Facebook’s
database, but a classic one is the “Like” button. When you click it, that generates an event, a fact
with a very simple structure: <em>you</em> (identified by your user ID) <em>like</em> (an action verb) <em>some item</em>
(identified by its ID).</p>

<p>However, if you look at the output side, reading something on Facebook, it’s incredibly complicated.
In this example we have a Facebook post which is not just some text, but also the name of the author
and his profile photo; and it’s telling me that 160,216 people like this update, of which three have
been especially highlighted (presumably because Facebook thinks that amongst the likers of this
update, these are the ones I am most likely to know); it’s telling me that there are 6,027 shares
and 12,851 comments, of which the top 4 comments are shown (clearly some kind of comment ranking is
happening here); and so on.</p>

<p><img src="/2015/01/stream-22.png" alt="Facebook example: input and output data" width="550" height="412" /></p>

<p>There must be some translation process happening here, which takes the very simple events as input,
and produces the massively complex and personalized output structure. You can’t even conceive what
the database query would look like to fetch all the information in that Facebook update. There is no
way they could efficiently query all of that on the fly, not with over 100,000 likes. Clever caching
is absolutely essential if you want to build something like this.</p>

<p><img src="/2015/01/stream-23.png" alt="Events = source of truth, derived data can be denormalized" width="550" height="412" /></p>

<p>From the Twitter and Facebook examples we can see a certain pattern: the input events, corresponding
to the buttons in the user interface, are quite simple. They are immutable facts, we can simply
store them all, and we can treat them as the <em>source of truth</em>.</p>

<p>Everything that you can see on a website, i.e. everything that you read from the database, can be
derived from those raw events. There is a process which derives those aggregates from the raw
events, and which updates the caches when new events come in, and that process is entirely
deterministic. You could, if necessary, re-run it from scratch: if you feed in the entire history of
everything that ever happened on the site, you can reconstruct every cache entry to be exactly as it
was before. The database you read from is just a
<a href="http://blogs.msdn.com/b/pathelland/archive/2007/06/14/accountants-don-t-use-erasers.aspx">cached view of the event log</a>.</p>

<p>The beautiful thing about this separation between source of truth and caches is that in your caches,
you can denormalize data to your heart’s content. In regular databases, it is often considered best
practice to normalize data, because if something changes, you then only have to change it one place.
Normalization makes writes fast and simple, but means you have to do more work (joins) at read time.</p>

<p>In order to speed up reads, you can denormalize data, i.e. duplicate information in various places
so that it can be read faster. The problem is now that if the original data changes, all the places
where you copied it to also need to change. In a typical database, that’s a nightmare, because you
may not know all the places where something has been copied. But if your caches are built from your
raw events using a repeatable process, you have much more freedom to denormalize, because you know
what data is flowing where.</p>

<p><img src="/2015/01/stream-24.png" alt="Wikipedia example: input = wiki markup, output = wiki page" width="550" height="412" /></p>

<p>Another example: Wikipedia. This is almost a counter-example to Twitter and Facebook, because on
Wikipedia the input and the output are almost the same.</p>

<p><img src="/2015/01/stream-25.png" alt="Wikipedia example: input and output are almost the same" width="550" height="412" /></p>

<p>When you edit a page on Wikipedia, you get a big text field containing the entire page content
(using wiki markup), and when you click the save button, it sends that entire page content back to
the server. The server replaces the entire page with whatever you posted to it. When someone views
the page, it returns that same content back to the user (formatted into HTML).</p>

<p>So, in this case, the input and the output are the same. What would event sourcing mean in this
case? Would it perhaps make sense to represent a write event as a diff, like a patch file, rather
than a copy of the entire page? It’s an interesting case to think about. (Google Docs works by
continually applying diffs at a character granularity — effectively an event per keystroke.)</p>

<p><img src="/2015/01/stream-26.png" alt="LinkedIn example: input = profile edit, output = search index" width="550" height="412" /></p>

<p>Final example: LinkedIn. Say you update your LinkedIn profile, and add your current job, which
consists of a job title, a company, and some text. Again, the edit event for writing to the database
is very simple.</p>

<p>There are various ways how you can read this data, and in this example, let’s look at the search
feature. On the output side, one way you can read LinkedIn’s database is by typing some keywords and
maybe a company name into a search box, and find all the people matching those criteria.</p>

<p><img src="/2015/01/stream-27.png" alt="LinkedIn example: profile edit vs. search index" width="550" height="412" /></p>

<p>How is that implemented? Well, in order to search, you need a full-text index, which is essentially
a big dictionary — for every keyword, it tells you the IDs of all the profiles that contain the
keyword. This search index is another aggregate structure, and whenever some data is written to the
database, this structure needs to be updated with the new data.</p>

<p>So for example, if I add my job “Author at O’Reilly” to my profile, the search index must now be
updated to include my profile ID under the entries for “author” and “o’reilly”. The search index is
just another kind of cache. It also needs to be built from the source of truth (all the profile
edits that have ever occurred) and it needs to be updated whenever a new event occurs (someone edits
their profile).</p>

<p><img src="/2015/01/stream-09.png" alt="Streaming aggregation via event stream" width="550" height="412" /></p>

<p>Now, returning to stream processing. I first described how you might build something like Google
Analytics, and compared storing raw page view events versus aggregated counters, and discussed how
you can maintain those aggregates by consuming a stream of events.</p>

<p><img src="/2015/01/stream-29.png" alt="Stream processing as data integration" width="550" height="412" /></p>

<p>I then explained event sourcing, which applies a similar approach to databases: treat all the
database writes as a stream of events, and build aggregates (views, caches, search indexes) from
that stream. Once you have that event stream, you can do many great things with it:</p>

<ul>
  <li>You can take all the raw events, perhaps transform them a bit, and load them into a big data
warehouse where analysts can query the data to their heart’s content.</li>
  <li>You can update full-text search indexes, so that when a user hits the search box, they are
searching an up-to-date version of the data.</li>
  <li>You can invalidate or refill any caches, so that reads can be served from fast caches while also
making sure that the data in the cache remains fresh.</li>
  <li>And finally, you can even take one event stream, and process it in some way (perhaps joining a few
streams together) to create a new output stream. This way, you can plug the output of one system
into the input of another system. This is a very powerful way of building complex applications
cleanly.</li>
</ul>

<p><img src="/2015/01/stream-30.png" alt="Advantages of event sourcing" width="550" height="412" /></p>

<p>Moving to an event-sourcing-like approach for databases is a big change from the way that databases
have traditionally been used (where you can update and delete data at will). Why would you want to
go to all that effort of changing the way you do things? What’s the benefit of using append-only
streams of immutable events?</p>

<p>Several reasons:</p>

<ul>
  <li><strong>Loose coupling.</strong> If you write data to the database in the same schema as you use for reading,
you have tight coupling between the part of the application doing the writing (the “button”) and
the part doing the reading (the “screen”). We know that loose coupling is a good design principle
for software. By separating the form in which you write and read data, and by explicitly
translating from one to the other, you get much looser coupling between different parts of your
application.</li>
  <li><strong>Read and write performance.</strong> The decades-old debate over normalization (faster writes) vs.
denormalization (faster reads) exists only because of the assumption that writes and reads use the
same schema. If you separate the two, you can have fast writes <em>and</em> fast reads.</li>
  <li>Event streams are great for <strong>scalability</strong>, because they are a simple abstraction (comparatively
easy to parallelize and scale across multiple machines), and because they allow you to decompose
your application into producers and consumers of streams (which can operate independently, and can
take advantage of more parallelism in hardware).</li>
  <li><strong>Flexibility and agility.</strong> Raw events are so simple and obvious that a “schema migration”
doesn’t really make sense (you may just add a new field from time to time, but you don’t usually
need to rewrite historic data into a new format). On the other hand, the ways in which you want to
present data to users are much more complex, and may be continually changing. If you have an
explicit translation process between the source of truth and the caches that you read from, you
can experiment with new user interfaces by just building new caches using new logic, running the
new system in parallel with the old one, gradually moving people over from the old system, and
then discarding the old system (or reverting to the old system if the new one didn’t work). Such
flexibility is incredibly liberating.</li>
  <li>Finally, <strong>error scenarios</strong> are much easier to reason about if data is immutable. If something
goes wrong in your system, you can always replay events in the same order, and
<a href="http://martinfowler.com/articles/lmax.html">reconstruct exactly what happened</a> (especially
important in finance, where auditability is crucial). If you deploy buggy code that writes bad
data to a database, you can just re-run it after you fixed the bug, and thus correct the outputs.
Those things are not possible if your database writes are destructive.</li>
</ul>

<p><img src="/2015/01/stream-31.png" alt="Title: tools" width="550" height="193" /></p>

<p>Finally, let’s talk about how you might put these ideas into practice.</p>

<p>I should point out that in reality, database writes often already do have a certain event-like
<a href="http://www.cidrdb.org/cidr2015/Papers/CIDR15_Paper16.pdf">immutable quality</a>. The write-ahead log
that exists in most databases is essentially an event stream of writes, although it’s very specific
to a particular database. The MVCC mechanism in databases like PostgreSQL, MySQL’s InnoDB and
Oracle, and the append-only B-trees of CouchDB, Datomic and LMDB are examples of the same thinking:
it’s better to structure writes as an append-only log than to perform destructive overwrites.</p>

<p>However, here we’re not talking about the internals of storage engines, but about using event
streams at the application level.</p>

<p><img src="/2015/01/stream-32.png" alt="Kafka and Samza" width="550" height="412" /></p>

<p>Some databases such as <a href="http://geteventstore.com/">Event Store</a> have oriented themselves
specifically at the event sourcing model, and some people have implemented event sourcing on top of
relational databases. Those may be viable solutions if you’re operating at fairly small scale.</p>

<p>The systems I have worked with most are <a href="http://kafka.apache.org/">Apache Kafka</a> and
<a href="http://samza.apache.org/">Apache Samza</a>. They are open source projects that originated at LinkedIn,
and now have a big community around them. Kafka is a message broker, like a publish-subscribe
message queue, which supports event streams with many millions of messages per second, durably
stored on disk and replicated across multiple machines.</p>

<p>Samza is the processing counterpart to Kafka: a framework which lets you write code to consume input
streams and produce output streams, and it handles stuff like deploying your code to a cluster and
recovering from failures.</p>

<p><img src="/2015/01/stream-33.png" alt="Distributed stream processing frameworks overview" width="550" height="412" /></p>

<p>I would definitely recommend Kafka as a system for high-throughput reliable event streams. On the
processing side, there are a few choices: <a href="http://samza.apache.org/">Samza</a>,
<a href="http://storm.apache.org/">Storm</a> and <a href="https://spark.apache.org/streaming/">Spark Streaming</a> are the
most popular stream processing frameworks. They all allow you to run your stream processing code
distributed across multiple machines.</p>

<p>There are interesting design differences (pros and cons) between these three frameworks, which
I don’t have time to go into here. You can read up a detailed comparison between them in the
<a href="http://samza.apache.org/learn/documentation/0.8/comparisons/introduction.html">Samza documentation</a>.
And yes, I also think it’s funny that they all start with the letter S.</p>

<p>Today’s distributed stream processing systems have come out of internet companies (Samza from
LinkedIn, Storm from Backtype/Twitter). Arguably, they have their roots in stream processing
research from the early 2000s (<a href="http://db.csail.mit.edu/madden/html/TCQcidr03.pdf">TelegraphCQ</a>,
<a href="http://cs.brown.edu/research/borealis/public/">Borealis</a>, etc), which originated from a relational
database background. Just as NoSQL datastores stripped databases down to a minimal feature set,
modern stream processing systems look quite stripped-down compared to the earlier research.</p>

<p><img src="/2015/01/stream-34.png" alt="Stream query engines" width="550" height="412" /></p>

<p>The modern stream processing frameworks (Samza, Storm, Spark Streaming) are mostly concerned with
low-level matters: how to scale processing across multiple machines, how to deploy a job to
a cluster, how to handle faults (crashes, machine failures, network outages), and how to achieve
reliable performance in a
<a href="https://twitter.com/jaykreps/status/528235702480142336">multi-tenant environment</a>. The APIs they
provide are quite low-level (e.g. a callback that is invoked for every message). They look much more
like MapReduce and less like a database. They are more interested in reliable operation than in
fancy features.</p>

<p>By contrast, there is also some work on high-level query languages for stream processing,
and Complex Event Processing is especially worth mentioning. It originated in 1990s research on
<a href="http://www.complexevents.com/2006/08/01/what%E2%80%99s-the-difference-between-esp-and-cep/">event-driven simulation</a>,
and most CEP products are commercial, expensive enterprise software (only
<a href="http://esper.codehaus.org/">Esper</a> is free/open source, and it’s limited to running on a single
machine).</p>

<p>With CEP, you write queries or rules that match certain patterns in the events. They are comparable
to SQL queries (which describe what results you want to return from a database), except that the CEP
engine continually searches the stream for sets of events that match the query and notifies you
(generates a “complex event”) whenever a match is found. This is useful for fraud detection or
monitoring business processes, for example.</p>

<p>For use cases that can be easily described in terms of a CEP query language, such a high-level
language is much more convenient than a low-level event processing API. On the other hand,
a low-level API gives you more freedom, allowing you to do a wider range of things than a query
language would let you do. Also, by focussing their efforts on scalability and fault tolerance,
stream processing frameworks provide a solid foundation upon which query languages can be built.</p>

<p>A related idea is doing full-text search on streams, where you register a search query in advance,
and then get notified whenever an event matches your query. We’ve done some
<a href="https://github.com/romseygeek/samza-luwak">experimental work</a> with
<a href="https://github.com/flaxsearch/luwak">Luwak</a> in this area, but it’s still very new.</p>

<p><img src="/2015/01/stream-35.png" alt="Event-based technologies" width="550" height="412" /></p>

<p>Finally, there are a lot of other things that are somehow related to stream processing. Going into
laundry-list mode, here is a brief summary:</p>

<ul>
  <li><strong>Actor frameworks</strong> such as Akka, Orleans and Erlang OTP are also based on streams of immutable
events. However, they are primarily a mechanism for concurrency, less a mechanism for data
management. Some actor frameworks do have a distributed component, so you could build
a distributed stream processing framework on top of actors. However, it’s worth looking carefully
at the fault-tolerance guarantees and failure modes of these systems: many don’t provide
durability, for example.</li>
  <li>There’s a lot of buzz around “<strong>reactive</strong>”, which seems to encompass a quite loosely-defined
<a href="http://www.reactivemanifesto.org/">set of ideas</a>. My impression is that there is some good work
happening in dataflow languages and functional reactive programming (FRP), which I see as mostly
about bringing event streams to the user interface, i.e. updating the user interface when some
underlying data changes. This is a natural counterpart to event streams in the data backend.</li>
  <li>Finally, <strong>change data capture</strong> (CDC) means using an existing database in the familiar way, but
extracting any inserts, updates and deletes into a stream of data change events which other
applications can consume. This is a great migration path towards a stream-oriented architecture,
and I’ll be talking and writing more about CDC in future.</li>
</ul>

<p>I hope this talk helped you make some sense of the many facets of stream processing!</p>

<p><em><a href="https://martin.kleppmann.com/">Martin Kleppmann</a> is a software engineer and entrepreneur. He is
a committer on <a href="http://samza.apache.org/">Apache Samza</a> and author of the upcoming O’Reilly book
<a href="http://dataintensive.net/">Designing Data-Intensive Applications</a>. He’s
<a href="https://twitter.com/martinkl">@martinkl</a> on Twitter.</em></p>

                ]]></content:encoded>
            </item>
        
            <item>
                <title>Wouldn’t it be fun to build your own Google?</title>
                <link>http://martin.kleppmann.com/2014/12/10/build-your-own-google.html</link>
                <comments>http://martin.kleppmann.com/2014/12/10/build-your-own-google.html#disqus_thread</comments>
                <pubDate>Wed, 10 Dec 2014 00:00:00 +0000</pubDate>
                <dc:creator>Martin Kleppmann</dc:creator>
                
                    <guid isPermaLink="true">http://martin.kleppmann.com/2014/12/10/build-your-own-google.html</guid>
                
                <description><![CDATA[ Exploring open web crawl data. What if you had your own copy of the entire web, and you could do with it whatever you want? This article was originally published on O’Reilly Radar. For the last few millennia, libraries have been the custodians of human knowledge. By collecting books, and... ]]></description>
                <content:encoded><![CDATA[
                    <p><strong>Exploring open web crawl data. What if you had your own copy of the entire web, and you could do
with it whatever you want?</strong></p>

<p><em>This article was originally published on
<a href="http://radar.oreilly.com/2014/12/wouldnt-it-be-fun-to-build-your-own-google.html">O’Reilly Radar</a>.</em></p>

<p>For the last few <a href="http://en.wikipedia.org/wiki/Library_of_Alexandria">millennia</a>, libraries have
been the custodians of human knowledge. By collecting books, and making them findable and
accessible, they have done an incredible service to humanity. Our modern society, culture, science,
and technology are all founded upon ideas that were transmitted through books and libraries.</p>

<p>Then the web came along, and allowed us to also publish all the stuff that wasn’t good enough to put
in books, and do it all much faster and cheaper. Although the average quality of material you find
on the web is quite poor, there are some pockets of excellence, and in aggregate, the sum of all web
content is probably even more amazing than all libraries put together.</p>

<p>Google (and a few brave contenders like Bing, Baidu, DuckDuckGo and Blekko) have kindly indexed it
all for us, acting as the web’s librarians. Without search engines, it would be terribly difficult
to actually find anything, so hats off to them. However, what comes next, after search engines? It
seems unlikely that search engines are the last thing we’re going to do with the web.</p>

<h2 id="what-if-you-had-your-own-web-crawl">What if you had your own web crawl?</h2>

<p>A small number of organizations, including Google, have crawled the web, processed, and indexed it,
and generated a huge amount of value from it. However, there’s a problem: those indexes, the result
of crawling the web, are hidden away inside Google’s data centers. We’re allowed to make individual
search queries, but we don’t have bulk access to the data.</p>

<p>Imagine you had your own copy of the entire web, and you could do with it whatever you want. (Yes,
it would be very expensive, but we’ll get to that later.) You could do automated analyses and
surface the results to users. For example, you could collate the “best” articles (by some
definition) written on many different subjects, no matter where on the web they are published. You
could then create a tool which, whenever a user is reading something about one of those subjects,
suggests further reading: perhaps deeper background information, or a contrasting viewpoint, or an
argument on why the thing you’re reading is full of shit.</p>

<p>(I’ll gloss over the problem of actually implementing those analyses. The signal-to-noise ratio on
the web is terrible, so it’s difficult to determine algorithmically whether a particular piece of
content is any good. Nevertheless, search engines are able to give us useful search results because
they spend a huge amount of effort on spam filtering and other measures to improve the quality of
results. Any product that uses web crawl data will have to decide what is noise, and get rid of it.
However, you can’t even start solving the signal-to-noise problem until you have the raw data.
Having crawled the web is step one.)</p>

<p>Unfortunately, at the moment, only Google and a small number of other companies that have crawled
the web have the resources to perform such analyses and build such products. Much as I believe
Google try their best to be neutral, a pluralistic society requires a diversity of voices, not
a filter bubble controlled by one organization. Surely there are people outside of Google who want
to work on this kind of thing. Many a start-up could be founded on the basis of doing useful things
with data extracted from a web crawl.</p>

<h2 id="the-web-link-graph">The web link graph</h2>

<p>The idea of collating several related, useful pieces of content on one subject was recently
<a href="https://medium.com/@justpw/its-time-to-rethink-the-link-66d32ff0e2e2">suggested</a> by
<a href="http://fugit.co/">Justin Wohlstadter</a> (indeed it was a discussion with Justin that inspired me to
write this article). His start-up, <a href="http://wayfinder.is/">Wayfinder</a>, aims to create such
cross-references between URLs, by way of human curation. However, it relies on users actively
submitting links to Wayfinder’s service.</p>

<p>I argued to Justin that I shouldn’t need to submit anything to a centralized database. By writing
a blog post (such as this one) that references some things on the web, I am implicitly creating
a connection between the URLs that appear in this blog post. By linking to those URLs, I am
implicitly suggesting that they might be worth reading. (Of course, this is an
<a href="http://ilpubs.stanford.edu:8090/422/1/1999-66.pdf">old idea</a>.) The web is already, in a sense,
a huge distributed database.</p>

<p>By analogy, citations are very important in scientific publishing. Every scientific paper uses
references to acknowledge its sources, to cite prior work, and to make the reader aware of related
work. In the opposite direction, the number of times a paper is cited by other authors is a metric
of how important the work is, and citation counts have even become a
<a href="http://en.wikipedia.org/wiki/H-index">metric</a> for researchers’ careers.</p>

<p><a href="http://scholar.google.com/">Google Scholar</a> and
<a href="http://citeseer.ist.psu.edu/">bibliography databases</a> maintain an index of citations, so you can
find later work that builds upon (or invalidates!) a particular paper. As a researcher, following
those forward and backward citations is an important way of learning about the state of the art.</p>

<p>Similarly, if you want to analyze the web, you need to need to be able to traverse the link graph
and see which pages link to each other. For a given URL, you need to be able to see which pages it
links to (outgoing links) and which pages link to it (incoming links).</p>

<p>You can easily write a program that fetches the HTML for a given URL, and parses out all the links
– in other words, you can easily find all the outgoing links for an URL. Unfortunately, finding all
the incoming links is very difficult. You need to download every page on the entire web, extract all
of their links, and then collate all the web pages that reference the same URL. You need a copy of
the entire web.</p>

<p><img src="/2014/12/incoming-outgoing-links.png" width="550" height="430" /></p>

<p><em>Incoming and outgoing links for a URL. Finding the outgoing ones is easy (just parse the page);
finding the incoming ones is hard.</em></p>

<h2 id="publicly-available-crawl-data">Publicly available crawl data</h2>

<p>An interesting move in this direction is <a href="http://commoncrawl.org/">CommonCrawl</a>, a nonprofit. Every
couple of months they send a crawler out into the web, download a whole bunch of web pages (about
2.8 billion pages, at latest count), and store the result as a publicly available data set in S3.
The data is in <a href="http://en.wikipedia.org/wiki/Web_ARChive">WARC format</a>, and you can do whatever you
want with it. (If you want to play with the data, I wrote an
<a href="https://github.com/ept/warc-hadoop">implementation of WARC</a> for use in Hadoop.)</p>

<p>As an experiment, I wrote a simple MapReduce job that processed the entire CommonCrawl data set. It
cost me about $100 in EC2 instance time to process all 2.8 billion pages (a bit of optimization
would probably bring that down). Crunching through such quantities of data isn’t free, but it’s
surprisingly affordable.</p>

<p>The CommonCrawl data set is about 35 TB in size (unparsed, compressed HTML). That’s a lot, but
Google says they crawl 60 trillion distinct pages, and the index is
<a href="http://www.google.com/insidesearch/howsearchworks/thestory/">reported</a> as being over 100 PB, so
it’s safe to assume that the CommonCrawl data set represents only a small fraction of the web.</p>

<p><img src="/2014/12/dataset-size.png" width="550" height="311" /></p>

<p><em>Relative sizes of CommonCrawl, Internet Archive, and Google (sources:
<a href="http://blog.commoncrawl.org/2014/09/august-2014-crawl-data-available/">1</a>,
<a href="https://blog.archive.org/2013/01/09/updated-wayback/">2</a>,
<a href="http://www.google.com/insidesearch/howsearchworks/thestory/">3</a>).</em></p>

<p>CommonCrawl is a good start. But what would it take to create a publicly available crawl of the
entire web? Is it just a matter of getting some generous donations to finance CommonCrawl? But if
all the data is on S3, you have to either use EC2 to process it or pay Amazon for the bandwidth to
download it. A long-term solution would have to be less AWS-centric.</p>

<p>I don’t know for sure, but my gut instinct is that a full web crawl would best be undertaken as
a decentralized effort, with many organizations donating some bandwidth, storage, and computing
resources toward a shared goal. (Perhaps this is what <a href="http://www.faroo.com/hp/p2p/p2p.html">Faroo</a>
and <a href="http://yacy.net/en/Technology.html">YaCy</a> are doing, but I’m not familiar with the details of
their systems.)</p>

<h2 id="an-architectural-sketch">An architectural sketch</h2>

<p>Here are some rough ideas on how a decentralized web crawl project could look.</p>

<p>The participants in the crawl can communicate peer-to-peer, using something like BitTorrent.
A distributed hash table can be used to assign a portion of the URL space to a participant. That
means each URL is assigned to one or more participants, and that participant is in charge of
fetching the URL, storing the response, and parsing any links that appear in the page. Every URL
that is found in a link is sent to the crawl participant to whom the URL is assigned. The recipient
can ignore that message if it has already fetched that URL recently.</p>

<p>The system will need to ensure it is well-behaved as a whole (obey robots.txt, stay within rate
limits, de-duplicate URLs that return the same content, etc.). This will require some coordination
between crawl participants. However, even if the crawl was done by a single organization, it would
have to be distributed across multiple nodes, probably using asynchronous message passing for loose
coordination. The same principles apply if the crawl nodes are distributed across several
participants – it just means the message-passing is across the Internet rather than within one
organization’s data center.</p>

<p><img src="/2014/12/distributed-crawl.png" width="550" height="413" /></p>

<p><em>A hypothetical architecture for distributed web crawling. Each participant crawls and stores
a portion of the web.</em></p>

<p>There remain many questions. What if your crawler downloads some content that is illegal in your
country? How do you keep crawlers honest (ensuring they don’t manipulate the crawl results to their
own advantage)? How is the load balanced across participants with different amounts of resources to
offer? Is it necessary to enforce some kind of reciprocity (you can only use crawl data if you also
contribute data), or have a payment model (bitcoin?) to create an incentive for people to run
crawlers? How can index creation be distributed across participants?</p>

<p>(As an aside, I think <a href="http://samza.incubator.apache.org/">Samza</a>’s model of stream computation
would be a good starting point for implementing a scalable distributed crawler. I’d love to see
someone implement a proof of concept.)</p>

<h2 id="motivations-for-contributing">Motivations for contributing</h2>

<p>Why would different organizations – many of them probably competitors – potentially collaborate on
creating a public domain crawl data set? Well, there is precedence for this, namely in open source
software.</p>

<p>Simplifying for the sake of brevity, there are a few reasons why this model works well:</p>

<ul>
  <li><strong>Cost:</strong> Creating and maintaining a large software project (eg. Hadoop, Linux kernel, database
system) is very expensive, and only a small number of very large companies can afford to run
a project of that size by themselves. As a mid-size company, you have to either buy an
off-the-shelf product from a vendor or collaborate with other organizations in creating an open
solution.</li>
  <li><strong>Competitive advantage:</strong> With infrastructure software (databases, operating systems) there is
little competitive advantage in keeping a project proprietary because competitive differentiation
happens at the higher levels (closer to the user interface). On the other hand, by making it open,
everybody benefits from better software infrastructure. This makes open source a very attractive
option for infrastructure-level software.</li>
  <li><strong>Public relations:</strong> Companies want to be seen as doing good, and contributing to open source is
seen as such. Many engineers also want to work on open source, perhaps for idealistic reasons, or
because it makes their skills and accomplishments publicly visible and recognized, including to
prospective future employers.</li>
</ul>

<p>I would argue that all the same arguments apply to the creation of an open data set, not only to the
creation of open source software. If we believe that there is enough value in having publicly
accessible crawl data, it looks like it could be done.</p>

<h2 id="perhaps-we-can-make-it-happen">Perhaps we can make it happen</h2>

<p>What I’ve described is a pie in the sky right now (although CommonCrawl is totally real).</p>

<p>Collaboratively created data sets such as Wikipedia and OpenStreetMap are an amazing resource and
accomplishment. At first, people thought the creators of these projects were crazy, but they turned
out to work very well. We can safely say they have made a positive impact on the world, by
summarizing a certain subset of human knowledge and making it freely accessible to all.</p>

<p>I don’t know if freely available web crawl data would be similarly valuable because it’s hard to
imagine all the possible applications, which only arise when you actually have the data and start
exploring it. However, there must be interesting things you can do if you have access to the
collective outpourings of humanity. How about <a href="http://web.stanford.edu/dept/SUL/library/extra4/sloan/MouseSite/1968Demo.html">augmenting human
intellect</a>, which
we’ve talked about for so long? Can we use this data to create fairer societies, better mutual
understanding, better education, and such good things?</p>

                ]]></content:encoded>
            </item>
        
            <item>
                <title>Hermitage: Testing the “I” in ACID</title>
                <link>http://martin.kleppmann.com/2014/11/25/hermitage-testing-the-i-in-acid.html</link>
                <comments>http://martin.kleppmann.com/2014/11/25/hermitage-testing-the-i-in-acid.html#disqus_thread</comments>
                <pubDate>Tue, 25 Nov 2014 00:00:00 +0000</pubDate>
                <dc:creator>Martin Kleppmann</dc:creator>
                
                    <guid isPermaLink="true">http://martin.kleppmann.com/2014/11/25/hermitage-testing-the-i-in-acid.html</guid>
                
                <description><![CDATA[ tl;dr: I have created a test suite for comparing the transaction isolation levels in different databases. I did this as background research for my book. This post explains why. What is isolation? First came the NoSQL movement, with its siren call that our systems could be so scalable, so much... ]]></description>
                <content:encoded><![CDATA[
                    <p><em>tl;dr:</em> I have created a <a href="https://github.com/ept/hermitage">test suite</a> for comparing the
transaction isolation levels in different databases. I did this as background research for
<a href="http://dataintensive.net/">my book</a>. This post explains why.</p>

<h2 id="what-is-isolation">What is isolation?</h2>

<p>First came the NoSQL movement, with its siren call that our systems could be so scalable, so much
faster and so highly available if we just abandon ACID transactions and make our systems
<a href="http://queue.acm.org/detail.cfm?id=1394128">BASE</a> instead.</p>

<p>Then came the concurrency bugs — for example, the Bitcoin exchange that was
<a href="https://bitcointalk.org/index.php?topic=499580">almost bankrupted</a> because it had a race condition
on outgoing payments. (An attacker circumvented the account balance check by making many concurrent
transactions, and thus was able withdraw more money than they had in their account. And this
<a href="http://www.reddit.com/r/Bitcoin/comments/1wtbiu/how_i_stole_roughly_100_btc_from_an_exchange_and/">wasn’t even</a>
the only case.)</p>

<p>Internet commenters, in their infinite wisdom, were quick to point out that if you’re dealing with
money, you had <a href="https://twitter.com/kellabyte/status/452982674626711552">better use an ACID database</a>.
But there was a major flaw in their argument. Most so-called ACID databases — for example Postgres,
MySQL, Oracle or MS SQL Server — would <em>not</em> have prevented this race condition in their default
configuration.</p>

<p>Yes, you read that right. Those databases — the ones that have probably processed the vast majority
of commercial transactions over the last 20 years — do not by default guarantee that your
transactions are protected from race conditions. Let me explain.</p>

<h2 id="the-story-of-weak-isolation">The story of weak isolation</h2>

<p>Among the ACID properties, the letter I stands for <em>isolation</em>. The idea of isolation is that we
want our database to be able to process several transactions at the same time (otherwise it would be
terribly slow), but we don’t want to have to worry about concurrency (because it’s terribly
complicated). In an ideal world, the database could guarantee that transactions behave as if they
executed without any concurrency, one after another, <em>serially</em> — in other words, they are
<em>serializable</em>.</p>

<p>Unfortunately, most implementations of serializability have quite bad performance. The team working
on the first SQL database (<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.84.348&amp;rep=rep1&amp;type=pdf">System R</a>)
already <a href="http://citeseer.ist.psu.edu/viewdoc/download?doi=10.1.1.92.8248&amp;rep=rep1&amp;type=pdf">realised this in 1975</a>,
and decided to offer weaker isolation levels than serializability. Those isolation levels would not
quite prevent all race conditions, but they had much better performance, so it was considered an
acceptable trade-off. That research group made up some names for those weak isolation levels
(“repeatable read”, “read committed”, and “read uncommitted”). 39 years later, some implementation
details have changed, but on the whole isolation levels still look
<a href="http://www.bailis.org/blog/when-is-acid-acid-rarely/">remarkably similar</a> to System R.</p>

<p>The problem, however, is this: in order to understand what those isolation levels mean, you pretty
much have to understand how the database implements concurrency control internally. The levels
basically have no intuitive meaning, because they are an incredibly
<a href="http://www.joelonsoftware.com/articles/LeakyAbstractions.html">leaky abstraction</a> — they are
defined by implementation detail, not because someone thought they’d make a great API. And that’s
also why they are so hard to understand.</p>

<h2 id="understanding-weak-isolation">Understanding weak isolation</h2>

<p>Here’s a challenge for you: can you find any colleague in your organisation who can explain — off
the cuff and without looking at the docs — the difference between “repeatable read” and “read
committed”, including an example situation in which they behave differently? And what about the
difference between “repeatable read”, “snapshot isolation” and “serializable”?</p>

<p>If you can find such a person, I bet they worked on database systems previously. I can’t imagine
there are many application developers who really understand isolation levels, because they are so
confusing. Application developers are smart, but they have more important things to worry about
than obscure implementation details of databases.</p>

<p>That’s a big problem. The isolation level is part of the database’s API. It’s just as important as
the data model. People have
<a href="http://www.sarahmei.com/blog/2013/11/11/why-you-should-never-use-mongodb/">endless debates</a> about
relational vs. document vs. other data models, but I’ve never overheard a heated debate about
isolation levels — only seen slightly embarrassed smirks signifying “I really ought to know more
about this, but I don’t”.</p>

<p>If you don’t understand the concurrency guarantees, you have no idea whether your code will still
behave correctly when processing a few simultaneous requests. You can’t easily write unit tests for
concurrency, either. How do you know your code is correct?</p>

<h2 id="a-rigorous-system-for-comparing-isolation-levels">A rigorous system for comparing isolation levels</h2>

<p>Fortunately, some academic researchers have been on the case, creating formal models of isolation
levels and proving some of their properties. For example, <a href="http://www.bailis.org/">Peter Bailis</a> et
al. at <a href="http://db.cs.berkeley.edu/">Berkeley</a> have been doing some
<a href="http://www.bailis.org/blog/understanding-weak-isolation-is-a-serious-problem/">good work</a> in this
area. It’s not yet easy to understand, but at least it’s logically sound. </p>

<p><img src="/2014/11/isolation-levels.png" width="550" alt="Hierarchy of isolation levels" /></p>

<p>You may have seen this diagram from their <a href="http://arxiv.org/pdf/1302.0309.pdf">VLDB paper</a>, which
shows the relative strength of different isolation levels, and also relates them to availability (as
per the <a href="http://henryr.github.io/cap-faq/">CAP theorem</a>). It has also appeared in <a href="http://aphyr.com/">Kyle Kingsbury</a>’s
“<a href="https://www.youtube.com/watch?v=QdkS6ZjeR7Q">Linearizable Boogaloo</a>” talk (around 8:30). In this
diagram, 1SR = serializability, RR = repeatable read, SI = snapshot isolation, RC = read committed.</p>

<p>Unfortunately, even though terms like <em>repeatable read</em> sound like they should be standardised (and
indeed they are defined in the ANSI SQL standard), in practice different databases interpret them
differently. One database’s <em>repeatable read</em> guarantees things that another doesn’t. Not only are
the names of the isolation levels confusing, they are also inconsistent.</p>

<p>If you read the documentation of a database, the description of the isolation levels tends to be
quite vague. How do you know what guarantees your application actually needs, and thus which
isolation level you should use? How do you learn to look at a piece of code and say “this won’t be
safe to run at <em>read committed</em>, it’ll need at least <em>repeatable read</em>”? Even better, could we write
automated tests or type checkers which fail if we write code that is not concurrency-safe?</p>

<p>If we want any hope of reasoning about concurrency safety, we first need to understand exactly which
guarantees existing databases do and don’t provide. We need to express those guarantees in terms of
precisely defined, testable properties (not vague English sentences in the documentation). Research
on isolation has produced those precise definitions, but as far as I know they so far haven’t been
systematically tested on actual database implementations.</p>

<h2 id="introducing-hermitage">Introducing Hermitage</h2>

<p><a href="https://github.com/ept/hermitage">Hermitage</a> is a small project that I started to address this. It
is a test suite for databases which probes for a variety of concurrency issues, and thus allows
a fair and accurate comparison of isolation levels. Each test case simulates a particular kind of
race condition that can happen when two or more transactions concurrently access the same data. Each
test can pass (if the database’s implementation of isolation prevents the race condition from
occurring) or fail (if the race condition does occur).</p>

<p>The tests are fairly direct translations of the anomalies described in research papers. The
properties that they check have quite obscure names (such as Predicate-Many-Preceders) and
abbreviations (such as P4), but rather than inventing even more terminology I thought I’d rather
stick with the terms in the literature.</p>

<p>So far I have ported and run this test suite on four relational databases: PostgreSQL, MySQL, Oracle
DB and Microsoft SQL Server. But it’s not limited to old-school relational databases: in fact, it
would be very interesting to run it with some of the recent “NewSQL” databases that
<a href="https://foundationdb.com/acid-claims">claim to support ACID transactions</a>. The whole point of this
test suite is that it allows an exact comparison not just between isolation levels within one
database, but across different databases.</p>

<p>For the details please <a href="https://github.com/ept/hermitage">check out the repository</a>. To mention
just a few highlights:</p>

<ul>
  <li>PostgreSQL, MySQL and MS SQL Server all boast an isolation level called “repeatable read”, but
it means something different in every one of them. Yay for standards.</li>
  <li>On the other hand, “read committed” has the same safety properties in all databases I’ve tested.</li>
  <li>Oracle “serializable” is actually not serializable at all, but snapshot isolation. (This has been
<a href="http://www.researchgate.net/publication/220225203_Making_snapshot_isolation_serializable/file/e0b49520567eace81f.pdf">documented</a>
<a href="http://www.bailis.org/papers/hat-hotos2013.pdf">previously</a>.)</li>
  <li>SQL Server’s lock-based isolation modes (the default) are remarkably similar to System R in 1975,
which seems pretty terrible to me. If you use SQL Server in production, I’d be interested to hear
whether you’ve configured it to use MVCC instead (allow_snapshot_isolation on, and/or
read_committed_snapshot on).</li>
</ul>

<h2 id="why">Why?</h2>

<p>In the book I’m writing (<a href="http://dataintensive.net/">Designing Data-Intensive Applications</a>) I’m
determined to explain transaction isolation in a way that we can all understand. But in order to
write that chapter, I first had to understand isolation levels myself. </p>

<p>So Hermitage was primarily background research for the book, allowing me to write an
easy-to-understand but <em>accurate</em> description of what databases do in practice. If you find
Hermitage incredibly confusing, don’t worry: the book doesn’t go into all the detail of G1b, PMP,
P4 and G2-item. I first had to make it complicated for myself so that I could make it simple
(but accurate) for readers.</p>

<p>However, going beyond this book, I’m hoping that this little project can encourage others to
explore how application developers interact with isolation. Simply cataloguing what databases
actually do, using test cases rather than vague langauge, is a step forward.</p>

<p>And perhaps some enterprising researcher could take this further. For example, perhaps it’s
possible to create a type system for transactions, which would allow
<a href="http://www.vldb.org/conf/2007/papers/industrial/p1263-jorwekar.pdf">static analysis</a> of an
application to verify that it doesn’t contain concurrency bugs when run at a particular isolation
level. I don’t know enough about type systems to know whether that’s even possible – but if yes, it
could potentially cut out a whole class of bugs that are very hard to find through testing.</p>

<p><em>Thank you to Peter Bailis and his collaborators for feedback on a draft of this post.</em></p>

                ]]></content:encoded>
            </item>
        
            <item>
                <title>Hey, I'm writing a book!</title>
                <link>http://martin.kleppmann.com/2014/09/15/writing-a-book.html</link>
                <comments>http://martin.kleppmann.com/2014/09/15/writing-a-book.html#disqus_thread</comments>
                <pubDate>Mon, 15 Sep 2014 00:00:00 +0000</pubDate>
                <dc:creator>Martin Kleppmann</dc:creator>
                
                    <guid isPermaLink="true">http://martin.kleppmann.com/2014/09/15/writing-a-book.html</guid>
                
                <description><![CDATA[ About two years ago I wrote a blog post called “Rethinking caching in web apps”. At almost 4,000 words, it was a lot longer than the received wisdom says a blog post should be. Nevertheless I had the feeling that I was only scratching the surface of what needed to... ]]></description>
                <content:encoded><![CDATA[
                    <p>About two years ago I wrote a blog post called
<a href="/2012/10/01/rethinking-caching-in-web-apps.html">“Rethinking caching in web apps”</a>.
At almost 4,000 words, it was a lot longer than the
<a href="http://blog.bufferapp.com/the-ideal-length-of-everything-online-according-to-science">received wisdom</a>
says a blog post should be. Nevertheless I had the feeling that I was only scratching
the surface of what needed to be said.</p>

<p>That got me thinking whether I should try writing something longer, like a book perhaps.
I love writing because it forces me to research something in depth, think it through,
and then try to explain it in a logical way. That helps me understand it much better
than if I just casually read about it. Or, put more eloquently:</p>

<blockquote>
  <p>“Writing is nature’s way of letting you know how sloppy your thinking is.”
– Dick Guindon</p>
</blockquote>

<h2 id="existing-books">Existing books</h2>

<p>I am writing because the book I wanted to read didn’t exist. I wanted a book that
would explain data systems to me – the whole area of databases, distributed systems,
batch and stream processing, consistency, caching and indexing – at the right level.
But I found that almost all the existing books, blog posts etc. fell into one of the
following categories:</p>

<ol>
  <li>Most computing books are hands-on guides to one particular technology. They assume that
you’ve been told to use database X or programming language Y, and so they teach you
how to use it. Those books are fine, but they are of little use if you’re trying to
decide whether X or Y is the right tool for you in the first place. These books tend to
focus on the strong points of that particular technology, and fail to mention its
shortcomings.</li>
  <li>It’s common to see blog posts with side-by-side comparisons of several similar
technologies, but I find they tend to just focus on superficial aspects (performance
benchmarks, API, software license) while completely missing the fundamental workings
of the technology. They are like Top Trumps for databases, and don’t actually help you
understand anything any better.</li>
  <li>By contrast, academic textbooks cover the fundamental principles and trade-offs that
are common to many different technologies, but in doing so, they often lose all
contact with reality. These books are generally written by academics with deep research
experience in their field, but little awareness of the practicalities of real
production systems. They often end up saying things which are technically correct,
but useless or misleading if you want to actually build a real system.</li>
</ol>

<p>I wanted something in between all of these. A book which would tell a story of the big ideas in
data systems, the fundamental principles which don’t change from one software version to
another. But the book would also stay grounded in reality, explaining what works in practice
and what doesn’t, and <em>why</em>. The book would examine the tools and systems that we already
use in production, compare their fundamental approaches, and help you figure out which
technology is appropriate to which use case.</p>

<p>I wanted to understand not just how to <em>use</em> a particular system, but also how it <em>works under
the hood</em>. That is partly out of intellectual curiosity, but equally importantly, because it
allows me to imagine what the system is doing. If some kind of unexpected behaviour occurs, or
if I want to push the limits of what a technology can do, it is tremendously useful to have
at least a rough idea of what is happening internally.</p>

<p>As I spoke to various people about these ideas, including some folks at O’Reilly, it became
clear that I wasn’t the only one who wanted a book like this. And so,
<a href="http://dataintensive.net/">Designing Data-Intensive Applications</a> was born.
And you’ll know it when you see it, because it has an awesome Indian Wild Boar on the cover.</p>

<p style="text-align: center"><a href="http://dataintensive.net" title="Designing Data-Intensive Applications (the wild boar book)"><img src="/2014/09/book-cover.png" alt="Designing Data-Intensive Applications (the wild boar book)" width="250" height="328" /></a>
</p>

<p><em>Designing Data-Intensive Applications</em> (sorry about the verbose title – you can just call it
“the wild boar book”) has been in the works for some time, and today we’re announcing the
<a href="http://shop.oreilly.com/product/0636920032175.do">early release</a>. The first four chapters
are now available – ten or eleven are planned in total, so there’s still long way to go.
But I left my job to work on this book full-time, so it’s definitely happening.</p>

<h2 id="who-should-read-this">Who should read this?</h2>

<p>If you’re a software engineer working on server-side applications (a web application backend,
for instance), then this book is for you. It assumes that you already know how to build an
application and use a database, and that you want to “level up” in your craft. Perhaps you
want to work on highly scalable systems with millions of users, perhaps you want to deal with
particularly complex or ever-changing data, or perhaps you want to make an old legacy
environment more agile.</p>

<p>This book starts at the foundations, and gradually builds up a picture of modern data systems
layer by layer, one chapter at a time. I’m not trying to sell you any particular architecture
or approach, because I firmly believe that different use cases require different solutions.
Therefore, each chapter contains a broad overview and comparison of the different approaches
that have been successful in different circumstances.</p>

<p>It doesn’t matter what your preferred programming language or framework is – this book is
agnostic. It’s about architecture and algorithms, about fundamental principles and practical
constraints, about the reasoning behind every design decision.</p>

<p>None of the ideas in this book are really new, and indeed many ideas are decades old.
Everything has already been said somewhere, in conference presentations, research papers,
blog posts, code, bug trackers, and engineering folklore. However, to my knowledge the ideas
haven’t previously been collected, compared and evaluated like this.</p>

<p>I hope that by understanding what our options are, and the pros and cons of each approach,
we’ll all become better engineers. By making conscious trade-offs and choosing our tools
wisely, we will build systems that are more reliable and much easier to maintain in the long
run. It’s a quest to help us engineers be better at our jobs, and build better software.</p>

<h2 id="lets-make-software-development-better">Let’s make software development better</h2>

<p>Please join me on this quest by reading the draft of the book, and sending us your feedback:</p>

<ul>
  <li>The book’s website is <a href="http://dataintensive.net/">dataintensive.net</a>.</li>
  <li>You can buy the early release ebook from
<a href="http://shop.oreilly.com/product/0636920032175.do">O’Reilly</a>, or if you’re a Safari Books
subscriber, you can <a href="http://my.safaribooksonline.com/9781491903063">read it online</a>.</li>
  <li>If you can think of any way the book could be improved, please email your thoughts to
<a href="&#109;&#097;&#105;&#108;&#116;&#111;:&#102;&#101;&#101;&#100;&#098;&#097;&#099;&#107;&#064;&#100;&#097;&#116;&#097;&#105;&#110;&#116;&#101;&#110;&#115;&#105;&#118;&#101;&#046;&#110;&#101;&#116;">&#102;&#101;&#101;&#100;&#098;&#097;&#099;&#107;&#064;&#100;&#097;&#116;&#097;&#105;&#110;&#116;&#101;&#110;&#115;&#105;&#118;&#101;&#046;&#110;&#101;&#116;</a> or tweet us
<a href="https://twitter.com/intensivedata">@intensivedata</a>. Now is the time to be involved.</li>
  <li>…and if you like the sound of this, don’t forget to tell your friends and colleagues
about it!</li>
</ul>

                ]]></content:encoded>
            </item>
        
            <item>
                <title>Upcoming conference talks about Samza</title>
                <link>http://martin.kleppmann.com/2014/08/28/upcoming-conference-talks-about-samza.html</link>
                <comments>http://martin.kleppmann.com/2014/08/28/upcoming-conference-talks-about-samza.html#disqus_thread</comments>
                <pubDate>Thu, 28 Aug 2014 00:00:00 +0000</pubDate>
                <dc:creator>Martin Kleppmann</dc:creator>
                
                    <guid isPermaLink="true">http://martin.kleppmann.com/2014/08/28/upcoming-conference-talks-about-samza.html</guid>
                
                <description><![CDATA[ After my talk about Samza fault tolerance at Berlin Buzzwords was well received a few months ago, I submitted several more talk proposals to a variety of conferences. To my surprise, all the proposals were accepted, so I’m now going to have a fairly busy time in the next few... ]]></description>
                <content:encoded><![CDATA[
                    <p>After my <a href="https://www.youtube.com/watch?v=d63kSjxVsGA&amp;index=11&amp;list=PLq-odUc2x7i-Q5gQtkmba4ov37XRPjp6n">talk about Samza fault tolerance</a>
at <a href="http://berlinbuzzwords.de/">Berlin Buzzwords</a> was well received a few months ago,
I submitted several more talk proposals to a variety of conferences. To my surprise,
all the proposals were accepted, so I’m now going to have a fairly busy time in the
next few months!</p>

<p>Here are the four conferences at which I’ll be speaking between September and November.
All the talks are about <a href="http://samza.incubator.apache.org/">Apache Samza</a>, the stream
processing project I’ve been working on. However, all the talks are different, each
focussing on a different aspect and perspective.</p>

<p>If you don’t yet have a ticket for these conferences, there are a few discount codes
below. Hope to see you there :-)</p>

<p><a href="https://thestrangeloop.com/sessions/turning-the-database-inside-out-with-apache-samza"><strong>Turning the database inside out with Apache Samza</strong></a><br />
<a href="https://thestrangeloop.com/">Strange Loop</a>, September 18–19 in St. Louis, Missouri.
(<a href="http://lanyrd.com/2014/strangeloop/">Lanyrd</a>, <a href="https://twitter.com/strangeloop_stl">Twitter</a>)</p>

<p>The Strange Loop conference explores the future of software development from a wonderfully
eclectic range of viewpoints, ranging from functional programming to distributed systems.
In this talk I’ll discuss the potential of stream processing as a fundamental programming
model, which has big advantages compared to the way we usually build applications today.</p>

<p><a href="http://strataconf.com/stratany2014/public/schedule/detail/36045"><strong>Building real-time data products at LinkedIn with Apache Samza</strong></a><br />
<a href="http://strataconf.com/stratany2014">Strata + Hadoop World</a>, October 15–17 in New York.
(<a href="http://lanyrd.com/2014/strata-new-york/">Lanyrd</a>, <a href="https://twitter.com/strataconf">Twitter</a>)<br />
Use discount code SPEAKER20 to get 20% off.</p>

<p>MapReduce and its cousins are powerful tools for building data products such as recommendation
engines, detecting anomalies and improving relevance. However, with batch processing there may
be several hours delay before new data is reflected in the output. With stream processing, you
can potentially respond in seconds rather than hours, but you have to learn a whole new way of
thinking in order to write your jobs. In this talk I’ll discuss some real-life examples of
stream processing at LinkedIn, and show how to use Samza to solve real-time data problems.</p>

<p><a href="http://london-2014.spanconf.io/martin-kleppmann/"><strong>Staying agile in the face of the data deluge</strong></a><br />
<a href="http://spanconf.io">Span conference</a>, October 28 in London, UK.
(<a href="http://lanyrd.com/2014/spanconf/">Lanyrd</a>, <a href="https://twitter.com/spanconf">Twitter</a>)<br />
Use <a href="https://ti.to/span/london-2014?discount_code=kleppmann">this link</a> to get a 20% discount.</p>

<p>An often-overlooked but important aspect of tools is their <em>plasticity</em>: if your
application’s requirements change, how easily do the tools let you adapt your existing
code and data to the new requirements? Samza is designed with plasticity in mind. In
this talk I’ll discuss how re-processing of data streams can keep your application
development agile.</p>

<p><a href="http://apacheconeu2014.sched.org/event/3633e195715f88c3357749d57b7b3b8c"><strong>Scalable stream processing with Apache Samza and Apache Kafka</strong></a><br />
<a href="http://events.linuxfoundation.org/events/apachecon-europe">ApacheCon Europe</a>, November 17–21 in Budapest, Hungary.
(<a href="http://lanyrd.com/2014/apachecon-europe/">Lanyrd</a>, <a href="https://twitter.com/apachecon">Twitter</a>)</p>

<p>Many of the most important open source data infrastructure tools are projects of the
Apache Software Foundatation: Hadoop, Zookeeper, Storm and Spark, to name just a few.
In this talk I’ll focus on how Samza and Kafka (also Apache projects) fit into this
lively open source ecosystem.</p>

<p><strong>Background reading</strong></p>

<p>If you don’t yet know about Samza, don’t worry: I’ll start each talk with a quick
introduction to Samza, and not assume any prior knowledge.</p>

<p>But if you want to ask smart-ass questions and embarrass me in front of the audience, you
can begin by reading the Samza
<a href="http://samza.incubator.apache.org/learn/documentation/latest/">documentation</a>
(thoroughly updated over the last few months by yours truly), and start thinking of
particularly tricky questions to ask.</p>

<p>You may also be interested in this excellent series of articles by
<a href="https://twitter.com/jaykreps">Jay Kreps</a>, which are relevant to the upcoming talks:</p>

<ul>
  <li><a href="http://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying">The Log: What every software engineer should know about real-time data’s unifying abstraction</a></li>
  <li><a href="http://radar.oreilly.com/2014/07/questioning-the-lambda-architecture.html">Questioning the Lambda Architecture</a></li>
  <li><a href="http://radar.oreilly.com/2014/07/why-local-state-is-a-fundamental-primitive-in-stream-processing.html">Why local state is a fundamental primitive in stream processing</a> – What do you get if you cross a distributed database with a stream processing system?</li>
</ul>

                ]]></content:encoded>
            </item>
        
            <item>
                <title>Six things I wish we had known about scaling</title>
                <link>http://martin.kleppmann.com/2014/03/26/six-things-about-scaling.html</link>
                <comments>http://martin.kleppmann.com/2014/03/26/six-things-about-scaling.html#disqus_thread</comments>
                <pubDate>Wed, 26 Mar 2014 00:00:00 +0000</pubDate>
                <dc:creator>Martin Kleppmann</dc:creator>
                
                    <guid isPermaLink="true">http://martin.kleppmann.com/2014/03/26/six-things-about-scaling.html</guid>
                
                <description><![CDATA[ Looking back at the last few years of building Rapportive and LinkedIn Intro, I realised that there were a number of lessons that we had to learn the hard way. We built some reasonably large data systems, and there are a few things I really wish we had known beforehand.... ]]></description>
                <content:encoded><![CDATA[
                    <p>Looking back at the last few years of building <a href="https://rapportive.com/">Rapportive</a> and
<a href="http://engineering.linkedin.com/mobile/linkedin-intro-doing-impossible-ios">LinkedIn Intro</a>,
I realised that there were a number of lessons that we had to learn the hard way. We built some
reasonably large data systems, and there are a few things I really wish we had known beforehand.</p>

<p>None of these lessons are particularly obscure – they are all well-documented, if you know where to
look. They are the kind of things that made me think <em>“I can’t believe I didn’t know that, I’m so
stupid #facepalm”</em> in retrospect. But perhaps I’m not the only one who started out not knowing these
things, so I’ll write them down for the benefit of anyone else who finds themself having to scale
a system. </p>

<p>The kind of system I’m talking about is the data backend of a consumer web/mobile app with a million
users (order of magnitude). At the scale of Google, LinkedIn, Facebook or Twitter (hundreds of
millions of users), you’ll have an entirely different set of problems, but you’ll also have a bigger
team of experienced developers and operations people around you. The mid-range scale of about
a million users is interesting, because it’s quite feasible for a small startup team to get there
with some luck and good marketing skills. If that sounds like you, here are a few things to keep in
mind.</p>

<h2 id="realistic-load-testing-is-hard">1. Realistic load testing is hard</h2>

<p>Improving the performance of a system is ideally a very scientific process. You have in your head
a model of what your system is doing, and a theory of where the expensive operations are. You
propose a change to the system, and predict what the outcome will be. Then you make the change,
observe the system’s behaviour under laboratory conditions, and thus gather evidence which either
confirms or contradicts your theory. That way you iterate your way to a better theory, and also
a better-performing implementation.</p>

<p>Sadly, we hardly ever managed to do it that way in practice. If we were optimising a microbenchmark,
running the same code a million times in a tight loop, it would be easy. But we are dealing with
large volumes of data, spread out across multiple machines. If you read the same item a million
times in a loop, it will simply be cached, and the load test tells you nothing. If you want
meaningful results, the load test needs to simulate a realistically large working set, a realistic
mixture of reads and writes, realistic distribution of requests over time, and so on. And that is
difficult.</p>

<p>It’s difficult enough to simply <em>know</em> what your access patterns actually are, let alone simulate
them. As a starting point, you can replay a few hours worth of access logs against a copy of your
real dataset. However, that only really works for read requests. Simulating writes is harder, as
you may need to account for business logic rules (e.g. a sequential workflow must first update A,
then update B, then update C) and deal with changes that can happen only once (if your write changes
state from D to E, you can’t change from D to E again later in the test, as you’re already in state
E). That means you have to synchronise your access logs with your database snapshot, or somehow
generate suitable synthetic write load.</p>

<p>Even harder if you want to test with a dataset that is larger than the one you actually have (so
that you can find out what happens when you double your userbase, and prepare for that event). Now
you have to work out the statistical properties of your dataset (the distribution of friends per
user is a power law with x parameters, the correlation between one user’s number of friends and the
number of friends that their friends have is y, etc) and generate a synthetic dataset with those
parameters. You are now in deep, deep yak shaving territory. Step back from that yak.</p>

<p>In practice, it hardly ever works that way. We’re lucky if, sometimes, we can run the old code and
the new code side-by-side, and observe how they perform in comparison. Often, not even that is
possible. Usually we often just cross our fingers, deploy, and roll back if the change seems to have
made things worse. That is deeply unsatisfying for a scientifically-minded person, but it more or
less gets the job done.</p>

<h2 id="data-evolution-is-difficult">2. Data evolution is difficult</h2>

<p>Being able to rapidly respond to change is one of the biggest advantages of a small startup. Agility
in product and process means you also need the freedom to change your mind about the structure of
your code and your data. There is lot of talk about making code easy to change, eg. with good
automated tests. But what about changing the structure of your data?</p>

<p>Schema changes have a reputation of being very painful, a reputation that is chiefly MySQL’s fault:
simply adding a column to a table requires the
<a href="http://dev.mysql.com/doc/refman/5.6/en/alter-table.html">entire table to be copied</a>. On a large
table, that might mean several hours during which you can’t write to the table. Various
<a href="http://www.percona.com/doc/percona-toolkit/2.2/pt-online-schema-change.html">tools</a>
<a href="https://github.com/soundcloud/lhm">exist</a> to make that less painful, but I find it unbelievable
that the world’s most popular open source database handles such a common operation so badly.</p>

<p>Postgres can make simple schema changes without copying the table, which means they are almost
instant. And of course the avoidance of schema changes is a primary selling point of document
databases such as MongoDB (so it’s up to application code to deal with a database that uses
different schemas for different documents). But simple schema changes, such as adding a new field or
two, don’t tell the entire story.</p>

<p>Not all your data is in databases; some might be in archived log files or some kind of blob storage.
How do you deal with changing the schema of that data? And sometimes you need to make complex
changes to the data, such as breaking a large thing apart, or combining several small things, or
migrating from one datastore to another. Standard tools don’t help much here, and document databases
don’t make it any easier.</p>

<p>We’ve written large migration jobs that break the entire dataset into chunks, process chunks
gradually over the course of a weekend, retry failed chunks, track which things were modified while
the migration was happening, and finally catch up on the missed updates. A whole lot of complexity
just for a one-off data migration. Sometimes that’s unavoidable, but it’s heavy lifting that you’d
rather not have to do in the first place.</p>

<p>Hadoop data pipelines can help with this sort of thing, but now you have to set up a Hadoop cluster,
learn how to use it, figure out how to get your data into it, and figure out how to get the
transformed data out to your live systems again. Big companies like LinkedIn have figured out how to
do that, but in a small team it can be a massive time-sink.</p>

<h2 id="database-connections-are-a-real-limitation">3. Database connections are a real limitation</h2>

<p>In PostgreSQL, each client connection to the database is handled by a separate unix process; in
MySQL, each connection uses a separate thread. Both of these models impose a fairly low limit on the
number of connections you can have to the database – typically a few hundred. Every connection adds
overhead, so the entire database slows down, even if those connections aren’t actively processing
queries. For example, Heroku Postgres limits you to 60 connections on the smallest plan, and 500
connections on the <a href="https://devcenter.heroku.com/articles/heroku-postgres-plans#standard-tier">largest plan</a>,
although having anywhere near 500 connections is
<a href="https://postgres.heroku.com/blog/past/2013/11/22/connection_limit_guidance/">actively discouraged</a>.</p>

<p>In a fast-growing app, it doesn’t take long before you reach a few hundred connections. Each
instance of your application server uses at least one. Each background worker process that needs to
access the database uses one. Adding more machines running your application is fairly easy if they
are stateless, but every machine you add means more connections.</p>

<p>Partitioning (sharding) and read replicas probably won’t help you with your connection limit, unless
you can somehow load-balance requests so that all the requests for a particular partition are
handled by a particular server instance. A better bet is to use a
<a href="https://wiki.postgresql.org/wiki/PgBouncer">connection pooler</a>, or to write your own data access
layer which wraps database access behind an internal API.</p>

<p>That’s all doable, but it doesn’t seem a particularly valuable use of your time when you’re also
trying to iterate on product features. And every additional service you deploy is another thing that
can go wrong, another thing that needs to be monitored and maintained.</p>

<p>(Databases that use a lightweight connection model don’t have this problem, but they may have other
problems instead.)</p>

<h2 id="read-replicas-are-an-operational-pain">4. Read replicas are an operational pain</h2>

<p>A common architecture is to designate one database instance as a <em>leader</em> (also known as <em>master</em>)
and to send all database writes to that instance. The writes are then replicated to other database
instances (called <em>read replicas</em>, <em>followers</em> or <em>slaves</em>), and many read-only queries can be
served from the replicas, which takes load off the leader. This architecture is also good for fault
tolerance, since it gives you a <em>warm standby</em> – if your leader dies, you can quickly promote one
of the replicas to be the new leader (you wouldn’t want to be offline for hours while you restore
the database from a backup).</p>

<p>What they don’t tell you is that setting up and maintaining replicas is significant operational
pain. MySQL is particularly bad in this regard: in order to set up a new replica, you have to first
<a href="http://dev.mysql.com/doc/refman/5.6/en/replication-howto-masterstatus.html">lock the leader to stop all writes</a>
and take a consistent snapshot (which may take hours on a large database). How does your app cope if
it can’t write to the database? What do your users think if they can’t post stuff?</p>

<p>With Postgres, you don’t need to stop writes to set up a replica, but it’s still
<a href="http://www.postgresql.org/docs/current/static/warm-standby.html">some hassle</a>. One of the things
I like most about <a href="https://www.heroku.com/postgres">Heroku Postgres</a> is that it wraps all the
complexity of replication and WAL archiving behind a straightforward command-line tool.</p>

<p>Even so, you still need to failover manually if your leader fails. You need to monitor and maintain
the replicas. Your database library may not support read replicas out of the box, so you may need to
add that. Some reads need to be made on the leader, so that a user sees their own writes, even if
there is replication lag. That’s all doable, but it’s additional complexity, and doesn’t add any
value from users’ point of view.</p>

<p>Some distributed datastores such as MongoDB, RethinkDB and Couchbase also use this replication
model, and they automate the replica creation and master failover processes. Just because they do
that doesn’t mean they automatically give you magic scaling sauce, but it is a very valuable
feature.</p>

<h2 id="think-about-memory-efficiency">5. Think about memory efficiency</h2>

<p>At various times, we puzzled about weird latency spikes in our database activity. After many
<a href="http://www.pagerduty.com/">PagerDuty</a> alerts and troubleshooting, it usually turned out that we
could fix the issue by throwing more RAM at the problem, either in the form of a bigger database
instance, or separate caches in front of it. It’s sad, but true: many performance problems can be
solved by simply buying more RAM. And if you’re in a hurry because your hair is on fire, it’s often
the best thing to do. There are limitations to that approach, of course – a m2.4xlarge instance on
EC2 costs quite a bit of money, and eventually there are no bigger machines to turn to. </p>

<p>Besides buying more RAM, an effective solution is to use RAM more efficiently in the first place, so
that a bigger part of your dataset fits in RAM. In order to decide where to optimise, you need to
know what all your memory is being used for – and that’s surprisingly non-trivial. With a bit of
digging, you can usually get your database to report how much disk space each of your tables and
indexes is taking. Figuring out the working set, and how much memory is actually used for what, is
harder.</p>

<p>As a rule of thumb, your performance will probably be more predictable if your indexes completely
fit in RAM – so that there’s a maximum of one disk read per query, which reduces your exposure to
fluctuations in I/O latency. But indexes can get rather large if you have a lot of data, so this can
be an expensive proposition.</p>

<p>At one point we found ourselves reading up about the internal structure of an index in Postgres, and
realised that we could save a few bytes per row by indexing on the hash of a string column rather
than the string itself. (More on that in another post.) That reduced the memory pressure on the
system, and helped keep things ticking along for another few months. That’s just one example of how
it can be helpful to think about using memory efficiently.</p>

<h2 id="change-capture-is-under-appreciated">6. Change capture is under-appreciated</h2>

<p>So far I’ve only talked about things that suck – sorry about the negativity. As final point, I’d
like to mention a technique which is awesome, but not nearly as widely known and appreciated as it
should be: <em>change capture</em>.</p>

<p>The idea of change capture is simple: let the application consume a feed of all writes to the
database. In other words, you have a background process which gets notified every time something
changes in the database (insert, update or delete).</p>

<p>You could achieve a similar thing if, every time you write something to the database, you also post
it to a message queue. However, change capture is better because it contains exactly the same data
as what was committed to the database (avoiding race conditions). A good change capture system also
allows you to stream through the entire existing dataset, and then seamlessly switch to consuming
real-time updates when it has caught up.</p>

<p>Consumers of this changelog are decoupled from the app that generates the writes, which gives you
great freedom to experiment without fear of bringing down the main site. You can use the changelog
for updating and invalidating caches, for maintaining full-text indexes, for calculating analytics,
for sending out emails and push notifications, for importing the data into Hadoop, and
<a href="http://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying">much more</a>.</p>

<p>LinkedIn built a technology called <a href="http://www.socc2012.org/s18-das.pdf?attredirects=0">Databus</a> to
do this. The <a href="https://github.com/linkedin/databus">open source release of Databus</a> is for Oracle DB,
and there is a proof-of-concept <a href="https://github.com/linkedin/databus/wiki/Databus-for-MySQL">MySQL version</a>
(which is different from the version of Databus for MySQL that LinkedIn uses in production).</p>

<p>The new project I am working on, <a href="http://samza.incubator.apache.org/">Apache Samza</a>, also sits
squarely in this space – it is a framework for processing real-time data feeds, somewhat like
MapReduce for streams. I am excited about it because I think this pattern of processing change
capture streams can help many people build apps that scale better, are easier to maintain and more
reliable than many apps today. It’s open source, and you should go and
<a href="http://samza.incubator.apache.org/">try it out</a>.</p>

<h2 id="in-conclusion">In conclusion</h2>

<p>The problems discussed in this post are primarily data systems problems. That’s no coincidence:
if you write your applications in a stateless way, they are pretty easy to scale, since you can
just run more copies of them. Thus, whether you use Rails or Express.js or whatever framework
<em>du jour</em> really doesn’t matter much. The hard part is scaling the stateful parts of your system:
your databases.</p>

<p>There are no easy solutions for these problems. Some new technologies and services can help –
for example, the new generation of distributed datastores tries to solve some of the above problems
(especially around automating replication and failover), but they have other limitations. There
certainly is no panacea.</p>

<p>Personally I’m totally fine with using new and experimental tools for derived data, such as caches
and analytics, where data loss is annoying but not end of your business. I’m more cautious with the
system of record (also known as <em>source of truth</em>). Every system has operational quirks, and the
devil you know may let you sleep better at night than the one you don’t. I don’t really mind what
that devil is in your particular case.</p>

<p>I’m interested to see whether database-as-a-service offerings such as
<a href="https://www.firebase.com/">Firebase</a>, <a href="http://orchestrate.io/">Orchestrate</a> or
<a href="https://fauna.org/">Fauna</a> can help (I’ve not used any of them seriously, so I can’t vouch for them
at this point). I see big potential advantages for small teams in outsourcing operations, but also
a big potential risk in locking yourself to a system that you couldn’t choose to host yourself if
necessary.</p>

<p>Building scalable systems is not all sexy roflscale fun. It’s a lot of plumbing and yak shaving.
A lot of hacking together tools that really ought to exist already, but all the open source
solutions out there are too bad (and yours ends up bad too, but at least it solves your particular
problem).</p>

<p>On the other hand, consider yourself lucky. If you’ve got scaling problems, you must be doing
something right – you must be making something that people want.</p>

                ]]></content:encoded>
            </item>
        
    </channel>
</rss>
