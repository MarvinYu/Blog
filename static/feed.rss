<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0"
    xmlns:content="http://purl.org/rss/1.0/modules/content/"
    xmlns:dc="http://purl.org/dc/elements/1.1/"
    xmlns:atom="http://www.w3.org/2005/Atom"
    xmlns:sy="http://purl.org/rss/1.0/modules/syndication/">

    <channel>
        <title>Martin Kleppmann's blog</title>
        <atom:link href="http://martin.kleppmann.com/feed.rss" rel="self" type="application/rss+xml" />
        <link>http://martin.kleppmann.com/</link>
        <description>Entrepreneurship, web technology and the user experience</description>
        <lastBuildDate>Thu, 29 Jan 2015 16:16:33 GMT</lastBuildDate>
        <language>en</language>
        <sy:updatePeriod>hourly</sy:updatePeriod>
        <sy:updateFrequency>1</sy:updateFrequency>

        
        
            <item>
                <title>Wouldn’t it be fun to build your own Google?</title>
                <link>http://martin.kleppmann.com/2014/12/10/build-your-own-google.html</link>
                <comments>http://martin.kleppmann.com/2014/12/10/build-your-own-google.html#disqus_thread</comments>
                <pubDate>Wed, 10 Dec 2014 00:00:00 +0000</pubDate>
                <dc:creator>Martin Kleppmann</dc:creator>
                
                    <guid isPermaLink="true">http://martin.kleppmann.com/2014/12/10/build-your-own-google.html</guid>
                
                <description><![CDATA[ Exploring open web crawl data. What if you had your own copy of the entire web, and you could do with it whatever you want? This article was originally published on O’Reilly Radar. For the last few millennia, libraries have been the custodians of human knowledge. By collecting books, and... ]]></description>
                <content:encoded><![CDATA[
                    <p><strong>Exploring open web crawl data. What if you had your own copy of the entire web, and you could do
with it whatever you want?</strong></p>

<p><em>This article was originally published on
<a href="http://radar.oreilly.com/2014/12/wouldnt-it-be-fun-to-build-your-own-google.html">O’Reilly Radar</a>.</em></p>

<p>For the last few <a href="http://en.wikipedia.org/wiki/Library_of_Alexandria">millennia</a>, libraries have
been the custodians of human knowledge. By collecting books, and making them findable and
accessible, they have done an incredible service to humanity. Our modern society, culture, science,
and technology are all founded upon ideas that were transmitted through books and libraries.</p>

<p>Then the web came along, and allowed us to also publish all the stuff that wasn’t good enough to put
in books, and do it all much faster and cheaper. Although the average quality of material you find
on the web is quite poor, there are some pockets of excellence, and in aggregate, the sum of all web
content is probably even more amazing than all libraries put together.</p>

<p>Google (and a few brave contenders like Bing, Baidu, DuckDuckGo and Blekko) have kindly indexed it
all for us, acting as the web’s librarians. Without search engines, it would be terribly difficult
to actually find anything, so hats off to them. However, what comes next, after search engines? It
seems unlikely that search engines are the last thing we’re going to do with the web.</p>

<h2 id="what-if-you-had-your-own-web-crawl">What if you had your own web crawl?</h2>

<p>A small number of organizations, including Google, have crawled the web, processed, and indexed it,
and generated a huge amount of value from it. However, there’s a problem: those indexes, the result
of crawling the web, are hidden away inside Google’s data centers. We’re allowed to make individual
search queries, but we don’t have bulk access to the data.</p>

<p>Imagine you had your own copy of the entire web, and you could do with it whatever you want. (Yes,
it would be very expensive, but we’ll get to that later.) You could do automated analyses and
surface the results to users. For example, you could collate the “best” articles (by some
definition) written on many different subjects, no matter where on the web they are published. You
could then create a tool which, whenever a user is reading something about one of those subjects,
suggests further reading: perhaps deeper background information, or a contrasting viewpoint, or an
argument on why the thing you’re reading is full of shit.</p>

<p>(I’ll gloss over the problem of actually implementing those analyses. The signal-to-noise ratio on
the web is terrible, so it’s difficult to determine algorithmically whether a particular piece of
content is any good. Nevertheless, search engines are able to give us useful search results because
they spend a huge amount of effort on spam filtering and other measures to improve the quality of
results. Any product that uses web crawl data will have to decide what is noise, and get rid of it.
However, you can’t even start solving the signal-to-noise problem until you have the raw data.
Having crawled the web is step one.)</p>

<p>Unfortunately, at the moment, only Google and a small number of other companies that have crawled
the web have the resources to perform such analyses and build such products. Much as I believe
Google try their best to be neutral, a pluralistic society requires a diversity of voices, not
a filter bubble controlled by one organization. Surely there are people outside of Google who want
to work on this kind of thing. Many a start-up could be founded on the basis of doing useful things
with data extracted from a web crawl.</p>

<h2 id="the-web-link-graph">The web link graph</h2>

<p>The idea of collating several related, useful pieces of content on one subject was recently
<a href="https://medium.com/@justpw/its-time-to-rethink-the-link-66d32ff0e2e2">suggested</a> by
<a href="http://fugit.co/">Justin Wohlstadter</a> (indeed it was a discussion with Justin that inspired me to
write this article). His start-up, <a href="http://wayfinder.is/">Wayfinder</a>, aims to create such
cross-references between URLs, by way of human curation. However, it relies on users actively
submitting links to Wayfinder’s service.</p>

<p>I argued to Justin that I shouldn’t need to submit anything to a centralized database. By writing
a blog post (such as this one) that references some things on the web, I am implicitly creating
a connection between the URLs that appear in this blog post. By linking to those URLs, I am
implicitly suggesting that they might be worth reading. (Of course, this is an
<a href="http://ilpubs.stanford.edu:8090/422/1/1999-66.pdf">old idea</a>.) The web is already, in a sense,
a huge distributed database.</p>

<p>By analogy, citations are very important in scientific publishing. Every scientific paper uses
references to acknowledge its sources, to cite prior work, and to make the reader aware of related
work. In the opposite direction, the number of times a paper is cited by other authors is a metric
of how important the work is, and citation counts have even become a
<a href="http://en.wikipedia.org/wiki/H-index">metric</a> for researchers’ careers.</p>

<p><a href="http://scholar.google.com/">Google Scholar</a> and
<a href="http://citeseer.ist.psu.edu/">bibliography databases</a> maintain an index of citations, so you can
find later work that builds upon (or invalidates!) a particular paper. As a researcher, following
those forward and backward citations is an important way of learning about the state of the art.</p>

<p>Similarly, if you want to analyze the web, you need to need to be able to traverse the link graph
and see which pages link to each other. For a given URL, you need to be able to see which pages it
links to (outgoing links) and which pages link to it (incoming links).</p>

<p>You can easily write a program that fetches the HTML for a given URL, and parses out all the links
– in other words, you can easily find all the outgoing links for an URL. Unfortunately, finding all
the incoming links is very difficult. You need to download every page on the entire web, extract all
of their links, and then collate all the web pages that reference the same URL. You need a copy of
the entire web.</p>

<p><img src="/2014/12/incoming-outgoing-links.png" width="550" height="430" /></p>

<p><em>Incoming and outgoing links for a URL. Finding the outgoing ones is easy (just parse the page);
finding the incoming ones is hard.</em></p>

<h2 id="publicly-available-crawl-data">Publicly available crawl data</h2>

<p>An interesting move in this direction is <a href="http://commoncrawl.org/">CommonCrawl</a>, a nonprofit. Every
couple of months they send a crawler out into the web, download a whole bunch of web pages (about
2.8 billion pages, at latest count), and store the result as a publicly available data set in S3.
The data is in <a href="http://en.wikipedia.org/wiki/Web_ARChive">WARC format</a>, and you can do whatever you
want with it. (If you want to play with the data, I wrote an
<a href="https://github.com/ept/warc-hadoop">implementation of WARC</a> for use in Hadoop.)</p>

<p>As an experiment, I wrote a simple MapReduce job that processed the entire CommonCrawl data set. It
cost me about $100 in EC2 instance time to process all 2.8 billion pages (a bit of optimization
would probably bring that down). Crunching through such quantities of data isn’t free, but it’s
surprisingly affordable.</p>

<p>The CommonCrawl data set is about 35 TB in size (unparsed, compressed HTML). That’s a lot, but
Google says they crawl 60 trillion distinct pages, and the index is
<a href="http://www.google.com/insidesearch/howsearchworks/thestory/">reported</a> as being over 100 PB, so
it’s safe to assume that the CommonCrawl data set represents only a small fraction of the web.</p>

<p><img src="/2014/12/dataset-size.png" width="550" height="311" /></p>

<p><em>Relative sizes of CommonCrawl, Internet Archive, and Google (sources:
<a href="http://blog.commoncrawl.org/2014/09/august-2014-crawl-data-available/">1</a>,
<a href="https://blog.archive.org/2013/01/09/updated-wayback/">2</a>,
<a href="http://www.google.com/insidesearch/howsearchworks/thestory/">3</a>).</em></p>

<p>CommonCrawl is a good start. But what would it take to create a publicly available crawl of the
entire web? Is it just a matter of getting some generous donations to finance CommonCrawl? But if
all the data is on S3, you have to either use EC2 to process it or pay Amazon for the bandwidth to
download it. A long-term solution would have to be less AWS-centric.</p>

<p>I don’t know for sure, but my gut instinct is that a full web crawl would best be undertaken as
a decentralized effort, with many organizations donating some bandwidth, storage, and computing
resources toward a shared goal. (Perhaps this is what <a href="http://www.faroo.com/hp/p2p/p2p.html">Faroo</a>
and <a href="http://yacy.net/en/Technology.html">YaCy</a> are doing, but I’m not familiar with the details of
their systems.)</p>

<h2 id="an-architectural-sketch">An architectural sketch</h2>

<p>Here are some rough ideas on how a decentralized web crawl project could look.</p>

<p>The participants in the crawl can communicate peer-to-peer, using something like BitTorrent.
A distributed hash table can be used to assign a portion of the URL space to a participant. That
means each URL is assigned to one or more participants, and that participant is in charge of
fetching the URL, storing the response, and parsing any links that appear in the page. Every URL
that is found in a link is sent to the crawl participant to whom the URL is assigned. The recipient
can ignore that message if it has already fetched that URL recently.</p>

<p>The system will need to ensure it is well-behaved as a whole (obey robots.txt, stay within rate
limits, de-duplicate URLs that return the same content, etc.). This will require some coordination
between crawl participants. However, even if the crawl was done by a single organization, it would
have to be distributed across multiple nodes, probably using asynchronous message passing for loose
coordination. The same principles apply if the crawl nodes are distributed across several
participants – it just means the message-passing is across the Internet rather than within one
organization’s data center.</p>

<p><img src="/2014/12/distributed-crawl.png" width="550" height="413" /></p>

<p><em>A hypothetical architecture for distributed web crawling. Each participant crawls and stores
a portion of the web.</em></p>

<p>There remain many questions. What if your crawler downloads some content that is illegal in your
country? How do you keep crawlers honest (ensuring they don’t manipulate the crawl results to their
own advantage)? How is the load balanced across participants with different amounts of resources to
offer? Is it necessary to enforce some kind of reciprocity (you can only use crawl data if you also
contribute data), or have a payment model (bitcoin?) to create an incentive for people to run
crawlers? How can index creation be distributed across participants?</p>

<p>(As an aside, I think <a href="http://samza.incubator.apache.org/">Samza</a>’s model of stream computation
would be a good starting point for implementing a scalable distributed crawler. I’d love to see
someone implement a proof of concept.)</p>

<h2 id="motivations-for-contributing">Motivations for contributing</h2>

<p>Why would different organizations – many of them probably competitors – potentially collaborate on
creating a public domain crawl data set? Well, there is precedence for this, namely in open source
software.</p>

<p>Simplifying for the sake of brevity, there are a few reasons why this model works well:</p>

<ul>
  <li><strong>Cost:</strong> Creating and maintaining a large software project (eg. Hadoop, Linux kernel, database
system) is very expensive, and only a small number of very large companies can afford to run
a project of that size by themselves. As a mid-size company, you have to either buy an
off-the-shelf product from a vendor or collaborate with other organizations in creating an open
solution.</li>
  <li><strong>Competitive advantage:</strong> With infrastructure software (databases, operating systems) there is
little competitive advantage in keeping a project proprietary because competitive differentiation
happens at the higher levels (closer to the user interface). On the other hand, by making it open,
everybody benefits from better software infrastructure. This makes open source a very attractive
option for infrastructure-level software.</li>
  <li><strong>Public relations:</strong> Companies want to be seen as doing good, and contributing to open source is
seen as such. Many engineers also want to work on open source, perhaps for idealistic reasons, or
because it makes their skills and accomplishments publicly visible and recognized, including to
prospective future employers.</li>
</ul>

<p>I would argue that all the same arguments apply to the creation of an open data set, not only to the
creation of open source software. If we believe that there is enough value in having publicly
accessible crawl data, it looks like it could be done.</p>

<h2 id="perhaps-we-can-make-it-happen">Perhaps we can make it happen</h2>

<p>What I’ve described is a pie in the sky right now (although CommonCrawl is totally real).</p>

<p>Collaboratively created data sets such as Wikipedia and OpenStreetMap are an amazing resource and
accomplishment. At first, people thought the creators of these projects were crazy, but they turned
out to work very well. We can safely say they have made a positive impact on the world, by
summarizing a certain subset of human knowledge and making it freely accessible to all.</p>

<p>I don’t know if freely available web crawl data would be similarly valuable because it’s hard to
imagine all the possible applications, which only arise when you actually have the data and start
exploring it. However, there must be interesting things you can do if you have access to the
collective outpourings of humanity. How about <a href="http://web.stanford.edu/dept/SUL/library/extra4/sloan/MouseSite/1968Demo.html">augmenting human
intellect</a>, which
we’ve talked about for so long? Can we use this data to create fairer societies, better mutual
understanding, better education, and such good things?</p>

                ]]></content:encoded>
            </item>
        
            <item>
                <title>Hermitage: Testing the “I” in ACID</title>
                <link>http://martin.kleppmann.com/2014/11/25/hermitage-testing-the-i-in-acid.html</link>
                <comments>http://martin.kleppmann.com/2014/11/25/hermitage-testing-the-i-in-acid.html#disqus_thread</comments>
                <pubDate>Tue, 25 Nov 2014 00:00:00 +0000</pubDate>
                <dc:creator>Martin Kleppmann</dc:creator>
                
                    <guid isPermaLink="true">http://martin.kleppmann.com/2014/11/25/hermitage-testing-the-i-in-acid.html</guid>
                
                <description><![CDATA[ tl;dr: I have created a test suite for comparing the transaction isolation levels in different databases. I did this as background research for my book. This post explains why. What is isolation? First came the NoSQL movement, with its siren call that our systems could be so scalable, so much... ]]></description>
                <content:encoded><![CDATA[
                    <p><em>tl;dr:</em> I have created a <a href="https://github.com/ept/hermitage">test suite</a> for comparing the
transaction isolation levels in different databases. I did this as background research for
<a href="http://dataintensive.net/">my book</a>. This post explains why.</p>

<h2 id="what-is-isolation">What is isolation?</h2>

<p>First came the NoSQL movement, with its siren call that our systems could be so scalable, so much
faster and so highly available if we just abandon ACID transactions and make our systems
<a href="http://queue.acm.org/detail.cfm?id=1394128">BASE</a> instead.</p>

<p>Then came the concurrency bugs — for example, the Bitcoin exchange that was
<a href="https://bitcointalk.org/index.php?topic=499580">almost bankrupted</a> because it had a race condition
on outgoing payments. (An attacker circumvented the account balance check by making many concurrent
transactions, and thus was able withdraw more money than they had in their account. And this
<a href="http://www.reddit.com/r/Bitcoin/comments/1wtbiu/how_i_stole_roughly_100_btc_from_an_exchange_and/">wasn’t even</a>
the only case.)</p>

<p>Internet commenters, in their infinite wisdom, were quick to point out that if you’re dealing with
money, you had <a href="https://twitter.com/kellabyte/status/452982674626711552">better use an ACID database</a>.
But there was a major flaw in their argument. Most so-called ACID databases — for example Postgres,
MySQL, Oracle or MS SQL Server — would <em>not</em> have prevented this race condition in their default
configuration.</p>

<p>Yes, you read that right. Those databases — the ones that have probably processed the vast majority
of commercial transactions over the last 20 years — do not by default guarantee that your
transactions are protected from race conditions. Let me explain.</p>

<h2 id="the-story-of-weak-isolation">The story of weak isolation</h2>

<p>Among the ACID properties, the letter I stands for <em>isolation</em>. The idea of isolation is that we
want our database to be able to process several transactions at the same time (otherwise it would be
terribly slow), but we don’t want to have to worry about concurrency (because it’s terribly
complicated). In an ideal world, the database could guarantee that transactions behave as if they
executed without any concurrency, one after another, <em>serially</em> — in other words, they are
<em>serializable</em>.</p>

<p>Unfortunately, most implementations of serializability have quite bad performance. The team working
on the first SQL database (<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.84.348&amp;rep=rep1&amp;type=pdf">System R</a>)
already <a href="http://citeseer.ist.psu.edu/viewdoc/download?doi=10.1.1.92.8248&amp;rep=rep1&amp;type=pdf">realised this in 1975</a>,
and decided to offer weaker isolation levels than serializability. Those isolation levels would not
quite prevent all race conditions, but they had much better performance, so it was considered an
acceptable trade-off. That research group made up some names for those weak isolation levels
(“repeatable read”, “read committed”, and “read uncommitted”). 39 years later, some implementation
details have changed, but on the whole isolation levels still look
<a href="http://www.bailis.org/blog/when-is-acid-acid-rarely/">remarkably similar</a> to System R.</p>

<p>The problem, however, is this: in order to understand what those isolation levels mean, you pretty
much have to understand how the database implements concurrency control internally. The levels
basically have no intuitive meaning, because they are an incredibly
<a href="http://www.joelonsoftware.com/articles/LeakyAbstractions.html">leaky abstraction</a> — they are
defined by implementation detail, not because someone thought they’d make a great API. And that’s
also why they are so hard to understand.</p>

<h2 id="understanding-weak-isolation">Understanding weak isolation</h2>

<p>Here’s a challenge for you: can you find any colleague in your organisation who can explain — off
the cuff and without looking at the docs — the difference between “repeatable read” and “read
committed”, including an example situation in which they behave differently? And what about the
difference between “repeatable read”, “snapshot isolation” and “serializable”?</p>

<p>If you can find such a person, I bet they worked on database systems previously. I can’t imagine
there are many application developers who really understand isolation levels, because they are so
confusing. Application developers are smart, but they have more important things to worry about
than obscure implementation details of databases.</p>

<p>That’s a big problem. The isolation level is part of the database’s API. It’s just as important as
the data model. People have
<a href="http://www.sarahmei.com/blog/2013/11/11/why-you-should-never-use-mongodb/">endless debates</a> about
relational vs. document vs. other data models, but I’ve never overheard a heated debate about
isolation levels — only seen slightly embarrassed smirks signifying “I really ought to know more
about this, but I don’t”.</p>

<p>If you don’t understand the concurrency guarantees, you have no idea whether your code will still
behave correctly when processing a few simultaneous requests. You can’t easily write unit tests for
concurrency, either. How do you know your code is correct?</p>

<h2 id="a-rigorous-system-for-comparing-isolation-levels">A rigorous system for comparing isolation levels</h2>

<p>Fortunately, some academic researchers have been on the case, creating formal models of isolation
levels and proving some of their properties. For example, <a href="http://www.bailis.org/">Peter Bailis</a> et
al. at <a href="http://db.cs.berkeley.edu/">Berkeley</a> have been doing some
<a href="http://www.bailis.org/blog/understanding-weak-isolation-is-a-serious-problem/">good work</a> in this
area. It’s not yet easy to understand, but at least it’s logically sound. </p>

<p><img src="/2014/11/isolation-levels.png" width="550" alt="Hierarchy of isolation levels" /></p>

<p>You may have seen this diagram from their <a href="http://arxiv.org/pdf/1302.0309.pdf">VLDB paper</a>, which
shows the relative strength of different isolation levels, and also relates them to availability (as
per the <a href="http://henryr.github.io/cap-faq/">CAP theorem</a>). It has also appeared in <a href="http://aphyr.com/">Kyle Kingsbury</a>’s
“<a href="https://www.youtube.com/watch?v=QdkS6ZjeR7Q">Linearizable Boogaloo</a>” talk (around 8:30). In this
diagram, 1SR = serializability, RR = repeatable read, SI = snapshot isolation, RC = read committed.</p>

<p>Unfortunately, even though terms like <em>repeatable read</em> sound like they should be standardised (and
indeed they are defined in the ANSI SQL standard), in practice different databases interpret them
differently. One database’s <em>repeatable read</em> guarantees things that another doesn’t. Not only are
the names of the isolation levels confusing, they are also inconsistent.</p>

<p>If you read the documentation of a database, the description of the isolation levels tends to be
quite vague. How do you know what guarantees your application actually needs, and thus which
isolation level you should use? How do you learn to look at a piece of code and say “this won’t be
safe to run at <em>read committed</em>, it’ll need at least <em>repeatable read</em>”? Even better, could we write
automated tests or type checkers which fail if we write code that is not concurrency-safe?</p>

<p>If we want any hope of reasoning about concurrency safety, we first need to understand exactly which
guarantees existing databases do and don’t provide. We need to express those guarantees in terms of
precisely defined, testable properties (not vague English sentences in the documentation). Research
on isolation has produced those precise definitions, but as far as I know they so far haven’t been
systematically tested on actual database implementations.</p>

<h2 id="introducing-hermitage">Introducing Hermitage</h2>

<p><a href="https://github.com/ept/hermitage">Hermitage</a> is a small project that I started to address this. It
is a test suite for databases which probes for a variety of concurrency issues, and thus allows
a fair and accurate comparison of isolation levels. Each test case simulates a particular kind of
race condition that can happen when two or more transactions concurrently access the same data. Each
test can pass (if the database’s implementation of isolation prevents the race condition from
occurring) or fail (if the race condition does occur).</p>

<p>The tests are fairly direct translations of the anomalies described in research papers. The
properties that they check have quite obscure names (such as Predicate-Many-Preceders) and
abbreviations (such as P4), but rather than inventing even more terminology I thought I’d rather
stick with the terms in the literature.</p>

<p>So far I have ported and run this test suite on four relational databases: PostgreSQL, MySQL, Oracle
DB and Microsoft SQL Server. But it’s not limited to old-school relational databases: in fact, it
would be very interesting to run it with some of the recent “NewSQL” databases that
<a href="https://foundationdb.com/acid-claims">claim to support ACID transactions</a>. The whole point of this
test suite is that it allows an exact comparison not just between isolation levels within one
database, but across different databases.</p>

<p>For the details please <a href="https://github.com/ept/hermitage">check out the repository</a>. To mention
just a few highlights:</p>

<ul>
  <li>PostgreSQL, MySQL and MS SQL Server all boast an isolation level called “repeatable read”, but
it means something different in every one of them. Yay for standards.</li>
  <li>On the other hand, “read committed” has the same safety properties in all databases I’ve tested.</li>
  <li>Oracle “serializable” is actually not serializable at all, but snapshot isolation. (This has been
<a href="http://www.researchgate.net/publication/220225203_Making_snapshot_isolation_serializable/file/e0b49520567eace81f.pdf">documented</a>
<a href="http://www.bailis.org/papers/hat-hotos2013.pdf">previously</a>.)</li>
  <li>SQL Server’s lock-based isolation modes (the default) are remarkably similar to System R in 1975,
which seems pretty terrible to me. If you use SQL Server in production, I’d be interested to hear
whether you’ve configured it to use MVCC instead (allow_snapshot_isolation on, and/or
read_committed_snapshot on).</li>
</ul>

<h2 id="why">Why?</h2>

<p>In the book I’m writing (<a href="http://dataintensive.net/">Designing Data-Intensive Applications</a>) I’m
determined to explain transaction isolation in a way that we can all understand. But in order to
write that chapter, I first had to understand isolation levels myself. </p>

<p>So Hermitage was primarily background research for the book, allowing me to write an
easy-to-understand but <em>accurate</em> description of what databases do in practice. If you find
Hermitage incredibly confusing, don’t worry: the book doesn’t go into all the detail of G1b, PMP,
P4 and G2-item. I first had to make it complicated for myself so that I could make it simple
(but accurate) for readers.</p>

<p>However, going beyond this book, I’m hoping that this little project can encourage others to
explore how application developers interact with isolation. Simply cataloguing what databases
actually do, using test cases rather than vague langauge, is a step forward.</p>

<p>And perhaps some enterprising researcher could take this further. For example, perhaps it’s
possible to create a type system for transactions, which would allow
<a href="http://www.vldb.org/conf/2007/papers/industrial/p1263-jorwekar.pdf">static analysis</a> of an
application to verify that it doesn’t contain concurrency bugs when run at a particular isolation
level. I don’t know enough about type systems to know whether that’s even possible – but if yes, it
could potentially cut out a whole class of bugs that are very hard to find through testing.</p>

<p><em>Thank you to Peter Bailis and his collaborators for feedback on a draft of this post.</em></p>

                ]]></content:encoded>
            </item>
        
            <item>
                <title>Hey, I'm writing a book!</title>
                <link>http://martin.kleppmann.com/2014/09/15/writing-a-book.html</link>
                <comments>http://martin.kleppmann.com/2014/09/15/writing-a-book.html#disqus_thread</comments>
                <pubDate>Mon, 15 Sep 2014 00:00:00 +0000</pubDate>
                <dc:creator>Martin Kleppmann</dc:creator>
                
                    <guid isPermaLink="true">http://martin.kleppmann.com/2014/09/15/writing-a-book.html</guid>
                
                <description><![CDATA[ About two years ago I wrote a blog post called “Rethinking caching in web apps”. At almost 4,000 words, it was a lot longer than the received wisdom says a blog post should be. Nevertheless I had the feeling that I was only scratching the surface of what needed to... ]]></description>
                <content:encoded><![CDATA[
                    <p>About two years ago I wrote a blog post called
<a href="/2012/10/01/rethinking-caching-in-web-apps.html">“Rethinking caching in web apps”</a>.
At almost 4,000 words, it was a lot longer than the
<a href="http://blog.bufferapp.com/the-ideal-length-of-everything-online-according-to-science">received wisdom</a>
says a blog post should be. Nevertheless I had the feeling that I was only scratching
the surface of what needed to be said.</p>

<p>That got me thinking whether I should try writing something longer, like a book perhaps.
I love writing because it forces me to research something in depth, think it through,
and then try to explain it in a logical way. That helps me understand it much better
than if I just casually read about it. Or, put more eloquently:</p>

<blockquote>
  <p>“Writing is nature’s way of letting you know how sloppy your thinking is.”
– Dick Guindon</p>
</blockquote>

<h2 id="existing-books">Existing books</h2>

<p>I am writing because the book I wanted to read didn’t exist. I wanted a book that
would explain data systems to me – the whole area of databases, distributed systems,
batch and stream processing, consistency, caching and indexing – at the right level.
But I found that almost all the existing books, blog posts etc. fell into one of the
following categories:</p>

<ol>
  <li>Most computing books are hands-on guides to one particular technology. They assume that
you’ve been told to use database X or programming language Y, and so they teach you
how to use it. Those books are fine, but they are of little use if you’re trying to
decide whether X or Y is the right tool for you in the first place. These books tend to
focus on the strong points of that particular technology, and fail to mention its
shortcomings.</li>
  <li>It’s common to see blog posts with side-by-side comparisons of several similar
technologies, but I find they tend to just focus on superficial aspects (performance
benchmarks, API, software license) while completely missing the fundamental workings
of the technology. They are like Top Trumps for databases, and don’t actually help you
understand anything any better.</li>
  <li>By contrast, academic textbooks cover the fundamental principles and trade-offs that
are common to many different technologies, but in doing so, they often lose all
contact with reality. These books are generally written by academics with deep research
experience in their field, but little awareness of the practicalities of real
production systems. They often end up saying things which are technically correct,
but useless or misleading if you want to actually build a real system.</li>
</ol>

<p>I wanted something in between all of these. A book which would tell a story of the big ideas in
data systems, the fundamental principles which don’t change from one software version to
another. But the book would also stay grounded in reality, explaining what works in practice
and what doesn’t, and <em>why</em>. The book would examine the tools and systems that we already
use in production, compare their fundamental approaches, and help you figure out which
technology is appropriate to which use case.</p>

<p>I wanted to understand not just how to <em>use</em> a particular system, but also how it <em>works under
the hood</em>. That is partly out of intellectual curiosity, but equally importantly, because it
allows me to imagine what the system is doing. If some kind of unexpected behaviour occurs, or
if I want to push the limits of what a technology can do, it is tremendously useful to have
at least a rough idea of what is happening internally.</p>

<p>As I spoke to various people about these ideas, including some folks at O’Reilly, it became
clear that I wasn’t the only one who wanted a book like this. And so,
<a href="http://dataintensive.net/">Designing Data-Intensive Applications</a> was born.
And you’ll know it when you see it, because it has an awesome Indian Wild Boar on the cover.</p>

<p style="text-align: center"><a href="http://dataintensive.net" title="Designing Data-Intensive Applications (the wild boar book)"><img src="/2014/09/book-cover.png" alt="Designing Data-Intensive Applications (the wild boar book)" width="250" height="328" /></a>
</p>

<p><em>Designing Data-Intensive Applications</em> (sorry about the verbose title – you can just call it
“the wild boar book”) has been in the works for some time, and today we’re announcing the
<a href="http://shop.oreilly.com/product/0636920032175.do">early release</a>. The first four chapters
are now available – ten or eleven are planned in total, so there’s still long way to go.
But I left my job to work on this book full-time, so it’s definitely happening.</p>

<h2 id="who-should-read-this">Who should read this?</h2>

<p>If you’re a software engineer working on server-side applications (a web application backend,
for instance), then this book is for you. It assumes that you already know how to build an
application and use a database, and that you want to “level up” in your craft. Perhaps you
want to work on highly scalable systems with millions of users, perhaps you want to deal with
particularly complex or ever-changing data, or perhaps you want to make an old legacy
environment more agile.</p>

<p>This book starts at the foundations, and gradually builds up a picture of modern data systems
layer by layer, one chapter at a time. I’m not trying to sell you any particular architecture
or approach, because I firmly believe that different use cases require different solutions.
Therefore, each chapter contains a broad overview and comparison of the different approaches
that have been successful in different circumstances.</p>

<p>It doesn’t matter what your preferred programming language or framework is – this book is
agnostic. It’s about architecture and algorithms, about fundamental principles and practical
constraints, about the reasoning behind every design decision.</p>

<p>None of the ideas in this book are really new, and indeed many ideas are decades old.
Everything has already been said somewhere, in conference presentations, research papers,
blog posts, code, bug trackers, and engineering folklore. However, to my knowledge the ideas
haven’t previously been collected, compared and evaluated like this.</p>

<p>I hope that by understanding what our options are, and the pros and cons of each approach,
we’ll all become better engineers. By making conscious trade-offs and choosing our tools
wisely, we will build systems that are more reliable and much easier to maintain in the long
run. It’s a quest to help us engineers be better at our jobs, and build better software.</p>

<h2 id="lets-make-software-development-better">Let’s make software development better</h2>

<p>Please join me on this quest by reading the draft of the book, and sending us your feedback:</p>

<ul>
  <li>The book’s website is <a href="http://dataintensive.net/">dataintensive.net</a>.</li>
  <li>You can buy the early release ebook from
<a href="http://shop.oreilly.com/product/0636920032175.do">O’Reilly</a>, or if you’re a Safari Books
subscriber, you can <a href="http://my.safaribooksonline.com/9781491903063">read it online</a>.</li>
  <li>If you can think of any way the book could be improved, please email your thoughts to
<a href="&#109;&#097;&#105;&#108;&#116;&#111;:&#102;&#101;&#101;&#100;&#098;&#097;&#099;&#107;&#064;&#100;&#097;&#116;&#097;&#105;&#110;&#116;&#101;&#110;&#115;&#105;&#118;&#101;&#046;&#110;&#101;&#116;">&#102;&#101;&#101;&#100;&#098;&#097;&#099;&#107;&#064;&#100;&#097;&#116;&#097;&#105;&#110;&#116;&#101;&#110;&#115;&#105;&#118;&#101;&#046;&#110;&#101;&#116;</a> or tweet us
<a href="https://twitter.com/intensivedata">@intensivedata</a>. Now is the time to be involved.</li>
  <li>…and if you like the sound of this, don’t forget to tell your friends and colleagues
about it!</li>
</ul>

                ]]></content:encoded>
            </item>
        
            <item>
                <title>Upcoming conference talks about Samza</title>
                <link>http://martin.kleppmann.com/2014/08/28/upcoming-conference-talks-about-samza.html</link>
                <comments>http://martin.kleppmann.com/2014/08/28/upcoming-conference-talks-about-samza.html#disqus_thread</comments>
                <pubDate>Thu, 28 Aug 2014 00:00:00 +0000</pubDate>
                <dc:creator>Martin Kleppmann</dc:creator>
                
                    <guid isPermaLink="true">http://martin.kleppmann.com/2014/08/28/upcoming-conference-talks-about-samza.html</guid>
                
                <description><![CDATA[ After my talk about Samza fault tolerance at Berlin Buzzwords was well received a few months ago, I submitted several more talk proposals to a variety of conferences. To my surprise, all the proposals were accepted, so I’m now going to have a fairly busy time in the next few... ]]></description>
                <content:encoded><![CDATA[
                    <p>After my <a href="https://www.youtube.com/watch?v=d63kSjxVsGA&amp;index=11&amp;list=PLq-odUc2x7i-Q5gQtkmba4ov37XRPjp6n">talk about Samza fault tolerance</a>
at <a href="http://berlinbuzzwords.de/">Berlin Buzzwords</a> was well received a few months ago,
I submitted several more talk proposals to a variety of conferences. To my surprise,
all the proposals were accepted, so I’m now going to have a fairly busy time in the
next few months!</p>

<p>Here are the four conferences at which I’ll be speaking between September and November.
All the talks are about <a href="http://samza.incubator.apache.org/">Apache Samza</a>, the stream
processing project I’ve been working on. However, all the talks are different, each
focussing on a different aspect and perspective.</p>

<p>If you don’t yet have a ticket for these conferences, there are a few discount codes
below. Hope to see you there :-)</p>

<p><a href="https://thestrangeloop.com/sessions/turning-the-database-inside-out-with-apache-samza"><strong>Turning the database inside out with Apache Samza</strong></a><br />
<a href="https://thestrangeloop.com/">Strange Loop</a>, September 18–19 in St. Louis, Missouri.
(<a href="http://lanyrd.com/2014/strangeloop/">Lanyrd</a>, <a href="https://twitter.com/strangeloop_stl">Twitter</a>)</p>

<p>The Strange Loop conference explores the future of software development from a wonderfully
eclectic range of viewpoints, ranging from functional programming to distributed systems.
In this talk I’ll discuss the potential of stream processing as a fundamental programming
model, which has big advantages compared to the way we usually build applications today.</p>

<p><a href="http://strataconf.com/stratany2014/public/schedule/detail/36045"><strong>Building real-time data products at LinkedIn with Apache Samza</strong></a><br />
<a href="http://strataconf.com/stratany2014">Strata + Hadoop World</a>, October 15–17 in New York.
(<a href="http://lanyrd.com/2014/strata-new-york/">Lanyrd</a>, <a href="https://twitter.com/strataconf">Twitter</a>)<br />
Use discount code SPEAKER20 to get 20% off.</p>

<p>MapReduce and its cousins are powerful tools for building data products such as recommendation
engines, detecting anomalies and improving relevance. However, with batch processing there may
be several hours delay before new data is reflected in the output. With stream processing, you
can potentially respond in seconds rather than hours, but you have to learn a whole new way of
thinking in order to write your jobs. In this talk I’ll discuss some real-life examples of
stream processing at LinkedIn, and show how to use Samza to solve real-time data problems.</p>

<p><a href="http://london-2014.spanconf.io/martin-kleppmann/"><strong>Staying agile in the face of the data deluge</strong></a><br />
<a href="http://spanconf.io">Span conference</a>, October 28 in London, UK.
(<a href="http://lanyrd.com/2014/spanconf/">Lanyrd</a>, <a href="https://twitter.com/spanconf">Twitter</a>)<br />
Use <a href="https://ti.to/span/london-2014?discount_code=kleppmann">this link</a> to get a 20% discount.</p>

<p>An often-overlooked but important aspect of tools is their <em>plasticity</em>: if your
application’s requirements change, how easily do the tools let you adapt your existing
code and data to the new requirements? Samza is designed with plasticity in mind. In
this talk I’ll discuss how re-processing of data streams can keep your application
development agile.</p>

<p><a href="http://apacheconeu2014.sched.org/event/3633e195715f88c3357749d57b7b3b8c"><strong>Scalable stream processing with Apache Samza and Apache Kafka</strong></a><br />
<a href="http://events.linuxfoundation.org/events/apachecon-europe">ApacheCon Europe</a>, November 17–21 in Budapest, Hungary.
(<a href="http://lanyrd.com/2014/apachecon-europe/">Lanyrd</a>, <a href="https://twitter.com/apachecon">Twitter</a>)</p>

<p>Many of the most important open source data infrastructure tools are projects of the
Apache Software Foundatation: Hadoop, Zookeeper, Storm and Spark, to name just a few.
In this talk I’ll focus on how Samza and Kafka (also Apache projects) fit into this
lively open source ecosystem.</p>

<p><strong>Background reading</strong></p>

<p>If you don’t yet know about Samza, don’t worry: I’ll start each talk with a quick
introduction to Samza, and not assume any prior knowledge.</p>

<p>But if you want to ask smart-ass questions and embarrass me in front of the audience, you
can begin by reading the Samza
<a href="http://samza.incubator.apache.org/learn/documentation/latest/">documentation</a>
(thoroughly updated over the last few months by yours truly), and start thinking of
particularly tricky questions to ask.</p>

<p>You may also be interested in this excellent series of articles by
<a href="https://twitter.com/jaykreps">Jay Kreps</a>, which are relevant to the upcoming talks:</p>

<ul>
  <li><a href="http://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying">The Log: What every software engineer should know about real-time data’s unifying abstraction</a></li>
  <li><a href="http://radar.oreilly.com/2014/07/questioning-the-lambda-architecture.html">Questioning the Lambda Architecture</a></li>
  <li><a href="http://radar.oreilly.com/2014/07/why-local-state-is-a-fundamental-primitive-in-stream-processing.html">Why local state is a fundamental primitive in stream processing</a> – What do you get if you cross a distributed database with a stream processing system?</li>
</ul>

                ]]></content:encoded>
            </item>
        
            <item>
                <title>Six things I wish we had known about scaling</title>
                <link>http://martin.kleppmann.com/2014/03/26/six-things-about-scaling.html</link>
                <comments>http://martin.kleppmann.com/2014/03/26/six-things-about-scaling.html#disqus_thread</comments>
                <pubDate>Wed, 26 Mar 2014 00:00:00 +0000</pubDate>
                <dc:creator>Martin Kleppmann</dc:creator>
                
                    <guid isPermaLink="true">http://martin.kleppmann.com/2014/03/26/six-things-about-scaling.html</guid>
                
                <description><![CDATA[ Looking back at the last few years of building Rapportive and LinkedIn Intro, I realised that there were a number of lessons that we had to learn the hard way. We built some reasonably large data systems, and there are a few things I really wish we had known beforehand.... ]]></description>
                <content:encoded><![CDATA[
                    <p>Looking back at the last few years of building <a href="https://rapportive.com/">Rapportive</a> and
<a href="http://engineering.linkedin.com/mobile/linkedin-intro-doing-impossible-ios">LinkedIn Intro</a>,
I realised that there were a number of lessons that we had to learn the hard way. We built some
reasonably large data systems, and there are a few things I really wish we had known beforehand.</p>

<p>None of these lessons are particularly obscure – they are all well-documented, if you know where to
look. They are the kind of things that made me think <em>“I can’t believe I didn’t know that, I’m so
stupid #facepalm”</em> in retrospect. But perhaps I’m not the only one who started out not knowing these
things, so I’ll write them down for the benefit of anyone else who finds themself having to scale
a system. </p>

<p>The kind of system I’m talking about is the data backend of a consumer web/mobile app with a million
users (order of magnitude). At the scale of Google, LinkedIn, Facebook or Twitter (hundreds of
millions of users), you’ll have an entirely different set of problems, but you’ll also have a bigger
team of experienced developers and operations people around you. The mid-range scale of about
a million users is interesting, because it’s quite feasible for a small startup team to get there
with some luck and good marketing skills. If that sounds like you, here are a few things to keep in
mind.</p>

<h2 id="realistic-load-testing-is-hard">1. Realistic load testing is hard</h2>

<p>Improving the performance of a system is ideally a very scientific process. You have in your head
a model of what your system is doing, and a theory of where the expensive operations are. You
propose a change to the system, and predict what the outcome will be. Then you make the change,
observe the system’s behaviour under laboratory conditions, and thus gather evidence which either
confirms or contradicts your theory. That way you iterate your way to a better theory, and also
a better-performing implementation.</p>

<p>Sadly, we hardly ever managed to do it that way in practice. If we were optimising a microbenchmark,
running the same code a million times in a tight loop, it would be easy. But we are dealing with
large volumes of data, spread out across multiple machines. If you read the same item a million
times in a loop, it will simply be cached, and the load test tells you nothing. If you want
meaningful results, the load test needs to simulate a realistically large working set, a realistic
mixture of reads and writes, realistic distribution of requests over time, and so on. And that is
difficult.</p>

<p>It’s difficult enough to simply <em>know</em> what your access patterns actually are, let alone simulate
them. As a starting point, you can replay a few hours worth of access logs against a copy of your
real dataset. However, that only really works for read requests. Simulating writes is harder, as
you may need to account for business logic rules (e.g. a sequential workflow must first update A,
then update B, then update C) and deal with changes that can happen only once (if your write changes
state from D to E, you can’t change from D to E again later in the test, as you’re already in state
E). That means you have to synchronise your access logs with your database snapshot, or somehow
generate suitable synthetic write load.</p>

<p>Even harder if you want to test with a dataset that is larger than the one you actually have (so
that you can find out what happens when you double your userbase, and prepare for that event). Now
you have to work out the statistical properties of your dataset (the distribution of friends per
user is a power law with x parameters, the correlation between one user’s number of friends and the
number of friends that their friends have is y, etc) and generate a synthetic dataset with those
parameters. You are now in deep, deep yak shaving territory. Step back from that yak.</p>

<p>In practice, it hardly ever works that way. We’re lucky if, sometimes, we can run the old code and
the new code side-by-side, and observe how they perform in comparison. Often, not even that is
possible. Usually we often just cross our fingers, deploy, and roll back if the change seems to have
made things worse. That is deeply unsatisfying for a scientifically-minded person, but it more or
less gets the job done.</p>

<h2 id="data-evolution-is-difficult">2. Data evolution is difficult</h2>

<p>Being able to rapidly respond to change is one of the biggest advantages of a small startup. Agility
in product and process means you also need the freedom to change your mind about the structure of
your code and your data. There is lot of talk about making code easy to change, eg. with good
automated tests. But what about changing the structure of your data?</p>

<p>Schema changes have a reputation of being very painful, a reputation that is chiefly MySQL’s fault:
simply adding a column to a table requires the
<a href="http://dev.mysql.com/doc/refman/5.6/en/alter-table.html">entire table to be copied</a>. On a large
table, that might mean several hours during which you can’t write to the table. Various
<a href="http://www.percona.com/doc/percona-toolkit/2.2/pt-online-schema-change.html">tools</a>
<a href="https://github.com/soundcloud/lhm">exist</a> to make that less painful, but I find it unbelievable
that the world’s most popular open source database handles such a common operation so badly.</p>

<p>Postgres can make simple schema changes without copying the table, which means they are almost
instant. And of course the avoidance of schema changes is a primary selling point of document
databases such as MongoDB (so it’s up to application code to deal with a database that uses
different schemas for different documents). But simple schema changes, such as adding a new field or
two, don’t tell the entire story.</p>

<p>Not all your data is in databases; some might be in archived log files or some kind of blob storage.
How do you deal with changing the schema of that data? And sometimes you need to make complex
changes to the data, such as breaking a large thing apart, or combining several small things, or
migrating from one datastore to another. Standard tools don’t help much here, and document databases
don’t make it any easier.</p>

<p>We’ve written large migration jobs that break the entire dataset into chunks, process chunks
gradually over the course of a weekend, retry failed chunks, track which things were modified while
the migration was happening, and finally catch up on the missed updates. A whole lot of complexity
just for a one-off data migration. Sometimes that’s unavoidable, but it’s heavy lifting that you’d
rather not have to do in the first place.</p>

<p>Hadoop data pipelines can help with this sort of thing, but now you have to set up a Hadoop cluster,
learn how to use it, figure out how to get your data into it, and figure out how to get the
transformed data out to your live systems again. Big companies like LinkedIn have figured out how to
do that, but in a small team it can be a massive time-sink.</p>

<h2 id="database-connections-are-a-real-limitation">3. Database connections are a real limitation</h2>

<p>In PostgreSQL, each client connection to the database is handled by a separate unix process; in
MySQL, each connection uses a separate thread. Both of these models impose a fairly low limit on the
number of connections you can have to the database – typically a few hundred. Every connection adds
overhead, so the entire database slows down, even if those connections aren’t actively processing
queries. For example, Heroku Postgres limits you to 60 connections on the smallest plan, and 500
connections on the <a href="https://devcenter.heroku.com/articles/heroku-postgres-plans#standard-tier">largest plan</a>,
although having anywhere near 500 connections is
<a href="https://postgres.heroku.com/blog/past/2013/11/22/connection_limit_guidance/">actively discouraged</a>.</p>

<p>In a fast-growing app, it doesn’t take long before you reach a few hundred connections. Each
instance of your application server uses at least one. Each background worker process that needs to
access the database uses one. Adding more machines running your application is fairly easy if they
are stateless, but every machine you add means more connections.</p>

<p>Partitioning (sharding) and read replicas probably won’t help you with your connection limit, unless
you can somehow load-balance requests so that all the requests for a particular partition are
handled by a particular server instance. A better bet is to use a
<a href="https://wiki.postgresql.org/wiki/PgBouncer">connection pooler</a>, or to write your own data access
layer which wraps database access behind an internal API.</p>

<p>That’s all doable, but it doesn’t seem a particularly valuable use of your time when you’re also
trying to iterate on product features. And every additional service you deploy is another thing that
can go wrong, another thing that needs to be monitored and maintained.</p>

<p>(Databases that use a lightweight connection model don’t have this problem, but they may have other
problems instead.)</p>

<h2 id="read-replicas-are-an-operational-pain">4. Read replicas are an operational pain</h2>

<p>A common architecture is to designate one database instance as a <em>leader</em> (also known as <em>master</em>)
and to send all database writes to that instance. The writes are then replicated to other database
instances (called <em>read replicas</em>, <em>followers</em> or <em>slaves</em>), and many read-only queries can be
served from the replicas, which takes load off the leader. This architecture is also good for fault
tolerance, since it gives you a <em>warm standby</em> – if your leader dies, you can quickly promote one
of the replicas to be the new leader (you wouldn’t want to be offline for hours while you restore
the database from a backup).</p>

<p>What they don’t tell you is that setting up and maintaining replicas is significant operational
pain. MySQL is particularly bad in this regard: in order to set up a new replica, you have to first
<a href="http://dev.mysql.com/doc/refman/5.6/en/replication-howto-masterstatus.html">lock the leader to stop all writes</a>
and take a consistent snapshot (which may take hours on a large database). How does your app cope if
it can’t write to the database? What do your users think if they can’t post stuff?</p>

<p>With Postgres, you don’t need to stop writes to set up a replica, but it’s still
<a href="http://www.postgresql.org/docs/current/static/warm-standby.html">some hassle</a>. One of the things
I like most about <a href="https://www.heroku.com/postgres">Heroku Postgres</a> is that it wraps all the
complexity of replication and WAL archiving behind a straightforward command-line tool.</p>

<p>Even so, you still need to failover manually if your leader fails. You need to monitor and maintain
the replicas. Your database library may not support read replicas out of the box, so you may need to
add that. Some reads need to be made on the leader, so that a user sees their own writes, even if
there is replication lag. That’s all doable, but it’s additional complexity, and doesn’t add any
value from users’ point of view.</p>

<p>Some distributed datastores such as MongoDB, RethinkDB and Couchbase also use this replication
model, and they automate the replica creation and master failover processes. Just because they do
that doesn’t mean they automatically give you magic scaling sauce, but it is a very valuable
feature.</p>

<h2 id="think-about-memory-efficiency">5. Think about memory efficiency</h2>

<p>At various times, we puzzled about weird latency spikes in our database activity. After many
<a href="http://www.pagerduty.com/">PagerDuty</a> alerts and troubleshooting, it usually turned out that we
could fix the issue by throwing more RAM at the problem, either in the form of a bigger database
instance, or separate caches in front of it. It’s sad, but true: many performance problems can be
solved by simply buying more RAM. And if you’re in a hurry because your hair is on fire, it’s often
the best thing to do. There are limitations to that approach, of course – a m2.4xlarge instance on
EC2 costs quite a bit of money, and eventually there are no bigger machines to turn to. </p>

<p>Besides buying more RAM, an effective solution is to use RAM more efficiently in the first place, so
that a bigger part of your dataset fits in RAM. In order to decide where to optimise, you need to
know what all your memory is being used for – and that’s surprisingly non-trivial. With a bit of
digging, you can usually get your database to report how much disk space each of your tables and
indexes is taking. Figuring out the working set, and how much memory is actually used for what, is
harder.</p>

<p>As a rule of thumb, your performance will probably be more predictable if your indexes completely
fit in RAM – so that there’s a maximum of one disk read per query, which reduces your exposure to
fluctuations in I/O latency. But indexes can get rather large if you have a lot of data, so this can
be an expensive proposition.</p>

<p>At one point we found ourselves reading up about the internal structure of an index in Postgres, and
realised that we could save a few bytes per row by indexing on the hash of a string column rather
than the string itself. (More on that in another post.) That reduced the memory pressure on the
system, and helped keep things ticking along for another few months. That’s just one example of how
it can be helpful to think about using memory efficiently.</p>

<h2 id="change-capture-is-under-appreciated">6. Change capture is under-appreciated</h2>

<p>So far I’ve only talked about things that suck – sorry about the negativity. As final point, I’d
like to mention a technique which is awesome, but not nearly as widely known and appreciated as it
should be: <em>change capture</em>.</p>

<p>The idea of change capture is simple: let the application consume a feed of all writes to the
database. In other words, you have a background process which gets notified every time something
changes in the database (insert, update or delete).</p>

<p>You could achieve a similar thing if, every time you write something to the database, you also post
it to a message queue. However, change capture is better because it contains exactly the same data
as what was committed to the database (avoiding race conditions). A good change capture system also
allows you to stream through the entire existing dataset, and then seamlessly switch to consuming
real-time updates when it has caught up.</p>

<p>Consumers of this changelog are decoupled from the app that generates the writes, which gives you
great freedom to experiment without fear of bringing down the main site. You can use the changelog
for updating and invalidating caches, for maintaining full-text indexes, for calculating analytics,
for sending out emails and push notifications, for importing the data into Hadoop, and
<a href="http://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying">much more</a>.</p>

<p>LinkedIn built a technology called <a href="http://www.socc2012.org/s18-das.pdf?attredirects=0">Databus</a> to
do this. The <a href="https://github.com/linkedin/databus">open source release of Databus</a> is for Oracle DB,
and there is a proof-of-concept <a href="https://github.com/linkedin/databus/wiki/Databus-for-MySQL">MySQL version</a>
(which is different from the version of Databus for MySQL that LinkedIn uses in production).</p>

<p>The new project I am working on, <a href="http://samza.incubator.apache.org/">Apache Samza</a>, also sits
squarely in this space – it is a framework for processing real-time data feeds, somewhat like
MapReduce for streams. I am excited about it because I think this pattern of processing change
capture streams can help many people build apps that scale better, are easier to maintain and more
reliable than many apps today. It’s open source, and you should go and
<a href="http://samza.incubator.apache.org/">try it out</a>.</p>

<h2 id="in-conclusion">In conclusion</h2>

<p>The problems discussed in this post are primarily data systems problems. That’s no coincidence:
if you write your applications in a stateless way, they are pretty easy to scale, since you can
just run more copies of them. Thus, whether you use Rails or Express.js or whatever framework
<em>du jour</em> really doesn’t matter much. The hard part is scaling the stateful parts of your system:
your databases.</p>

<p>There are no easy solutions for these problems. Some new technologies and services can help –
for example, the new generation of distributed datastores tries to solve some of the above problems
(especially around automating replication and failover), but they have other limitations. There
certainly is no panacea.</p>

<p>Personally I’m totally fine with using new and experimental tools for derived data, such as caches
and analytics, where data loss is annoying but not end of your business. I’m more cautious with the
system of record (also known as <em>source of truth</em>). Every system has operational quirks, and the
devil you know may let you sleep better at night than the one you don’t. I don’t really mind what
that devil is in your particular case.</p>

<p>I’m interested to see whether database-as-a-service offerings such as
<a href="https://www.firebase.com/">Firebase</a>, <a href="http://orchestrate.io/">Orchestrate</a> or
<a href="https://fauna.org/">Fauna</a> can help (I’ve not used any of them seriously, so I can’t vouch for them
at this point). I see big potential advantages for small teams in outsourcing operations, but also
a big potential risk in locking yourself to a system that you couldn’t choose to host yourself if
necessary.</p>

<p>Building scalable systems is not all sexy roflscale fun. It’s a lot of plumbing and yak shaving.
A lot of hacking together tools that really ought to exist already, but all the open source
solutions out there are too bad (and yours ends up bad too, but at least it solves your particular
problem).</p>

<p>On the other hand, consider yourself lucky. If you’ve got scaling problems, you must be doing
something right – you must be making something that people want.</p>

                ]]></content:encoded>
            </item>
        
            <item>
                <title>LinkedIn Intro: Doing the Impossible on iOS</title>
                <link>http://martin.kleppmann.com/2013/10/23/linkedin-intro-doing-the-impossible-on-ios.html</link>
                <comments>http://martin.kleppmann.com/2013/10/23/linkedin-intro-doing-the-impossible-on-ios.html#disqus_thread</comments>
                <pubDate>Wed, 23 Oct 2013 00:00:00 +0000</pubDate>
                <dc:creator>Martin Kleppmann</dc:creator>
                
                    <guid isPermaLink="true">http://martin.kleppmann.com/2013/10/23/linkedin-intro-doing-the-impossible-on-ios.html</guid>
                
                <description><![CDATA[ This is a copy of a post I originally wrote on the LinkedIn engineering blog. We recently launched LinkedIn Intro — a new product that shows you LinkedIn profiles, right inside the native iPhone mail client. That’s right: we have extended Apple’s built-in iOS Mail app, a feat that many... ]]></description>
                <content:encoded><![CDATA[
                    <p><em>This is a copy of a post I originally wrote on the
<a href="http://engineering.linkedin.com/mobile/linkedin-intro-doing-impossible-ios">LinkedIn engineering blog</a>.</em></p>

<p>We recently launched <a href="https://intro.linkedin.com/">LinkedIn Intro</a> — a new product that shows you
LinkedIn profiles, <em>right inside the native iPhone mail client</em>. That’s right: we have extended
Apple’s built-in iOS Mail app, a feat that many people consider to be impossible. This post is
a short summary of how Intro works, and some of the ways we bent technology to our will.</p>

<p>With Intro, you can see at a glance the picture of the person who’s emailing you, learn more about
their background, and connect with them on LinkedIn. This is what it looks like:</p>

<p style="text-align: center; font-style: italic; color: #666; font-size: 85%;">
<img src="/2013/10/intro_before_after.png" width="550" alt="The iPhone mail app, before and after Intro" />
<br />The iPhone mail app, before and after Intro</p>

<h2 id="how-intro-came-to-be">How Intro Came to Be</h2>

<p>The origins of Intro go back to before the acquisition of <a href="http://rapportive.com/">Rapportive</a> by
LinkedIn. At Rapportive, we had built a browser extension that modified Gmail to show the profile of
an email’s sender within the Gmail page. The product was popular, but people kept asking: “I love
Rapportive in Gmail, when can I have it on mobile too?”</p>

<p>The magic of Rapportive is that you don’t have to remember to use it. Once you have it installed, it
is right there inside your email, showing you everything you need to know about your contacts. You
don’t need to fire up a new app or do a search in another browser tab, because the information is
right there when you need it. It just feels natural.</p>

<p>At LinkedIn, we want to <a href="http://www.youtube.com/watch?v=t6SdSqI0JR0">work wherever our members work</a>.
And we know that professionals spend a lot of time on their phone, checking and replying to emails
— so we had to figure out how to enhance mobile email, giving professionals the information they
need to be brilliant with people.</p>

<p>But how do we do that? Ask any iOS engineer: there is no API for extending the built-in mail app on
the iPhone. If you wanted to build something like Rapportive, most people would tell you that it is
impossible. Yet we figured it out.</p>

<h2 id="impossible-1-extending-the-ios-mail-client">Impossible #1: Extending the iOS Mail Client</h2>

<p>Our key insight was this: we cannot extend the mail client, but we can add information to the
messages themselves. One way to do this would be to modify the messages on the server — but then the
modification would appear on all your clients, both desktop and mobile. That would not be what users
want.</p>

<p>Instead, we can add information to messages by using a proxy server.</p>

<p style="text-align: center; font-style: italic; color: #666; font-size: 85%;">
<img src="/2013/10/intro_imap_proxy.png" width="550" alt="Rewriting messages using an IMAP proxy" />
<br />Rewriting messages using an IMAP proxy</p>

<p>Normally your device connects directly to the servers of your email provider (Gmail, Yahoo, AOL,
etc.), but we can configure the device to connect to the Intro proxy server instead.</p>

<p>The Intro proxy server speaks the <a href="http://tools.ietf.org/html/rfc3501">IMAP</a> protocol just like an
email provider, but it doesn’t store messages itself. Instead, it forwards requests from the device
to your email provider, and forwards responses from the email provider back to the device. En route,
it inserts Intro information at the beginning of each message body — we call this the top bar.</p>

<p>The great thing about this approach: the proxy server can tailor the top bar to the device, since it
knows which device is downloading the message. It can adapt the layout to be appropriate to the
screen size, and it can take advantage of the client’s latest features, because it doesn’t need to
worry about compatibility with other devices.</p>

<p>Our proxy server is written in Ruby using EventMachine, which allows it to efficiently handle many
concurrent IMAP connections. We have developed some libraries to make the evented programming model
nicer to work with, including
<a href="https://github.com/samstokes/deferrable_gratification">Deferrable Gratification</a> and
<a href="https://github.com/ConradIrwin/lspace">LSpace</a>.</p>

<h2 id="impossible-2-interactive-ui-in-email">Impossible #2: Interactive UI in Email</h2>

<p>Ok, we have a way of adding information about the sender to a message — but so far it’s just
a static piece of HTML. The top bar is deliberately minimal, because we don’t want it to get in the
way. But wouldn’t it be <em>awesome</em> if you could tap the top bar and see the full LinkedIn profile…
without leaving the mail app?</p>

<p>“But that’s impossible,” they cry, “you can’t run JavaScript in the mail client!” And that’s true
— any JavaScript in an email is simply ignored. But iOS Mail does have powerful CSS capabilities,
since it uses the same rendering engine as Safari.</p>

<p>Recall that CSS has a <code>:hover</code> state that is triggered when you hover the mouse over an element.
This is used for popup menus in the navigation of many websites, or for tooltips. But what do you do
on a touchscreen device, where there is no hovering or clicking, only tapping?</p>

<p>A little-known fact about CSS on Mobile Safari: in certain circumstances, tapping a link once
simulates a <code>:hover</code> state on that link, and tapping it twice has the effect of a click. Thanks to
this feature, popup menus and tooltips still work on iOS.</p>

<p>With some creativity, we figured out how to use this effect to create an interactive user interface
<em>within</em> a message! Just tap the top bar to see the full LinkedIn profile:</p>

<p style="text-align: center; font-style: italic; color: #666; font-size: 85%;">
<img src="/2013/10/intro_drawer.png" width="550" alt="With CSS tricks we can embed an entire LinkedIn profile in a message" />
<br />With CSS tricks we can embed an entire LinkedIn profile in a message</p>

<h2 id="impossible-3-dynamic-content-in-email">Impossible #3: Dynamic Content in Email</h2>

<p>This <code>:hover</code> trick allows us to have some interactivity within a message, but for more complex
interactions we have to take you to the browser (where we can run a normal web app, without the mail
app’s limitations). For example, if you want to connect with your contact on LinkedIn, we take you
to Safari.</p>

<p>That’s fine, but it leaves us with a problem: the top bar needs to show if you’re already connected
with someone. Say you send an invitation, and the other person accepts — now you’re connected, but
if you open the same email again, it still says that you’re not connected!</p>

<p>This is because once a message has been downloaded, an IMAP client may assume that the message will
never change. It is cached on the device, and unlike a web page, it never gets refreshed. Now that
you’re connected, the top bar content needs to change. How do we update it?</p>

<p>Our solution: the connect button is in a tiny <code>&amp;lt;iframe&amp;gt;</code> which is refreshed every time you
open the message. And if you open the message while your device is offline? No problem: the <code>iframe</code>
is positioned on top of an identical-looking button in the static top bar HTML. If the <code>iframe</code>
fails to load, it simply falls back to the connection status at the time when the message was
downloaded.</p>

<p>This allows the top bar to contain dynamic content, even though it’s impossible for the server to modify a message once it has been downloaded by the device.</p>

<p style="text-align: center; font-style: italic; color: #666; font-size: 85%;">
<img src="/2013/10/intro_connection_status.png" width="550" alt="Using an embedded iframe to keep the connection status up-to-date, within an otherwise static top bar" />
<br />Using an embedded iframe to keep the connection status up-to-date, within an otherwise static top bar</p>

<h2 id="impossible-4-easy-installation">Impossible #4: Easy Installation</h2>

<p>Once we got the IMAP proxy working, we were faced with another problem: how do we configure a device
to use the proxy? We cannot expect users to manually enter IMAP and SMTP hostnames, choose the
correct TLS settings, etc — it’s too tedious and error-prone.</p>

<p>Fortunately, Apple provides a friendly way of setting up email accounts by using
<a href="https://developer.apple.com/library/ios/featuredarticles/iPhoneConfigurationProfileRef/">configuration profiles</a>
— a facility that is often used in enterprise deployments of iOS devices. Using this technique, we
can simply ask the user for their email address and password, autodiscover the email provider
settings, and send a configuration profile to the device. The user just needs to tap “ok” a few
times, and then they have a new mail account.</p>

<p>Moreover, for Gmail and Google Apps accounts, we can use OAuth, and never need to ask for the user’s password. Even better!</p>

<p style="text-align: center; font-style: italic; color: #666; font-size: 85%;">
<img src="/2013/10/intro_installer_0.png" width="550" alt="iOS configuration profiles make setup of new email accounts a breeze" />
<br />iOS configuration profiles make setup of new email accounts a breeze</p>

<h2 id="security-and-privacy">Security and Privacy</h2>

<p>We understand that operating an email proxy server carries great responsibility. We respect the fact
that your email may contain very personal or sensitive information, and we will do everything we can
to make sure that it is safe. Our principles and key security measures are detailed in our
<a href="https://intro.linkedin.com/micro/privacy">pledge of privacy</a>.</p>

<h2 id="conclusion">Conclusion</h2>

<p>When we first built Rapportive for Gmail, people thought that we were crazy — writing a browser
extension that modified the Gmail page on the fly, effectively writing an application inside someone
else’s application! But it turned out to be a great success, and many others have since followed our
footsteps and written browser extensions for Gmail.</p>

<p>Similarly, Intro’s approach of proxying IMAP is a novel way of delivering software to users. It
operates at the limit of what is technically possible, but it has a big advantage: we can enhance
the apps you already use. Of course the idea isn’t limited to the iPhone, so watch out for new
platforms coming your way soon :)</p>

<p>This post has only scratched the surface of the interesting challenges we have overcome while
building Intro. In follow-up posts we will talk about some of our CSS techniques, testing and
monitoring tools, things we do to achieve high performance and high reliability, and more. In the
meantime, check out <a href="https://intro.linkedin.com/">Intro</a> and let us know what you think!</p>

                ]]></content:encoded>
            </item>
        
            <item>
                <title>System operations over seven centuries</title>
                <link>http://martin.kleppmann.com/2013/08/12/system-operations-over-seven-centuries.html</link>
                <comments>http://martin.kleppmann.com/2013/08/12/system-operations-over-seven-centuries.html#disqus_thread</comments>
                <pubDate>Mon, 12 Aug 2013 00:00:00 +0000</pubDate>
                <dc:creator>Martin Kleppmann</dc:creator>
                
                    <guid isPermaLink="true">http://martin.kleppmann.com/2013/08/12/system-operations-over-seven-centuries.html</guid>
                
                <description><![CDATA[ On a walk in the Alps last week we came across a wonderful piece of engineering, more successful than most software systems could claim to be. It is the system of Waale, an ancient irrigation system in the Vinschgau, South Tyrol. The climate in the Vinschgau is sunny, dry and... ]]></description>
                <content:encoded><![CDATA[
                    <p>On a <a href="http://www.planetoutdoor.de/de/touren/detail.htm?tour=20874&amp;region=138">walk</a> in the Alps last
week we came across a wonderful piece of engineering, more successful than most software systems
could claim to be. It is the system of <em><a href="http://de.wikipedia.org/wiki/Waal_%28Bew%C3%A4sserung%29">Waale</a></em>,
an ancient irrigation system in the <a href="http://en.wikipedia.org/wiki/Vinschgau">Vinschgau</a>, South
Tyrol.</p>

<p>The climate in the Vinschgau is sunny, dry and windy. Without irrigation, agriculture would barely
be possible, but if water from mountain streams is channelled to the fields,
<a href="http://www.vip.coop/">apple trees</a> and meadows can flourish. The area has been inhabited at least
since the Bronze Age, and it is likely that artificial irrigation started early. The oldest
documents on the Waal system date from the 12th century, and some Waale built in the 14th century
are still in use today.</p>

<p>
  <a style="display: block; float: left" href="http://www.flickr.com/photos/martinkleppmann/9485964300/in/set-72157635017969591"><img src="/2013/08/leitenwaal-1.jpg" width="250" height="375" /></a>
  <a style="display: block; float: right" href="http://www.flickr.com/photos/martinkleppmann/9482339202/in/set-72157635017969591"><img src="/2013/08/leitenwaal-2.jpg" width="250" height="375" /></a>
</p>
<p style="clear: both"></p>

<p>The pictures in this post show the Leitenwaal and the
<a href="http://www.youtube.com/watch?v=hO0BAjPh5n4">Berkwaal</a> near the village of
<a href="http://en.wikipedia.org/wiki/Schluderns">Schluderns</a> in South Tyrol, northern Italy. These two
conduits carry water from a mountain stream (the Saldurbach) to the fields and meadows around
Schluderns. Along their combined length of about six kilometers, they overcome many obstacles:
twisting along the face of steep mountainsides, crossing aqueducts over deep ravines, tunnelling
underneath boulders, before they finally arrive at the fields they supply.</p>

<p>Some sections look almost like a natural stream – except that they flow across the mountainside,
not down, because they are designed to cover the greatest possible distance with the smallest
possible loss in altitude. Other sections are more obviously artificial, where the furrow has been
lined with flat stones or blanks of wood.</p>

<p>This system was originally built almost 700 years ago, using the technology available at the time:
spade, axe, hammer and chisel. Of course, nowadays, electric pumps can take water from the river
at the valley floor, and sprinkle it on the fields on the slopes above. But for many centuries, the
only feasible option was to take water from a stream at high altitude, and let it flow down from
there.</p>

<p><a href="http://www.flickr.com/photos/martinkleppmann/9483183131/in/set-72157635017969591"><img src="/2013/08/source.jpg" width="550" height="366" /></a></p>

<p>Here a feed of water is taken from a stream, and carried along a wooden gulley: the input to the
irrigation system. Along the way, gates regulate the flow of water in the direction of various
farms. For centuries, the details of water distribution – how much water shall be directed towards
which farm at which time – have been governed by detailed agreements, and led to many disputes
between farmers.</p>

<p><a href="http://www.flickr.com/photos/martinkleppmann/9482379264/in/set-72157635017969591"><img src="/2013/08/distribution.jpg" width="550" height="366" /></a></p>

<p>If the system were to fail for too long, crops would wither, so it was important that the system was
always well-maintained and operational. And of course, parts of the system would fail from time to
time – erosion, landslides, decay, accidents or any number of other faults could occur. When a part
of the system broke, it was replaced using whatever technology was available at the time.</p>

<p>Thus, the system is now a patchwork of different water-carrying technologies from different ages.
The oldest “pipes” were made from hollowed-out tree trunks, and some of them are still in use (water
flows through tree trunks across a ravine in the left picture below). Later replacements have been
made with concrete, steel or plastic pipes – whatever is believed to be the most reliable solution
in the long term.</p>

<p>
  <a style="display: block; float: left" href="http://www.flickr.com/photos/martinkleppmann/9485976696/in/set-72157635017969591"><img src="/2013/08/pipes-1.jpg" width="250" height="375" /></a>
  <a style="display: block; float: right" href="http://www.flickr.com/photos/martinkleppmann/9485971172/in/set-72157635017969591"><img src="/2013/08/pipes-2.jpg" width="250" height="375" /></a>
</p>
<p style="clear: both"></p>

<p>Perhaps the most impressive aspect of this system are its <em>operability</em> features, i.e. the things
that help the operator of the Waal in his job of keeping the system running smoothly. For example,
at regular intervals, the water flows through gratings which filter out twigs or other objects
before they can cause blockages in pipes. The gratings are cleaned regularly, and tools for clearing
out pipes are kept near the Waal. Routine inspections help detect problems early, before they
escalate and cause further damage.</p>

<p>After heavy rainfall or melting of snow, the influx of water may exceed the Waal’s capacity. This is
problematic: if the Waal bursts its banks, those banks would be damaged by erosion or washed away,
making the problem much worse. Thus, the system includes overflow points at which water is
channelled back into the natural stream if the Waal is over capacity (left photo below).</p>

<p>There is even an ingenious monitoring system (right photo below). A waterwheel is placed in the
stream, and a cowbell is attached so that it rings on each rotation of the wheel
(<a href="http://www.youtube.com/watch?v=LbCG8AUJKSk">video</a>). Thus, the operator can tell the rate of water
flow from a distance, simply by listening for the rhythm of the bell.</p>

<p>
  <a style="display: block; float: left" href="http://www.flickr.com/photos/martinkleppmann/9483176701/in/set-72157635017969591"><img src="/2013/08/operations-1.jpg" width="250" height="375" /></a>
  <a style="display: block; float: right" href="http://www.flickr.com/photos/martinkleppmann/9485968436/in/set-72157635017969591"><img src="/2013/08/operations-2.jpg" width="250" height="375" /></a>
</p>
<p style="clear: both"></p>

<p>The <em>Waaler</em>, the operator in charge of maintenance of the <em>Waal</em>, is an important and
highly-regarded member of the local community. Traditionally, this role is elected every year on the
first Sunday of Lent. The operator can be re-elected by the community if they were satisfied with
his work in the previous year.</p>

<p>Looking at the lessons from this ancient irrigation system, and adapting them to software systems,
my take-aways are:</p>

<ul>
  <li>Good interface design can survive through multiple generations of technology. A stream of water,
flowing downhill, is a simple interface that can be implemented in stone-lined furrows,
hollowed-out tree trunks, concrete, steel and plastic pipes, and more.</li>
  <li>When replacing obsolete technology with new technology, some work is required to join them up –
two pieces of standardised plastic piping may fit snugly, but you can’t expect the same from a
hollow tree trunk interfacing with a plastic pipe.</li>
  <li>New technology is not necessarily better than old technology. Hollow tree trunks are still used
to feed water into 21st-century sprinkler irrigation systems.</li>
  <li>API rate limits are not a new thing.</li>
  <li>Continuously monitor the health of your system, and detect problems early.</li>
  <li>Operations doesn’t just happen; it has to be someone’s job.</li>
  <li>If a system solves an important problem, is well-engineered and well-operated, it can stick around
for a very, very long time.</li>
</ul>

                ]]></content:encoded>
            </item>
        
            <item>
                <title>Improving the security of your SSH private key files</title>
                <link>http://martin.kleppmann.com/2013/05/24/improving-security-of-ssh-private-keys.html</link>
                <comments>http://martin.kleppmann.com/2013/05/24/improving-security-of-ssh-private-keys.html#disqus_thread</comments>
                <pubDate>Fri, 24 May 2013 00:00:00 +0000</pubDate>
                <dc:creator>Martin Kleppmann</dc:creator>
                
                    <guid isPermaLink="true">http://martin.kleppmann.com/2013/05/24/improving-security-of-ssh-private-keys.html</guid>
                
                <description><![CDATA[ Ever wondered how those key files in ~/.ssh actually work? How secure are they actually? As you probably do too, I use ssh many times every single day — every git fetch and git push, every deploy, every login to a server. And recently I realised that to me, ssh... ]]></description>
                <content:encoded><![CDATA[
                    <p>Ever wondered how those key files in <code>~/.ssh</code> actually <em>work</em>? How secure are they actually?</p>

<p>As you probably do too, I use ssh many times every single day — every <code>git fetch</code> and <code>git push</code>,
every deploy, every login to a server. And recently I realised that to me, ssh was just some crypto
voodoo that I had become accustomed to using, but I didn’t really understand. That’s a shame — I
like to know how stuff works. So I went on a little journey of discovery, and here are some of the
things I found.</p>

<p>When you start reading about “crypto stuff”, you very quickly get buried in an avalanche of
acronyms. I will briefly mention the acronyms as we go along; they don’t help you understand the
concepts, but they are useful in case you want to Google for further details.</p>

<p>Quick recap: If you’ve ever used public key authentication, you probably have a file <code>~/.ssh/id_rsa</code>
or <code>~/.ssh/id_dsa</code> in your home directory. This is your RSA/DSA private key, and <code>~/.ssh/id_rsa.pub</code>
or <code>~/.ssh/id_dsa.pub</code> is its public key counterpart. Any machine you want to log in to needs to
have your public key in <code>~/.ssh/authorized_keys</code> on that machine. When you try to log in, your SSH
client uses a digital signature to prove that you have the private key; the server checks that the
signature is valid, and that the public key is authorized for your username; if all is well, you are
granted access.</p>

<p>So what is actually inside this private key file?</p>

<h2 id="the-unencrypted-private-key-format">The unencrypted private key format</h2>

<p>Everyone recommends that you protect your private key with a passphrase (otherwise anybody who
steals the file from you can log into everything you have access to). If you leave the passphrase
blank, the key is not encrypted. Let’s look at this unencrypted format first, and consider
passphrase protection later.</p>

<p>A ssh private key file typically looks something like this:</p>

<pre><code>-----BEGIN RSA PRIVATE KEY-----
MIIEogIBAAKCAQEArCQG213utzqE5YVjTVF5exGRCkE9OuM7LCp/FOuPdoHrFUXk
y2MQcwf29J3A4i8zxpES9RdSEU6iIEsow98wIi0x1/Lnfx6jG5Y0/iQsG1NRlNCC
aydGvGaC+PwwWiwYRc7PtBgV4KOAVXMZdMB5nFRaekQ1ksdH/360KCGgljPtzTNl
09e97QBwHFIZ3ea5Eih/HireTrRSnvF+ywmwuxX4ubDr0ZeSceuF2S5WLXH2+TV0
   ... etc ... lots of base64 blah blah ...
-----END RSA PRIVATE KEY-----
</code></pre>

<p>The private key is an <a href="http://en.wikipedia.org/wiki/ASN.1">ASN.1</a> data structure, serialized to a
byte string using <a href="http://en.wikipedia.org/wiki/X.690#DER_encoding">DER</a>, and then
<a href="http://tools.ietf.org/html/rfc4648">Base64</a>-encoded. ASN.1 is roughly comparable to JSON (it
supports various data types such as integers, booleans, strings and lists/sequences that can be
nested in a tree structure). It’s very widely used for cryptographic purposes, but it has somehow
fallen out of fashion with the web generation (I don’t know why, it seems like a pretty decent
format).</p>

<p>To look inside, let’s generate a fake RSA key without passphrase using
<a href="http://www.openbsd.org/cgi-bin/man.cgi?query=ssh-keygen&amp;sektion=1">ssh-keygen</a>, and then decode it
using <a href="http://www.openssl.org/docs/apps/asn1parse.html">asn1parse</a>:</p>

<pre><code>$ ssh-keygen -t rsa -N '' -f test_rsa_key
$ openssl asn1parse -in test_rsa_key
    0:d=0  hl=4 l=1189 cons: SEQUENCE
    4:d=1  hl=2 l=   1 prim: INTEGER           :00
    7:d=1  hl=4 l= 257 prim: INTEGER           :C36EB2429D429C7768AD9D879F98C...
  268:d=1  hl=2 l=   3 prim: INTEGER           :010001
  273:d=1  hl=4 l= 257 prim: INTEGER           :A27759F60AEA1F4D1D56878901E27...
  534:d=1  hl=3 l= 129 prim: INTEGER           :F9D23EF31A387694F03AD0D050265...
  666:d=1  hl=3 l= 129 prim: INTEGER           :C84415C26A468934F1037F99B6D14...
  798:d=1  hl=3 l= 129 prim: INTEGER           :D0ACED4635B5CA5FB896F88BB9177...
  930:d=1  hl=3 l= 128 prim: INTEGER           :511810DF9AFD590E11126397310A6...
 1061:d=1  hl=3 l= 129 prim: INTEGER           :E3A296AE14E7CAF32F7E493FDF474...
</code></pre>

<p>Alternatively, you can paste the Base64 string into Lapo Luchini’s excellent
<a href="http://lapo.it/asn1js/">JavaScript ASN.1 decoder</a>. You can see that ASN.1 structure is quite
simple: a sequence of nine integers. Their meaning is defined in
<a href="http://tools.ietf.org/html/rfc2313#section-7.2">RFC2313</a>. The first integer is a version number
(0), and the third number is quite small (65537) – the public exponent <em>e</em>. The two important
numbers are the 2048-bit integers that appear second and fourth in the sequence: the RSA modulus
<em>n</em>, and the private exponent <em>d</em>. These numbers are used directly in the
<a href="http://en.wikipedia.org/wiki/RSA_%28algorithm%29">RSA algorithm</a>. The remaining five numbers can
be derived from <em>n</em> and <em>d</em>, and are only cached in the key file to speed up certain operations.</p>

<p>DSA keys are similar, a
<a href="http://blog.ngas.ch/archives/2008/10/23/asn_1_for_dsa_public_and_private_keys/index.html">sequence of six integers</a>:</p>

<pre><code>$ ssh-keygen -t dsa -N '' -f test_dsa_key
$ openssl asn1parse -in test_dsa_key
    0:d=0  hl=4 l= 444 cons: SEQUENCE
    4:d=1  hl=2 l=   1 prim: INTEGER           :00
    7:d=1  hl=3 l= 129 prim: INTEGER           :E497DFBFB5610906D18BCFB4C3CCD...
  139:d=1  hl=2 l=  21 prim: INTEGER           :CF2478A96A941FB440C38A86F22CF...
  162:d=1  hl=3 l= 129 prim: INTEGER           :83218C0CA49BA8F11BE40EE1A7C72...
  294:d=1  hl=3 l= 128 prim: INTEGER           :16953EA4012988E914B466B9C37CB...
  425:d=1  hl=2 l=  21 prim: INTEGER           :89A356E922688EDEB1D388258C825...
</code></pre>

<h2 id="passphrase-protected-keys">Passphrase-protected keys</h2>

<p>Next, in order to make life harder for an attacker who manages to steal your private key file, you
protect it with a passphrase. How does this actually work?</p>

<pre><code>$ ssh-keygen -t rsa -N 'super secret passphrase' -f test_rsa_key
$ cat test_rsa_key
-----BEGIN RSA PRIVATE KEY-----
Proc-Type: 4,ENCRYPTED
DEK-Info: AES-128-CBC,D54228DB5838E32589695E83A22595C7

3+Mz0A4wqbMuyzrvBIHx1HNc2ZUZU2cPPRagDc3M+rv+XnGJ6PpThbOeMawz4Cbu
lQX/Ahbx+UadJZOFrTx8aEWyZoI0ltBh9O5+ODov+vc25Hia3jtayE51McVWwSXg
wYeg2L6U7iZBk78yg+sIKFVijxiWnpA7W2dj2B9QV0X3ILQPxbU/cRAVTd7AVrKT
    ... etc ...
-----END RSA PRIVATE KEY-----
</code></pre>

<p>We’ve gained two header lines, and if you try to parse that Base64 text, you’ll find it’s no longer
valid ASN.1. That’s because the entire ASN.1 structure we saw above has been encrypted, and the
Base64-encoded text is the output of the encryption. The header tells us the encryption algorithm
that was used: <a href="http://en.wikipedia.org/wiki/Advanced_Encryption_Standard">AES-128</a> in
<a href="http://en.wikipedia.org/wiki/Block_cipher_modes_of_operation#Cipher-block_chaining_.28CBC.29">CBC mode</a>.
The 128-bit hex string in the <code>DEK-Info</code> header is the
<a href="http://en.wikipedia.org/wiki/Initialization_vector">initialization vector</a> (IV) for the cipher.
This is pretty standard stuff; all common crypto libraries can handle it.</p>

<p>But how do you get from the passphrase to the AES encryption key? I couldn’t find it documented
anywhere, so I had to dig through the OpenSSL source to find it:</p>

<ol>
  <li>Append the first 8 bytes of the IV to the passphrase, without a separator (serves as a salt).</li>
  <li>Take the MD5 hash of the resulting string (once).</li>
</ol>

<p>That’s it.  To prove it, let’s decrypt the private key manually (using the IV/salt from the
<code>DEK-Info</code> header above):</p>

<pre><code>$ tail -n +4 test_rsa_key | grep -v 'END ' | base64 -D |    # get just the binary blob
  openssl aes-128-cbc -d -iv D54228DB5838E32589695E83A22595C7 -K $(
    ruby -rdigest/md5 -e 'puts Digest::MD5.hexdigest(["super secret passphrase",0xD5,0x42,0x28,0xDB,0x58,0x38,0xE3,0x25].pack("a*cccccccc"))'
  ) |
  openssl asn1parse -inform DER
</code></pre>

<p>…which prints out the sequence of integers from the RSA key in the clear. Of course, if you want
to inspect the key, it’s much easier to do this:</p>

<pre><code>$ openssl rsa -text -in test_rsa_key -passin 'pass:super secret passphrase'
</code></pre>

<p>but I wanted to demonstrate exactly how the AES key is derived from the password. This is important
because the private key protection has two weaknesses:</p>

<ul>
  <li>The digest algorithm is hard-coded to be MD5, which means that without changing the format, it’s
not possible to upgrade to another hash function (e.g. SHA-1). This could be a problem if MD5
turns out not to be good enough.</li>
  <li>The hash function is only applied once — there is no
<a href="http://en.wikipedia.org/wiki/Key_stretching">stretching</a>. This is a problem because MD5 and AES
are both fast to compute, and thus a short passphrase is quite easy to break with brute force.</li>
</ul>

<p>If your private SSH key ever gets into the wrong hands, e.g. because someone steals your laptop or
your backup hard drive, the attacker can try a huge number of possible passphrases, even with
moderate computing resources. If your passphrase is a dictionary word, it can probably be broken in
a matter of seconds.</p>

<p>That was the bad news: the passphrase on your SSH key isn’t as useful as you thought it was.
But there is good news: you can upgrade to a more secure private key format, and everything
continues to work!</p>

<h2 id="better-key-protection-with-pkcs8">Better key protection with PKCS#8</h2>

<p>What we want is to derive a symmetric encryption key from the passphrase, and we want this
derivation to be slow to compute, so that an attacker needs to buy more computing time if they want
to brute-force the passphrase. If you’ve seen the <a href="http://codahale.com/how-to-safely-store-a-password/">use bcrypt</a>
meme, this should sound very familiar.</p>

<p>For SSH private keys, there are a few standards with clumsy names (acronym alert!) that can help us out:</p>

<ul>
  <li><a href="http://tools.ietf.org/html/rfc2898#section-5.2">PKCS #5 (RFC 2898)</a> defines
<a href="http://en.wikipedia.org/wiki/PBKDF2">PBKDF2</a> (Password-Based Key Derivation Function 2), an
algorithm for deriving an encryption key from a password by applying a hash function repeatedly.
PBES2 (Password-Based Encryption Scheme 2) is also defined here; it simply means using a
PBKDF2-generated key with a symmetric cipher.</li>
  <li><a href="http://tools.ietf.org/html/rfc5208">PKCS #8 (RFC 5208)</a> defines a format for storing encrypted
private keys that supports PBKDF2. OpenSSL transparently supports private keys in PKCS#8 format,
and OpenSSH uses OpenSSL, so if you’re using OpenSSH that means you can swap your traditional SSH
key files for PKCS#8 files and everything continues to work as normal!</li>
</ul>

<p>I don’t know why <code>ssh-keygen</code> still generates keys in SSH’s traditional format, even though a better
format has been available for years. Compatibility with servers is not a concern, because the
private key never leaves your machine. Fortunately it’s easy enough to
<a href="http://www.openssl.org/docs/apps/pkcs8.html">convert to PKCS#8</a>:</p>

<pre><code>$ mv test_rsa_key test_rsa_key.old
$ openssl pkcs8 -topk8 -v2 des3 \
    -in test_rsa_key.old -passin 'pass:super secret passphrase' \
    -out test_rsa_key -passout 'pass:super secret passphrase'
</code></pre>

<p>If you try using this new PKCS#8 file with a SSH client, you should find that it works exactly the
same as the file generated by <code>ssh-keygen</code>. But what’s inside it?</p>

<pre><code>$ cat test_rsa_key
-----BEGIN ENCRYPTED PRIVATE KEY-----
MIIFDjBABgkqhkiG9w0BBQ0wMzAbBgkqhkiG9w0BBQwwDgQIOu/S2/v547MCAggA
MBQGCCqGSIb3DQMHBAh4q+o4ELaHnwSCBMjA+ho9K816gN1h9MAof4stq0akPoO0
CNvXdtqLudIxBq0dNxX0AxvEW6exWxz45bUdLOjQ5miO6Bko0lFoNUrOeOo/Gq4H
dMyI7Ot1vL9UvZRqLNj51cj/7B/bmfa4msfJXeuFs8jMtDz9J19k6uuCLUGlJscP
    ... etc ...
-----END ENCRYPTED PRIVATE KEY-----
</code></pre>

<p>Notice that the header/footer lines have changed (<code>BEGIN ENCRYPTED PRIVATE KEY</code> instead of
<code>BEGIN RSA PRIVATE KEY</code>), and the plaintext <code>Proc-Type</code> and <code>DEK-Info</code> headers have gone. In fact,
the whole key file is once again a ASN.1 structure:</p>

<pre><code>$ openssl asn1parse -in test_rsa_key
    0:d=0  hl=4 l=1294 cons: SEQUENCE
    4:d=1  hl=2 l=  64 cons: SEQUENCE
    6:d=2  hl=2 l=   9 prim: OBJECT            :PBES2
   17:d=2  hl=2 l=  51 cons: SEQUENCE
   19:d=3  hl=2 l=  27 cons: SEQUENCE
   21:d=4  hl=2 l=   9 prim: OBJECT            :PBKDF2
   32:d=4  hl=2 l=  14 cons: SEQUENCE
   34:d=5  hl=2 l=   8 prim: OCTET STRING      [HEX DUMP]:3AEFD2DBFBF9E3B3
   44:d=5  hl=2 l=   2 prim: INTEGER           :0800
   48:d=3  hl=2 l=  20 cons: SEQUENCE
   50:d=4  hl=2 l=   8 prim: OBJECT            :des-ede3-cbc
   60:d=4  hl=2 l=   8 prim: OCTET STRING      [HEX DUMP]:78ABEA3810B6879F
   70:d=1  hl=4 l=1224 prim: OCTET STRING      [HEX DUMP]:C0FA1A3D2BCD7A80DD61F4C0287F8B2D...
</code></pre>

<p>Use Lapo Luchini’s <a href="http://lapo.it/asn1js/">JavaScript ASN.1 decoder</a> to display a nice ASN.1 tree
structure:</p>

<pre><code>Sequence (2 elements)
|- Sequence (2 elements)
|  |- Object identifier: 1.2.840.113549.1.5.13            // using PBES2 from PKCS#5
|  `- Sequence (2 elements)
|     |- Sequence (2 elements)
|     |  |- Object identifier: 1.2.840.113549.1.5.12      // using PBKDF2 -- yay! :)
|     |  `- Sequence (2 elements)
|     |     |- Byte string (8 bytes): 3AEFD2DBFBF9E3B3    // salt
|     |     `- Integer: 2048                              // iteration count
|     `- Sequence (2 elements)
|          Object identifier: 1.2.840.113549.3.7          // encrypted with Triple DES, CBC
|          Byte string (8 bytes): 78ABEA3810B6879F        // initialization vector
`- Byte string (1224 bytes): C0FA1A3D2BCD7A80DD61F4C0287F8B2DAB46A43E...  // encrypted key blob
</code></pre>

<p>The format uses <a href="http://en.wikipedia.org/wiki/Object_identifier">OIDs</a>, numeric codes allocated by a
registration authority to unambiguously refer to algorithms. The OIDs in this key file tell us that
the encryption scheme is <a href="http://oid-info.com/get/1.2.840.113549.1.5.13">pkcs5PBES2</a>, that the key
derivation function is <a href="http://oid-info.com/get/1.2.840.113549.1.5.12">PBKDF2</a>, and that the
encryption is performed using <a href="http://oid-info.com/get/1.2.840.113549.3.7">des-ede3-cbc</a>. The hash
function can be explicitly specified if needed; here it’s omitted, which means that it
<a href="http://tools.ietf.org/html/rfc3370#section-4.4.1">defaults</a> to
<a href="http://tools.ietf.org/html/rfc2104">hMAC-SHA1</a>.</p>

<p>The nice thing about having all those identifiers in the file is that if better algorithms are
invented in future, we can upgrade the key file without having to change the container file format.</p>

<p>You can also see that the key derivation function uses an iteration count of 2,048. Compared to just
one iteration in the traditional SSH key format, that’s good — it means that it’s much slower to
brute-force the passphrase. The number 2,048 is currently hard-coded in OpenSSL; I hope that it will
be configurable in future, as you could probably increase it without any noticeable slowdown on a
modern computer.</p>

<h2 id="conclusion-better-protection-for-your-ssh-private-keys">Conclusion: better protection for your SSH private keys</h2>

<p>If you already have a strong passphrase on your SSH private key, then converting it from the
traditional private key format to PKCS#8 is roughly comparable to adding two extra keystrokes to
your passphrase, for free. And if you have a weak passphrase, you can take your private key
protection from “easily breakable” to “slightly harder to break”.</p>

<p>It’s so easy, you can do it right now:</p>

<pre><code>$ mv ~/.ssh/id_rsa ~/.ssh/id_rsa.old
$ openssl pkcs8 -topk8 -v2 des3 -in ~/.ssh/id_rsa.old -out ~/.ssh/id_rsa
$ chmod 600 ~/.ssh/id_rsa
# Check that the converted key works; if yes, delete the old one:
$ rm ~/.ssh/id_rsa.old
</code></pre>

<p>The <code>openssl pkcs8</code> command asks for a passphrase three times: once to unlock your existing private
key, and twice for the passphrase for the new key. It doesn’t matter whether you use a new
passphrase for the converted key or keep it the same as the old key.</p>

<p>Not all software can read the PKCS8 format, but that’s fine — only your SSH client needs to be
able to read the private key, after all. From the server’s point of view, storing the private key in
a different format changes nothing at all.</p>

<p>Update: <a href="https://twitter.com/brendanliamt">Brendan Thompson</a> has wrapped this conversion in a handy
shell script called <a href="https://github.com/BrendanThompson/keycrypt">keycrypt</a>.</p>

<h2 id="update-to-undo-this-change">Update: to undo this change</h2>

<p>On Mac OS X 10.9 (Mavericks), the default installation of OpenSSH no longer supports PKCS#8 private
keys for some reason. If you followed the instructions above, you may no longer be able to log into
your servers. Fortunately, it’s easy to convert your private key from PKCS#8 format back into the
traditional key format:</p>

<pre><code>$ mv ~/.ssh/id_rsa ~/.ssh/id_rsa.pkcs8
$ openssl pkcs8 -in ~/.ssh/id_rsa.pkcs8 -out ~/.ssh/id_rsa
$ chmod 600 ~/.ssh/id_rsa
$ ssh-keygen -f ~/.ssh/id_rsa -p
</code></pre>

<p>The <code>openssl</code> command decrypts the key, and the <code>ssh-keygen</code> command re-encrypts it using the
traditional SSH key format.</p>

                ]]></content:encoded>
            </item>
        
            <item>
                <title>Schema evolution in Avro, Protocol Buffers and Thrift</title>
                <link>http://martin.kleppmann.com/2012/12/05/schema-evolution-in-avro-protocol-buffers-thrift.html</link>
                <comments>http://martin.kleppmann.com/2012/12/05/schema-evolution-in-avro-protocol-buffers-thrift.html#disqus_thread</comments>
                <pubDate>Wed, 05 Dec 2012 00:00:00 +0000</pubDate>
                <dc:creator>Martin Kleppmann</dc:creator>
                
                    <guid isPermaLink="true">http://martin.kleppmann.com/2012/12/05/schema-evolution-in-avro-protocol-buffers-thrift.html</guid>
                
                <description><![CDATA[ So you have some data that you want to store in a file or send over the network. You may find yourself going through several phases of evolution: Using your programming language’s built-in serialization, such as Java serialization, Ruby’s marshal, or Python’s pickle. Or maybe you even invent your own... ]]></description>
                <content:encoded><![CDATA[
                    <p>So you have some data that you want to store in a file or send over the network. You may find
yourself going through several phases of evolution:</p>

<ol>
  <li>Using your programming language’s built-in serialization, such as
<a href="http://docs.oracle.com/javase/6/docs/platform/serialization/spec/serialTOC.html">Java serialization</a>,
Ruby’s <a href="http://www.ruby-doc.org/core-1.9.3/Marshal.html">marshal</a>, or Python’s
<a href="http://docs.python.org/3.3/library/pickle.html">pickle</a>. Or maybe you even invent your own
format.</li>
  <li>Then you realise that being locked into one programming language sucks, so you move to using a
widely supported, language-agnostic format like JSON (or XML if you like to party like it’s
1999).</li>
  <li>Then you decide that JSON is too verbose and too slow to parse, you’re annoyed that it doesn’t
differentiate integers from floating point, and think that you’d quite like binary strings as
well as Unicode strings. So you invent some sort of binary format that’s kinda like JSON, but
binary (<a href="http://msgpack.org/">1</a>, <a href="http://bsonspec.org/">2</a>, <a href="http://ubjson.org/">3</a>,
<a href="http://bjson.org/">4</a>,
<a href="http://kaijaeger.com/articles/introducing-bison-binary-interchange-standard.html">5</a>,
<a href="https://github.com/voldemort/voldemort/wiki/Binary-JSON-Serialization">6</a>).</li>
  <li>Then you find that people are stuffing all sorts of random fields into their objects, using
inconsistent types, and you’d quite like a <strong>schema</strong> and some <strong>documentation</strong>, thank you very
much. Perhaps you’re also using a statically typed programming language and want to generate
model classes from a schema. Also you realize that your binary JSON-lookalike actually isn’t all
that compact, because you’re still storing field names over and over again; hey, if you had a
schema, you could avoid storing objects’ field names, and you could save some more bytes!</li>
</ol>

<p>Once you get to the fourth stage, your options are typically <a href="http://thrift.apache.org/">Thrift</a>,
<a href="http://code.google.com/p/protobuf/">Protocol Buffers</a> or <a href="http://avro.apache.org/">Avro</a>. All three
provide efficient, cross-language serialization of data using a schema, and code generation for the
Java folks.</p>

<p>Plenty of comparisons have been written about them already
(<a href="http://floatingsun.net/articles/thrift-vs-protocol-buffers/">1</a>,
<a href="http://www.igvita.com/2011/08/01/protocol-buffers-avro-thrift-messagepack/">2</a>,
<a href="http://blog.mirthlab.com/2009/06/01/thrift-vs-protocol-bufffers-vs-json/">3</a>,
<a href="http://tech.puredanger.com/2011/05/27/serialization-comparison/">4</a>).
However, many posts overlook a detail that seems mundane at first, but is actually cruicial: <strong>What
happens if the schema changes?</strong></p>

<p>In real life, data is always in flux. The moment you think you have finalised a schema, someone will
come up with a use case that wasn’t anticipated, and wants to “just quickly add a field”.
Fortunately Thrift, Protobuf and Avro all support <strong>schema evolution</strong>: you can change the schema,
you can have producers and consumers with different versions of the schema at the same time, and it
all continues to work. That is an extremely valuable feature when you’re dealing with a big
production system, because it allows you to update different components of the system independently,
at different times, without worrying about compatibility.</p>

<p>Which brings us to the topic of today’s post. I would like to explore how Protocol Buffers, Avro and
Thrift actually encode data into bytes — and this will also help explain how each of them deals
with schema changes. The design choices made by each of the frameworks are interesting, and by
comparing them I think you can become a better engineer (by a little bit).</p>

<p>The example I will use is a little object describing a person. In JSON I would write it like this:</p>

<div class="highlight"><pre><code class="language-js" data-lang="js"><span class="p">{</span>
    <span class="s2">&quot;userName&quot;</span><span class="o">:</span> <span class="s2">&quot;Martin&quot;</span><span class="p">,</span>
    <span class="s2">&quot;favouriteNumber&quot;</span><span class="o">:</span> <span class="mi">1337</span><span class="p">,</span>
    <span class="s2">&quot;interests&quot;</span><span class="o">:</span> <span class="p">[</span><span class="s2">&quot;daydreaming&quot;</span><span class="p">,</span> <span class="s2">&quot;hacking&quot;</span><span class="p">]</span>
<span class="p">}</span></code></pre></div>

<p>This JSON encoding can be our baseline. If I remove all the whitespace it consumes 82 bytes.</p>

<h2 id="protocol-buffers">Protocol Buffers</h2>

<p>The Protocol Buffers schema for the person object might look something like this:</p>

<pre><code>message Person {
    required string user_name        = 1;
    optional int64  favourite_number = 2;
    repeated string interests        = 3;
}
</code></pre>

<p>When we <a href="https://developers.google.com/protocol-buffers/docs/encoding">encode</a> the data above using
this schema, it uses 33 bytes, as follows:</p>

<p><a href="/2012/12/protobuf.png"><img src="/2012/12/protobuf_small.png" width="550" height="230" /></a></p>

<p>Look exactly at how the binary representation is structured, byte by byte. The person record is just
the concatentation of its fields. Each field starts with a byte that indicates its tag number (the
numbers <code>1</code>, <code>2</code>, <code>3</code> in the schema above), and the type of the field. If the first byte of a field
indicates that the field is a string, it is followed by the number of bytes in the string, and then
the UTF-8 encoding of the string. If the first byte indicates that the field is an integer, a
variable-length encoding of the number follows. There is no array type, but a tag number can appear
multiple times to represent a multi-valued field.</p>

<p>This encoding has consequences for schema evolution:</p>

<ul>
  <li>There is no difference in the encoding between <code>optional</code>, <code>required</code> and <code>repeated</code> fields
(except for the number of times the tag number can appear). This means that you can change a field
from <code>optional</code> to <code>repeated</code> and vice versa (if the parser is expecting an <code>optional</code> field but
sees the same tag number multiple times in one record, it discards all but the last value).
<code>required</code> has an additional validation check, so if you change it, you risk runtime errors (if
the sender of a message thinks that it’s optional, but the recipient thinks that it’s required).</li>
  <li>An <code>optional</code> field without a value, or a <code>repeated</code> field with zero values, does not appear in
the encoded data at all — the field with that tag number is simply absent. Thus, it is safe to
remove that kind of field from the schema. However, you must never reuse the tag number for
another field in future, because you may still have data stored that uses that tag for the field
you deleted.</li>
  <li>You can add a field to your record, as long as it is given a new tag number. If the Protobuf
parser parser sees a tag number that is not defined in its version of the schema, it has no way of
knowing what that field is called. But it <em>does</em> roughly know what type it is, because a 3-bit
type code is included in the first byte of the field. This means that even though the parser can’t
exactly interpret the field, it can figure out how many bytes it needs to skip in order to find
the next field in the record.</li>
  <li>You can rename fields, because field names don’t exist in the binary serialization, but you can
never change a tag number.</li>
</ul>

<p>This approach of using a tag number to represent each field is simple and effective. But as we’ll
see in a minute, it’s not the only way of doing things.</p>

<h2 id="avro">Avro</h2>

<p>Avro schemas can be written in two ways, either in a JSON format:</p>

<div class="highlight"><pre><code class="language-js" data-lang="js"><span class="p">{</span>
    <span class="s2">&quot;type&quot;</span><span class="o">:</span> <span class="s2">&quot;record&quot;</span><span class="p">,</span>
    <span class="s2">&quot;name&quot;</span><span class="o">:</span> <span class="s2">&quot;Person&quot;</span><span class="p">,</span>
    <span class="s2">&quot;fields&quot;</span><span class="o">:</span> <span class="p">[</span>
        <span class="p">{</span><span class="s2">&quot;name&quot;</span><span class="o">:</span> <span class="s2">&quot;userName&quot;</span><span class="p">,</span>        <span class="s2">&quot;type&quot;</span><span class="o">:</span> <span class="s2">&quot;string&quot;</span><span class="p">},</span>
        <span class="p">{</span><span class="s2">&quot;name&quot;</span><span class="o">:</span> <span class="s2">&quot;favouriteNumber&quot;</span><span class="p">,</span> <span class="s2">&quot;type&quot;</span><span class="o">:</span> <span class="p">[</span><span class="s2">&quot;null&quot;</span><span class="p">,</span> <span class="s2">&quot;long&quot;</span><span class="p">]},</span>
        <span class="p">{</span><span class="s2">&quot;name&quot;</span><span class="o">:</span> <span class="s2">&quot;interests&quot;</span><span class="p">,</span>       <span class="s2">&quot;type&quot;</span><span class="o">:</span> <span class="p">{</span><span class="s2">&quot;type&quot;</span><span class="o">:</span> <span class="s2">&quot;array&quot;</span><span class="p">,</span> <span class="s2">&quot;items&quot;</span><span class="o">:</span> <span class="s2">&quot;string&quot;</span><span class="p">}}</span>
    <span class="p">]</span>
<span class="p">}</span></code></pre></div>

<p>…or in an IDL:</p>

<pre><code>record Person {
    string               userName;
    union { null, long } favouriteNumber;
    array&lt;string&gt;        interests;
}
</code></pre>

<p>Notice that there are no tag numbers in the schema! So how does it work?</p>

<p>Here is the same example data <a href="http://avro.apache.org/docs/current/spec.html">encoded</a> in just 32
bytes:</p>

<p><a href="/2012/12/avro.png"><img src="/2012/12/avro_small.png" width="550" height="259" /></a></p>

<p>Strings are just a length prefix followed by UTF-8 bytes, but there’s nothing in the bytestream that
tells you that it is a string. It could just as well be a variable-length integer, or something else
entirely. The only way you can parse this binary data is by reading it alongside the schema, and the
schema tells you what type to expect next. You need to have the <strong>exact same version</strong> of the schema
as the writer of the data used. If you have the wrong schema, the parser will not be able to make
head or tail of the binary data.</p>

<p>So how does Avro support schema evolution? Well, although you need to know the exact schema with
which the data was written (the writer’s schema), that doesn’t have to be the same as the schema the
consumer is expecting (the reader’s schema). You can actually give <em>two different</em> schemas to the
Avro parser, and it uses
<a href="http://avro.apache.org/docs/1.7.2/api/java/org/apache/avro/io/parsing/doc-files/parsing.html">resolution rules</a>
to translate data from the writer schema into the reader schema.</p>

<p>This has some interesting consequences for schema evolution:</p>

<ul>
  <li>The Avro encoding doesn’t have an indicator to say which field is next; it just encodes one field
after another, in the order they appear in the schema. Since there is no way for the parser to
know that a field has been skipped, there is no such thing as an optional field in Avro. Instead,
if you want to be able to leave out a value, you can use a union type, like <code>union { null, long }</code>
above. This is encoded as a byte to tell the parser which of the possible union types to use,
followed by the value itself. By making a union with the <code>null</code> type (which is simply encoded as
zero bytes) you can make a field optional.</li>
  <li>Union types are powerful, but you must take care when changing them. If you want to add a type to
a union, you first need to update all readers with the new schema, so that they know what to
expect. Only once all readers are updated, the writers
may start putting this new type in the records they generate.</li>
  <li>You can reorder fields in a record however you like. Although the fields are encoded in the order
they are declared, the parser matches fields in the reader and writer schema by name, which is why
no tag numbers are needed in Avro.</li>
  <li>Because fields are matched by name, changing the name of a field is tricky. You need to first
update all <em>readers</em> of the data to use the new field name, while keeping the old name as an alias
(since the name matching uses aliases from the reader’s schema). Then you can update the writer’s
schema to use the new field name.</li>
  <li>You can add a field to a record, provided that you also give it a default value (e.g. <code>null</code> if
the field’s type is a union with <code>null</code>). The default is necessary so that when a reader using the
new schema parses a record written with the old schema (and hence lacking the field), it can fill
in the default instead.</li>
  <li>Conversely, you can remove a field from a record, provided that it previously had a default value.
(This is a good reason to give all your fields default values if possible.) This is so that when a
reader using the <em>old</em> schema parses a record written with the <em>new</em> schema, it can fall back to
the default.</li>
</ul>

<p>This leaves us with the problem of knowing the exact schema with which a given record was written.
The best solution depends on the context in which your data is being used:</p>

<ul>
  <li>In Hadoop you typically have large files containing millions of records, all encoded with the same
schema. <a href="http://avro.apache.org/docs/1.7.2/spec.html#Object+Container+Files">Object container files</a>
handle this case: they just include the schema once at the beginning of the file, and the rest of
the file can be decoded with that schema.</li>
  <li>In an RPC context, it’s probably too much overhead to send the schema with every request and
response. But if your RPC framework uses long-lived connections, it can negotiate the schema
once at the start of the connection, and amortize that overhead over many requests.</li>
  <li>If you’re storing records in a database one-by-one, you may end up with different schema versions
written at different times, and so you have to annotate each record with its schema version. If
storing the schema itself is too much overhead, you can use a
<a href="http://avro.apache.org/docs/1.7.2/spec.html#Schema+Fingerprints">hash</a> of the schema, or a
sequential schema version number. You then need a
<a href="https://issues.apache.org/jira/browse/AVRO-1124">schema registry</a> where you can look up the exact
schema definition for a given version number.</li>
</ul>

<p>One way of looking at it: in Protocol Buffers, every field in a record is tagged, whereas in Avro,
the entire record, file or network connection is tagged with a schema version.</p>

<p>At first glance it may seem that Avro’s approach suffers from greater complexity, because you need
to go to the additional effort of distributing schemas. However, I am beginning to think that Avro’s
approach also has some distinct advantages:</p>

<ul>
  <li>Object container files are wonderfully self-describing: the writer schema embedded in the file
contains all the field names and types, and even documentation strings (if the author of the
schema bothered to write some). This means you can load these files directly into interactive
tools like <a href="http://pig.apache.org/">Pig</a>, and it Just Works™ without any configuration.</li>
  <li>As Avro schemas are JSON, you can add your own metadata to them, e.g. describing application-level
semantics for a field. And as you distribute schemas, that metadata automatically gets distributed
too.</li>
  <li>A schema registry is probably a good thing in any case, serving as
<a href="https://github.com/ept/avrodoc">documentation</a> and helping you to find and reuse data. And
because you simply can’t parse Avro data without the schema, the schema registry is guaranteed to
be up-to-date. Of course you can set up a protobuf schema registry too, but since it’s not
<em>required</em> for operation, it’ll end up being on a best-effort basis.</li>
</ul>

<h2 id="thrift">Thrift</h2>

<p>Thrift is a much bigger project than Avro or Protocol Buffers, as it’s not just a data
serialization library, but also an entire RPC framework. It also has a somewhat different culture:
whereas Avro and Protobuf standardize a single binary encoding, Thrift
<a href="http://mail-archives.apache.org/mod_mbox/hadoop-general/200904.mbox/%3CC5FEF47F.90BAC%25cwalter%40microsoft.com%3E">embraces</a>
a whole variety of different serialization formats (which it calls “protocols”).</p>

<p>Indeed, Thrift has
<a href="https://builds.apache.org//job/Thrift/javadoc/org/apache/thrift/protocol/TJSONProtocol.html">two</a>
<a href="https://builds.apache.org//job/Thrift/javadoc/org/apache/thrift/protocol/TSimpleJSONProtocol.html">different</a>
JSON encodings, and no fewer than three different binary encodings. (However, one of the binary
encodings, DenseProtocol, is
<a href="http://wiki.apache.org/thrift/LibraryFeatures">only supported in the C++ implementation</a>; since
we’re interested in cross-language serialization, I will focus on the other two.)</p>

<p>All the encodings share the same schema definition, in Thrift IDL:</p>

<pre><code>struct Person {
  1: string       userName,
  2: optional i64 favouriteNumber,
  3: list&lt;string&gt; interests
}
</code></pre>

<p>The BinaryProtocol encoding is very straightforward, but also fairly wasteful (it takes 59 bytes to
encode our example record):</p>

<p><a href="/2012/12/binaryprotocol.png"><img src="/2012/12/binaryprotocol_small.png" width="550" height="269" /></a></p>

<p>The CompactProtocol encoding is semantically equivalent, but uses variable-length integers and bit
packing to reduce the size to 34 bytes:</p>

<p><a href="/2012/12/compactprotocol.png"><img src="/2012/12/compactprotocol_small.png" width="550" height="276" /></a></p>

<p>As you can see, Thrift’s approach to schema evolution is the same as Protobuf’s: each field is
manually assigned a tag in the IDL, and the tags and field types are stored in the binary encoding,
which enables the parser to skip unknown fields. Thrift defines an explicit list type rather than
Protobuf’s repeated field approach, but otherwise the two are very similar.</p>

<p>In terms of philosophy, the libraries are very different though. Thrift favours the “one-stop shop”
style that gives you an entire integrated RPC framework and many choices (with
<a href="http://wiki.apache.org/thrift/LibraryFeatures">varying cross-language support</a>), whereas Protocol
Buffers and Avro appear to follow much more of a
<a href="http://www.faqs.org/docs/artu/ch01s06.html">“do one thing and do it well”</a> style.</p>

<p><em>This post has been translated into <a href="http://www.sjava.net/319">Korean</a> by Justin Song.</em></p>

                ]]></content:encoded>
            </item>
        
            <item>
                <title>The complexity of user experience</title>
                <link>http://martin.kleppmann.com/2012/10/08/complexity-of-user-experience.html</link>
                <comments>http://martin.kleppmann.com/2012/10/08/complexity-of-user-experience.html#disqus_thread</comments>
                <pubDate>Mon, 08 Oct 2012 00:00:00 +0000</pubDate>
                <dc:creator>Martin Kleppmann</dc:creator>
                
                    <guid isPermaLink="true">http://martin.kleppmann.com/2012/10/08/complexity-of-user-experience.html</guid>
                
                <description><![CDATA[ The problem of overly complex software is nothing new; it is almost as old as software itself. Over and over again, software systems become so complex that they become very difficult to maintain and very time-consuming and expensive to modify. Most developers hate working on such systems, yet nevertheless we... ]]></description>
                <content:encoded><![CDATA[
                    <p>The problem of overly complex software is nothing new; it is almost as old as software itself. Over
and over again, software systems become so complex that they become very difficult to maintain and
very time-consuming and expensive to modify. Most developers hate working on such systems, yet
nevertheless we keep creating new, overly complex systems all the time.</p>

<p>Much has been written about this, including classic papers by Fred Brooks
(<a href="http://people.eecs.ku.edu/~saiedian/Teaching/Sp08/816/Papers/Background-Papers/no-silver-bullet.pdf">No Silver Bullet</a>),
and Ben Moseley and Peter Marks (<a href="http://shaffner.us/cs/papers/tarpit.pdf">Out of the Tar Pit</a>).
They are much more worth reading than this post, and it is presumptuous of me to think I could add
anything significant to this debate. But I will try nevertheless.</p>

<p>Pretty much everyone agrees that if you have a choice between a simpler software design and a more
complex design, all else being equal, that simpler is better. It is also widely thought to be
worthwhile to deliberately invest in simplicity — for example, to spend effort refactoring
existing code into a cleaner design — because the one-off cost of refactoring today is easily
offset by the benefits of easier maintenance tomorrow. Also, much thought by many smart people has
gone into finding ways of breaking down complex systems into manageable parts with manageable
dependencies. I don’t wish to dispute any of that.</p>

<p>But there is a subtlety that I have been missing in discussions about software complexity, that I
feel somewhat ambivalent about, and that I think is worth discussing. It concerns the points where
external humans (people outside of the team maintaining the system) touch the system — as
developers using an API exposed by the system, or as end users interacting with a user interface. I
will concentrate mostly on user interfaces, but much of this discussion applies to APIs too.</p>

<h2 id="examples">Examples</h2>

<p>Let me first give a few examples, and then try to extract a pattern from them. They are examples of
situations where, if you want, you can go to substantial engineering effort in order to make a user
interface a little bit nicer. (Each example based on a true story!)</p>

<ul>
  <li>You have an e-commerce site, and need to send out order confirmation emails that explain next
steps to the customer. Those next steps differ depending on availability, the tax status of the
product, the location of the customer, the type of account they have, and a myriad other
parameters. You want the emails to only include the information that is applicable to this
particular customer’s situation, and not burden them with edge cases that don’t apply to them. You
also want the emails to read as coherent prose, not as a bunch of fragmented bullet points
generated by <code>if</code> statements based on the order parameters. So you go and build a natural language
grammar model for constructing emails based on sentence snippets (providing pluralisation,
agreement, declension in languages that have it, etc), in such a way that for any one out of 100
million possible parameter combinations, the resulting email is grammatically correct and easy to
understand.</li>
  <li>You have a multi-step user flow that is used in various different contexts, but ultimatively
achieves the same thing in each context. (For example, <a href="http://rapportive.com/">Rapportive</a> has
several OAuth flows for connecting your account with various social networks, and there are
several different buttons in different places that all lead into the same user flow.) The simple
solution is to make the flow generic, and not care how the user got there. But if you want to make
the user feel good, you need to imagine what state their mind was in when they entered the flow,
and customise the images, text and structure of the flow in order to match their goal. This means
you have to keep track of where the user came from, what they were trying to do, and thread that
context through every step of the flow. This is not fundamentally hard, but it is fiddly,
time-consuming and error-prone.</li>
  <li>You have an application that requires some arcane configuration. You could take the stance that
you will give the user a help page and they will have to figure it out from there. Or you could
write a sophisticated auto-configuration tool that inspects the user’s environment, analyses
thousands of possible software combinations and configurations (and updates this database as new
versions of other products in the environment are released), and automatically chooses the correct
settings — hopefully without having to ask the user for help. With auto-configuration, the users
never even know that they were spared a confusing configuration dialog. But somehow, word gets
around that the product “just works”.</li>
</ul>

<h2 id="whats-a-user-requirement">What’s a user requirement?</h2>

<p>We said above that simplicity is good. However, taking simplicity to an exaggerated extreme, you end
up with software that does nothing. This implies that there are aspects of software complexity that
are <strong>essential</strong> to the user’s problem that is being solved. (Note that I don’t mean complexity of
the user interface, but complexity of the actual code that implements the solution to the user’s
problem.)</p>

<p>Unfortunately, there is a lot of additional complexity introduced by stuff that is not directly
visible or useful to users: stuff that is only required to “grease the wheels”, for example to make
legacy components work or to improve performance. Moseley and Marks call this latter type
<strong>accidental</strong> complexity, and argue that it should be removed or abstracted away as much as
possible. (Other authors define essential and accidental complexity slightly differently, but the
exact definition is not important for the purpose of this post.)</p>

<p>This suggests that it is important to understand what <strong>user problem</strong> is being solved, and that’s
where things start getting tricky. When you say that something is essential because it fulfils a
<strong>user requirement</strong> (as opposed to an implementation constraint or a performance optimisation),
that presupposes a very utilitarian view of software. It assumes that the user is trying to get a
job done, and that they are a rational actor. But what if, say, you are taking an emotional approach
and optimising for <strong>user delight</strong>?</p>

<p>What if the user didn’t know they had a problem, but you solve it anyway? If you introduce
complexity in the system for the sake of making things a little nicer for the user (but without
providing new core functionality), is that complexity really essential? What if you add a little
detail that is surprising but delightful?</p>

<p>You can try to reduce an emotional decision down to a rational one — for example, you can say that
when a user plays a game, it is solving the user’s problem of boredom by providing distraction. Thus
any feature which substantially contributes towards alleviating boredom may be considered essential.
Such reductionism can sometimes provide useful angles of insight, but I think a lot would be lost by
ignoring the emotional angle.</p>

<p>You can state categorically that “great user experience is an essential feature”. But what does that
mean? By itself, that statement is so general that could be used to argue for anything or nothing.
User experience is subjective. What’s preferable for one user may be an annoyance for another user,
even if both users are in the application’s target segment. Sometimes it just comes down to taste or
fashion. User experience tends to have an emotional angle that makes it hard to fit into a rational
reasoning framework.</p>

<p>What I am trying to get at: there are things in software that introduce a lot of complexity (and
that we should consequently be wary of), and that can’t be directly mapped to a bullet point on a
list of user requirements, but that are nevertheless important and valuable. These things do not
necessarily provide important functionality, but they contribute to how the user <strong>feels</strong> about the
application. Their effect may be invisible or subconscious, but that doesn’t make them any less
essential.</p>

<h2 id="data-driven-vs-emotional-design">Data-driven vs. emotional design</h2>

<p>Returning to the examples above: as an application developer, you can choose whether to take on
substantial additional complexity in the software in order to simplify or improve the experience for
the user. The increased software complexity actually <strong>reduces</strong> the complexity from the user’s
point of view. These examples also illustrate how user experience concerns are not just a matter of
graphic design, but can also have a big impact on how things are engineered.</p>

<p>The features described above arguably do not contribute to the utility of the software — in the
e-commerce example, orders will be fulfilled whether or not the confirmation emails are grammatical.
In that sense, the complexity is unnecessary. But I would argue that these kind of user experience
improvements are just as important as the utility of the product, because they determine how users
<strong>feel</strong> about it. And how they feel ultimately determines whether they come back, and thus the
success or failure of the product.</p>

<p>One could even argue that the utility of a product is a subset of its user experience: if the
software doesn’t do the job that it’s supposed to, then that’s one way of creating a pretty bad
experience; however, there are also many other ways of creating a bad experience, while remaining
fully functional from a utilitarian point of view.</p>

<p>The emotional side of user experience can be a difficult thing for organisations to grapple with,
because it doesn’t easily map to metrics. You can measure things like how long a user stayed on your
site, how many things they clicked on, conversion rates, funnels, repeat purchase rates, lifetime
values… but those numbers tell you very little about how happy you made a user. So you can take a
“data-driven” approach to design decisions and say that a feature is worthwhile if and only if it
makes the metrics go up — but I fear that an important side of the story is missed if you go
solely by the numbers.</p>

<h2 id="questions">Questions</h2>

<p>This is as far as my thinking has got: believing that a great user experience is essential for many
products; and recognising that building a great UX is hard, can require substantial additional
complexity in engineering, and can be hard to justify in terms of logical arguments and metrics.
Which leaves me with some unanswered questions:</p>

<ul>
  <li>Every budget is finite, so you have to prioritise things, and not everything will get done. When
you consider building something that improves user experience without strictly adding utility, it
has to be traded off against features that do add utility (is it better to shave a day off the
delivery time than to have a nice confirmation email?), and the cost of the increased complexity
(will that clever email generator be a nightmare to localise when we translate the site into other
languages?). How do you decide about that kind of trade-offs?</li>
  <li>User experience choices are often emotional and
<a href="http://martin.kleppmann.com/2010/10/30/intuition-has-no-transfer-encoding.html">intuitive</a>
(no number of focus groups and usability tests can replace good taste). That doesn’t make them any
more or less important than rational arguments, but combining emotional and rational arguments can
be tricky. Emotionally-driven people tend to let emotional choices overrule rational arguments,
and rationally-driven people vice versa. How do you find the healthy middle ground?</li>
  <li>If you’re aiming for a minimum viable product in order to test out a market (as opposed to
improving a mature product), does that change how you prioritise core utility relative to “icing
on the cake”?</li>
</ul>

<p>I suspect that the answers to the questions above are <em>“it depends”</em>. More precisely, <em>“how one
thing is valued relative to another is an aspect of your particular organisation’s culture, and
there’s no one right answer”</em>. That would imply that each of us should think about it; you should
have your own personal answers for how you decide these things in your own projects, and be able to
articulate them. But it’s difficult — I don’t think hard-and-fast rules have a chance of working
here.</p>

<p>I’d love to hear your thoughts in the comments below. If you liked this post, you can
<a href="http://eepurl.com/csJmf">subscribe to email notifications</a> when I write something new :)</p>

                ]]></content:encoded>
            </item>
        
    </channel>
</rss>
