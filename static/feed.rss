<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0"
    xmlns:content="http://purl.org/rss/1.0/modules/content/"
    xmlns:dc="http://purl.org/dc/elements/1.1/"
    xmlns:atom="http://www.w3.org/2005/Atom"
    xmlns:sy="http://purl.org/rss/1.0/modules/syndication/">

    <channel>
        <title>Martin Kleppmann's blog</title>
        <atom:link href="http://martin.kleppmann.com/feed.rss" rel="self" type="application/rss+xml" />
        <link>http://martin.kleppmann.com/</link>
        <description></description>
        <lastBuildDate>Mon, 08 Feb 2016 10:29:11 GMT</lastBuildDate>
        <language>en</language>
        <sy:updatePeriod>hourly</sy:updatePeriod>
        <sy:updateFrequency>1</sy:updateFrequency>

        
        
            <item>
                <title>How to do distributed locking</title>
                <link>http://martin.kleppmann.com/2016/02/08/how-to-do-distributed-locking.html</link>
                <comments>http://martin.kleppmann.com/2016/02/08/how-to-do-distributed-locking.html#disqus_thread</comments>
                <pubDate>Mon, 08 Feb 2016 00:00:00 +0000</pubDate>
                <dc:creator>Martin Kleppmann</dc:creator>
                
                    <guid isPermaLink="true">http://martin.kleppmann.com/2016/02/08/how-to-do-distributed-locking.html</guid>
                
                <description><![CDATA[ As part of the research for my book, I came across an algorithm called Redlock on the Redis website. The algorithm claims to implement fault-tolerant distributed locks (or rather, leases [1]) on top of Redis, and the page asks for feedback from people who are into distributed systems. The algorithm instinctively... ]]></description>
                <content:encoded><![CDATA[
                    <p>As part of the research for <a href="http://dataintensive.net/">my book</a>, I came across an algorithm called <a href="http://redis.io/topics/distlock">Redlock</a> on the
<a href="http://redis.io/">Redis</a> website. The algorithm claims to implement fault-tolerant distributed locks (or rather,
<a href="http://web.stanford.edu/class/cs240/readings/89-leases.pdf" title="Cary G Gray and David R Cheriton. Leases: An Efficient Fault-Tolerant Mechanism for Distributed File Cache Consistency. SOSP 1989">leases</a> [1]) on top of Redis, and the page asks for feedback from people who are into
distributed systems. The algorithm instinctively set off some alarm bells in the back of my mind, so
I spent a bit of time thinking about it and writing up these notes.</p>

<p>Since there are already <a href="http://redis.io/topics/distlock">over 10 independent implementations of Redlock</a> and we don’t know
who is already relying on this algorithm, I thought it would be worth sharing my notes publicly.
I won’t go into other aspects of Redis, some of which have already been critiqued
<a href="https://aphyr.com/tags/Redis">elsewhere</a>.</p>

<p>Before I go into the details of Redlock, let me say that I quite like Redis, and I have successfully
used it in production in the past. I think it’s a good fit in situations where you want to share
some transient, approximate, fast-changing data between servers, and where it’s not a big deal if
you occasionally lose that data for whatever reason. For example, a good use case is maintaining
request counters per IP address (for rate limiting purposes) and sets of distinct IP addresses per
user ID (for abuse detection).</p>

<p>However, Redis has been gradually making inroads into areas of data management where there are
stronger consistency and durability expectations – which worries me, because this is not what Redis
is designed for. Arguably, distributed locking is one of those areas. Let’s examine it in some more
detail.</p>

<h2 id="what-are-you-using-that-lock-for">What are you using that lock for?</h2>

<p>The purpose of a lock is to ensure that among several nodes that might try to do the same piece of
work, only one actually does it (at least only one at a time). That work might be to write some data
to a shared storage system, to perform some computation, to call some external API, or suchlike. At
a high level, there are two reasons why you might want a lock in a distributed application:
<a href="http://research.google.com/archive/chubby.html" title="Mike Burrows. The Chubby lock service for loosely-coupled distributed systems. OSDI 2006">for efficiency or for correctness</a> [2]. To distinguish these cases, you can ask what
would happen if the lock failed:</p>

<ul>
  <li><strong>Efficiency:</strong> Taking a lock saves you from unnecessarily doing the same work twice (e.g. some
expensive computation). If the lock fails and two nodes end up doing the same piece of work, the
result is a minor increase in cost (you end up paying 5 cents more to AWS than you otherwise would
have) or a minor inconvenience (e.g. a user ends up getting the same email notification twice).</li>
  <li><strong>Correctness:</strong> Taking a lock prevents concurrent processes from stepping on each others’ toes
and messing up the state of your system. If the lock fails and two nodes concurrently work on the
same piece of data, the result is a corrupted file, data loss, permanent inconsistency, the wrong
dose of a drug administered to a patient, or some other serious problem.</li>
</ul>

<p>Both are valid cases for wanting a lock, but you need to be very clear about which one of the two
you are dealing with.</p>

<p>I will argue that if you are using locks merely for efficiency purposes, it is unnecessary to incur
the cost and complexity of Redlock, running 5 Redis servers and checking for a majority to acquire
your lock. You are better off just using a single Redis instance, perhaps with asynchronous
replication to a secondary instance in case the primary crashes. </p>

<p>If you use a single Redis instance, of course you will drop some locks if the power suddenly goes
out on your Redis node, or something else goes wrong. But if you’re only using the locks as an
efficiency optimization, and the crashes don’t happen too often, that’s no big deal. This “no big
deal” scenario is where Redis shines. At least if you’re relying on a single Redis instance, it is
clear to everyone who looks at the system that the locks are approximate, and only to be used for
non-critical purposes.</p>

<p>On the other hand, the Redlock algorithm, with its 5 replicas and majority voting, looks at first
glance as though it is suitable for situations in which your locking is important for <em>correctness</em>.
I will argue in the following sections that it is <em>not</em> suitable for that purpose. For the rest of
this article we will assume that your locks are important for correctness, and that it is a serious
bug if two different nodes concurrently believe that they are holding the same lock.</p>

<h2 id="protecting-a-resource-with-a-lock">Protecting a resource with a lock</h2>

<p>Let’s leave the particulars of Redlock aside for a moment, and discuss how a distributed lock is
used in general (independent of the particular locking algorithm used). It’s important to remember
that a lock in a distributed system is not like a mutex in a multi-threaded application. It’s a more
complicated beast, due to the problem that different nodes and the network can all fail
independently in various ways.</p>

<p>For example, say you have an application in which a client needs to update a file in shared storage
(e.g. HDFS or S3). A client first acquires the lock, then reads the file, makes some changes, writes
the modified file back, and finally releases the lock. The lock prevents two clients from performing
this read-modify-write cycle concurrently, which would result in lost updates. The code might look
something like this:</p>

<div class="highlight"><pre><code class="language-js" data-lang="js"><span class="c1">// THIS CODE IS BROKEN</span>
<span class="kd">function</span> <span class="nx">writeData</span><span class="p">(</span><span class="nx">filename</span><span class="p">,</span> <span class="nx">data</span><span class="p">)</span> <span class="p">{</span>
    <span class="kd">var</span> <span class="nx">lock</span> <span class="o">=</span> <span class="nx">lockService</span><span class="p">.</span><span class="nx">acquireLock</span><span class="p">(</span><span class="nx">filename</span><span class="p">);</span>
    <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="nx">lock</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">throw</span> <span class="s1">&#39;Failed to acquire lock&#39;</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="k">try</span> <span class="p">{</span>
        <span class="kd">var</span> <span class="nx">file</span> <span class="o">=</span> <span class="nx">storage</span><span class="p">.</span><span class="nx">readFile</span><span class="p">(</span><span class="nx">filename</span><span class="p">);</span>
        <span class="kd">var</span> <span class="nx">updated</span> <span class="o">=</span> <span class="nx">updateContents</span><span class="p">(</span><span class="nx">file</span><span class="p">,</span> <span class="nx">data</span><span class="p">);</span>
        <span class="nx">storage</span><span class="p">.</span><span class="nx">writeFile</span><span class="p">(</span><span class="nx">filename</span><span class="p">,</span> <span class="nx">updated</span><span class="p">);</span>
    <span class="p">}</span> <span class="k">finally</span> <span class="p">{</span>
        <span class="nx">lock</span><span class="p">.</span><span class="nx">release</span><span class="p">();</span>
    <span class="p">}</span>
<span class="p">}</span></code></pre></div>

<p>Unfortunately, even if you have a perfect lock service, the code above is broken. The following
diagram shows how you can end up with corrupted data:</p>

<p><img src="/2016/02/unsafe-lock.png" width="550" height="200" alt="Unsafe access to a resource protected by a distributed lock" /></p>

<p>In this example, the client that acquired the lock is paused for an extended period of time while
holding the lock – for example because the garbage collector (GC) kicked in. The lock has a timeout
(i.e. it is a lease), which is always a good idea (otherwise a crashed client could end up holding
a lock forever and never releasing it). However, if the GC pause lasts longer than the lease expiry
period, and the client doesn’t realise that it has expired, it may go ahead and make some unsafe
change.</p>

<p>This bug is not theoretical: HBase used to <a href="http://www.slideshare.net/enissoz/hbase-and-hdfs-understanding-filesystem-usage" title="Enis Söztutar. HBase and HDFS: Understanding filesystem usage in HBase. HBaseCon 2013">have this problem</a> [3,4]. Normally,
GC pauses are quite short, but “stop-the-world” GC pauses have sometimes been known to last for
<a href="http://blog.cloudera.com/blog/2011/02/avoiding-full-gcs-in-hbase-with-memstore-local-allocation-buffers-part-1/" title="Todd Lipcon. Avoiding Full GCs in Apache HBase with MemStore-Local Allocation Buffers: Part 1. 2011">several minutes</a> [5] – certainly long enough for a lease to expire. Even so-called
“concurrent” garbage collectors like the HotSpot JVM’s CMS cannot fully run in parallel with the
application code – even they <a href="http://mechanical-sympathy.blogspot.co.uk/2013/07/java-garbage-collection-distilled.html" title="Martin Thompson. Java Garbage Collection Distilled. 2013">need to stop the world</a> from time to time [6].</p>

<p>You cannot fix this problem by inserting a check on the lock expiry just before writing back to
storage. Remember that GC can pause a running thread at <em>any point</em>, including the point that is
maximally inconvenient for you (between the last check and the write operation).</p>

<p>And if you’re feeling smug because your programming language runtime doesn’t have long GC pauses,
there are many other reasons why your process might get paused. Maybe your process tried to read an
address that is not yet loaded into memory, so it gets a page fault and is paused until the page is
loaded from disk. Maybe your disk is actually EBS, and so reading a variable unwittingly turned into
a synchronous network request over Amazon’s congested network. Maybe there are many other processes
contending for CPU, and you hit a <a href="https://twitter.com/aphyr/status/682077908953792512">black node in your scheduler tree</a>. Maybe someone
accidentally sent SIGSTOP to the process. Whatever. Your processes will get paused.</p>

<p>If you still don’t believe me about process pauses, then consider instead that the file-writing
request may get delayed in the network before reaching the storage service. Packet networks such as
Ethernet and IP may delay packets <em>arbitrarily</em>, and <a href="https://queue.acm.org/detail.cfm?id=2655736" title="P Bailis and K Kingsbury. The Network is Reliable. ACM Queue 12(7), 2014.">they do</a> [7]: in a famous
<a href="https://github.com/blog/1364-downtime-last-saturday" title="Mark Imbriaco. Downtime last Saturday. 2012">incident at GitHub</a>, packets were delayed in the network for approximately 90
seconds [8]. This means that an application process may send a write request, and it may reach
the storage server a minute later when the lease has already expired.</p>

<p>Even in well-managed networks, this kind of thing can happen. You simply cannot make any assumptions
about timing, which is why the code above is fundamentally unsafe, no matter what lock service you
use.</p>

<h2 id="making-the-lock-safe-with-fencing">Making the lock safe with fencing</h2>

<p>The fix for this problem is actually pretty simple: you need to include a <em>fencing token</em> with every
write request to the storage service. In this context, a fencing token is simply a number that
increases (e.g. incremented by the lock service) every time a client acquires the lock. This is
illustrated in the following diagram:</p>

<p><img src="/2016/02/fencing-tokens.png" width="550" height="200" alt="Using fencing tokens to make resource access safe" /></p>

<p>Client 1 acquires the lease and gets a token of 33, but then it goes into a long pause and the lease
expires. Client 2 acquires the lease, gets a token of 34 (the number always increases), and then
sends its write to the storage service, including the token of 34. Later, client 1 comes back to
life and sends its write to the storage service, including its token value 33. However, the storage
server rembers that it has already processed a write with a higher token number (34), and so it
rejects the request with token 33.</p>

<p>Note this requires the storage server to take an active role in checking tokens, and rejecting any
writes on which the token has gone backwards. But this is not particularly hard, once you know the
trick. And provided that the lock service generates strictly monotonically increasing tokens, this
makes the lock safe. For example, if you are using ZooKeeper as lock service, you can use the <code>zxid</code>
or the znode version number as fencing token, and you’re in good shape [3].</p>

<p>However, this leads us to the first big problem with Redlock: <em>it does not have any facility for
generating fencing tokens</em>. The algorithm does not produce any number that is guaranteed to increase
every time a client acquires a lock. This means that even if the algorithm were otherwise perfect,
it would not be safe to use, because you cannot prevent the race condition between clients in the
case where one client is paused or its packets are delayed.</p>

<p>And it’s not obvious to me how one would change the Redlock algorithm to start generating fencing
tokens. The unique random value it uses does not provide the required monotonicity. Simply keeping
a counter on one Redis node would not be sufficient, because that node may fail. Keeping counters on
several nodes would mean they would go out of sync. It’s likely that you would need a consensus
algorithm just to generate the fencing tokens. (If only <a href="https://twitter.com/lindsey/status/575006945213485056">incrementing a counter</a> was
simple.)</p>

<h2 id="using-time-to-solve-consensus">Using time to solve consensus</h2>

<p>The fact that Redlock fails to generate fencing tokens should already be sufficient reason not to
use it in situations where correctness depends on the lock. But there are some further problems that
are worth discussing.</p>

<p>In the academic literature, the most practical system model for this kind of algorithm is the
<a href="http://courses.csail.mit.edu/6.852/08/papers/CT96-JACM.pdf" title="TD Chandra and S Toueg. Unreliable Failure Detectors for Reliable Distributed Systems. JACM 43(2):225–267, 1996">asynchronous model with unreliable failure detectors</a> [9]. In plain English,
this means that the algorithms make no assumptions about timing: processes may pause for arbitrary
lengths of time, packets may be arbitrarily delayed in the network, and clocks may be arbitrarily
wrong – and the algorithm is nevertheless expected to do the right thing. Given what we discussed
above, these are very reasonable assumptions.</p>

<p>The only purpose for which algorithms may use clocks is to generate timeouts, to avoid waiting
forever if a node is down. But timeouts do not have to be accurate: just because a request times
out, that doesn’t mean that the other node is definitely down – it could just as well be that there
is a large delay in the network, or that your local clock is wrong. When used as a failure detector,
timeouts are just a guess that something is wrong. (If they could, distributed algorithms would do
without clocks entirely, but then <a href="http://www.cs.princeton.edu/courses/archive/fall07/cos518/papers/flp.pdf" title="MJ Fischer, N Lynch, and MS Paterson. Impossibility of Distributed Consensus with One Faulty Process. JACM 32(2):374–382, 1985">consensus becomes impossible</a> [10]. Acquiring a lock is
like a compare-and-set operation, which <a href="https://cs.brown.edu/~mph/Herlihy91/p124-herlihy.pdf" title="Maurice Herlihy. Wait-Free Synchronization. TOPLAS 13(1):124–149, 1991">requires consensus</a> [11].)</p>

<p>Note that Redis <a href="https://github.com/antirez/redis/blob/edd4d555df57dc84265fdfb4ef59a4678832f6da/src/server.c#L390-L404">uses <code>gettimeofday</code></a>, not a <a href="http://linux.die.net/man/2/clock_gettime">monotonic clock</a>, to
determine the <a href="https://github.com/antirez/redis/blob/f0b168e8944af41c4161249040f01ece227cfc0c/src/db.c#L933-L959">expiry of keys</a>. The man page for <code>gettimeofday</code> <a href="http://linux.die.net/man/2/gettimeofday">explicitly
says</a> that the time it returns is subject to discontinuous jumps in system time –
that is, it might suddenly jump forwards by a few minutes, or even jump back in time (e.g. if the
clock is <a href="https://www.eecis.udel.edu/~mills/ntp/html/clock.html">stepped by NTP</a> because it differs from a NTP server by too much, or if the
clock is manually adjusted by an administrator). Thus, if the system clock is doing weird things, it
could easily happen that the expiry of a key in Redis is much faster or much slower than expected.</p>

<p>For algorithms in the asynchronous model this is not a big problem: these algorithms generally
ensure that their <em>safety</em> properties always hold, <a href="http://www.net.t-labs.tu-berlin.de/~petr/ADC-07/papers/DLS88.pdf" title="C Dwork, N Lynch, and L Stockmeyer. Consensus in the Presence of Partial Synchrony. JACM 35(2):288–323, 1988">without making any timing
assumptions</a> [12]. Only <em>liveness</em> properties depend on timeouts or some other failure
detector. In plain English, this means that even if the timings in the system are all over the place
(processes pausing, networks delaying, clocks jumping forwards and backwards), the performance of an
algorithm might go to hell, but the algorithm will never make an incorrect decision.</p>

<p>However, Redlock is not like this. Its safety depends on a lot of timing assumptions: it assumes
that all Redis nodes hold keys for approximately the right length of time before expiring; that that
the network delay is small compared to the expiry duration; and that process pauses are much shorter
than the expiry duration.</p>

<h2 id="breaking-redlock-with-bad-timings">Breaking Redlock with bad timings</h2>

<p>Let’s look at some examples to demonstrate Redlock’s reliance on timing assumptions. Say the system
has five Redis nodes (A, B, C, D and E), and two clients (1 and 2). What happens if a clock on one
of the Redis nodes jumps forward?</p>

<ol>
  <li>Client 1 acquires lock on nodes A, B, C. Due to a network issue, D and E cannot be reached.</li>
  <li>The clock on node C jumps forward, causing the lock to expire.</li>
  <li>Client 2 acquires lock on nodes C, D, E. Due to a network issue, A and B cannot be reached.</li>
  <li>Clients 1 and 2 now both believe they hold the lock.</li>
</ol>

<p>A similar issue could happen if C crashes before persisting the lock to disk, and immediately
restarts. For this reason, the Redlock documentation <a href="http://redis.io/topics/distlock#performance-crash-recovery-and-fsync">recommends delaying restarts</a> of
crashed nodes for at least the time-to-live of the longest-lived lock. But this restart delay again
relies on a reasonably accurate measurement of time, and would fail if the clock jumps.</p>

<p>Okay, so maybe you think that a clock jump is unrealistic, because you’re very confident in having
correctly configured NTP to only ever slew the clock. In that case, let’s look at an example of how
a process pause may cause the algorithm to fail:</p>

<ol>
  <li>Client 1 requests lock on nodes A, B, C, D, E.</li>
  <li>While the responses to client 1 are in flight, client 1 goes into stop-the-world GC.</li>
  <li>Locks expire on all Redis nodes.</li>
  <li>Client 2 acquires lock on nodes A, B, C, D, E.</li>
  <li>Client 1 finishes GC, and receives the responses from Redis nodes indicating that it successfully
acquired the lock (they were held in client 1’s kernel network buffers while the process was
paused).</li>
  <li>Clients 1 and 2 now both believe they hold the lock.</li>
</ol>

<p>Note that even though Redis is written in C, and thus doesn’t have GC, that doesn’t help us here:
any system in which the <em>clients</em> may experience a GC pause has this problem. You can only make this
safe by preventing client 1 from performing any operations under the lock after client 2 has
acquired the lock, for example using the fencing approach above.</p>

<p>A long network delay can produce the same effect as the process pause. It perhaps depends on your
TCP user timeout – if you make the timeout significantly shorter than the Redis TTL, perhaps the
delayed network packets would be ignored, but we’d have to look in detail at the TCP implementation
to be sure. Also, with the timeout we’re back down to accuracy of time measurement again!</p>

<h2 id="the-synchrony-assumptions-of-redlock">The synchrony assumptions of Redlock</h2>

<p>These examples show that Redlock works correctly only if you assume a <em>synchronous</em> system model –
that is, a system with the following properties:</p>

<ul>
  <li>bounded network delay (you can guarantee that packets always arrive within some guaranteed maximum
delay),</li>
  <li>bounded process pauses (in other words, hard real-time constraints, which you typically only
find in car airbag systems and suchlike), and</li>
  <li>bounded clock error (cross your fingers that you don’t get your time from a <a href="http://xenia.media.mit.edu/~nelson/research/ntp-survey99/">bad NTP
server</a>).</li>
</ul>

<p>Note that a synchronous model does not mean exactly synchronised clocks: it means you are assuming
a <a href="http://www.net.t-labs.tu-berlin.de/~petr/ADC-07/papers/DLS88.pdf" title="C Dwork, N Lynch, and L Stockmeyer. Consensus in the Presence of Partial Synchrony. JACM 35(2):288–323, 1988"><em>known, fixed upper bound</em></a> on network delay, pauses and clock drift [12]. Redlock
assumes that delays, pauses and drift are all small relative to the time-to-live of a lock; if the
timing issues become as large as the time-to-live, the algorithm fails.</p>

<p>In a reasonably well-behaved datacenter environment, the timing assumptions will be satisfied <em>most</em>
of the time – this is known as a <a href="http://www.net.t-labs.tu-berlin.de/~petr/ADC-07/papers/DLS88.pdf" title="C Dwork, N Lynch, and L Stockmeyer. Consensus in the Presence of Partial Synchrony. JACM 35(2):288–323, 1988">partially synchronous system</a> [12]. But is that good
enough? As soon as those timing assumptions are broken, Redlock may violate its safety properties,
e.g. granting a lease to one client before another has expired. If you’re depending on your lock for
correctness, “most of the time” is not enough – you need it to <em>always</em> be correct. </p>

<p>There is plenty of evidence that it is not safe to assume a synchronous system model for most
practical system environments [7,8]. Keep reminding yourself of the GitHub incident with the
<a href="https://github.com/blog/1364-downtime-last-saturday" title="Mark Imbriaco. Downtime last Saturday. 2012">90-second packet delay</a>. It is unlikely that Redlock would survive a <a href="https://aphyr.com/tags/jepsen">Jepsen</a> test.</p>

<p>On the other hand, a consensus algorithm designed for a partially synchronous system model (or
asynchronous model with failure detector) actually has a chance of working. Raft, Viewstamped
Replication, Zab and Paxos all fall in this category. Such an algorithm must let go of all timing
assumptions. That’s hard: it’s so tempting to assume networks, processes and clocks are more
reliable than they really are. But in the messy reality of distributed systems, you have to be very
careful with your assumptions.</p>

<h2 id="conclusion">Conclusion</h2>

<p>I think the Redlock algorithm is a poor choice because it is “neither fish nor fowl”: it is
unnecessarily heavyweight and expensive for efficiency-optimization locks, but it is not
sufficiently safe for situations in which correctness depends on the lock.</p>

<p>In particular, the algorithm makes dangerous assumptions about timing and system clocks (essentially
assuming a synchronous system with bounded network delay and bounded execution time for operations),
and it violates safety properties if those assumptions are not met. Moreover, it lacks a facility
for generating fencing tokens (which protect a system against long delays in the network or in
paused processes).</p>

<p>If you need locks only on a best-effort basis (as an efficiency optimization, not for correctness),
I would recommend sticking with the <a href="http://redis.io/commands/set">straightforward single-node locking algorithm</a> for
Redis (conditional set-if-not-exists to obtain a lock, atomic delete-if-value-matches to release
a lock), and documenting very clearly in your code that the locks are only approximate and may
occasionally fail. Don’t bother with setting up a cluster of five Redis nodes.</p>

<p>On the other hand, if you need locks for correctness, please don’t use Redlock. Instead, please use
a proper consensus system such as <a href="https://zookeeper.apache.org/">ZooKeeper</a>, probably via one of the <a href="http://curator.apache.org/curator-recipes/index.html">Curator recipes</a>
that implements a lock. (At the very least, use a <a href="http://www.postgresql.org/">database with reasonable transactional
guarantees</a>.) And please enforce use of fencing tokens on all resource accesses under the
lock.</p>

<p>As I said at the beginning, Redis is an excellent tool if you use it correctly. None of the above
diminishes the usefulness of Redis for its intended purposes. <a href="http://antirez.com/">Salvatore</a> has been very
dedicated to the project for years, and its success is well deserved. But every tool has
limitations, and it is important to know them and to plan accordingly.</p>

<p>If you want to learn more, I explain this topic in greater detail in <a href="http://dataintensive.net/">chapters 8 and 9 of my
book</a>, now available in Early Release from O’Reilly. (The diagrams above are taken from my
book.) For learning how to use ZooKeeper, I recommend <a href="http://shop.oreilly.com/product/0636920028901.do" title="FP Junqueira and B Reed. ZooKeeper: Distributed Process Coordination. O'Reilly, 2013">Junqueira and Reed’s book</a> [3].
For a good introduction to the theory of distributed systems, I recommend <a href="http://www.distributedprogramming.net/" title="C Cachin, R Guerraoui, and L Rodrigues. Introduction to Reliable and Secure Distributed Programming, 2nd ed. Springer, 2011">Cachin, Guerraoui and
Rodrigues’ textbook</a> [13].</p>

<p><em>Thank you to <a href="https://aphyr.com">Kyle Kingsbury</a>, <a href="https://twitter.com/skamille">Camille Fournier</a>, <a href="https://twitter.com/fpjunqueira">Flavio Junqueira</a>, and
<a href="http://antirez.com/">Salvatore Sanfilippo</a> for reviewing a draft of this article. Any errors are mine, of
course. Salvatore may not agree with this analysis, and I respect his viewpoints.</em></p>

<h2 id="references">References</h2>

<p>[1] Cary G Gray and David R Cheriton:
“<a href="http://web.stanford.edu/class/cs240/readings/89-leases.pdf" title="Cary G Gray and David R Cheriton. Leases: An Efficient Fault-Tolerant Mechanism for Distributed File Cache Consistency. SOSP 1989">Leases: An Efficient Fault-Tolerant Mechanism for Distributed File Cache Consistency</a>,”
at <em>12th ACM Symposium on Operating Systems Principles</em> (SOSP), December 1989.
<a href="http://dx.doi.org/10.1145/74850.74870">doi:10.1145/74850.74870</a></p>

<p>[2] Mike Burrows:
“<a href="http://research.google.com/archive/chubby.html" title="Mike Burrows. The Chubby lock service for loosely-coupled distributed systems. OSDI 2006">The Chubby lock service for loosely-coupled distributed systems</a>,”
at <em>7th USENIX Symposium on Operating System Design and Implementation</em> (OSDI), November 2006.</p>

<p>[3] Flavio P Junqueira and Benjamin Reed:
<a href="http://shop.oreilly.com/product/0636920028901.do" title="FP Junqueira and B Reed. ZooKeeper: Distributed Process Coordination. O'Reilly, 2013"><em>ZooKeeper: Distributed Process Coordination</em></a>. O’Reilly Media, November 2013.
ISBN: 978-1-4493-6130-3</p>

<p>[4] Enis Söztutar:
“<a href="http://www.slideshare.net/enissoz/hbase-and-hdfs-understanding-filesystem-usage" title="Enis Söztutar. HBase and HDFS: Understanding filesystem usage in HBase. HBaseCon 2013">HBase and HDFS: Understanding filesystem usage in HBase</a>,” at <em>HBaseCon</em>, June 2013.</p>

<p>[5] Todd Lipcon:
“<a href="http://blog.cloudera.com/blog/2011/02/avoiding-full-gcs-in-hbase-with-memstore-local-allocation-buffers-part-1/" title="Todd Lipcon. Avoiding Full GCs in Apache HBase with MemStore-Local Allocation Buffers: Part 1. 2011">Avoiding Full GCs in Apache HBase with MemStore-Local Allocation Buffers: Part 1</a>,”
blog.cloudera.com, 24 February 2011.</p>

<p>[6] Martin Thompson: “<a href="http://mechanical-sympathy.blogspot.co.uk/2013/07/java-garbage-collection-distilled.html" title="Martin Thompson. Java Garbage Collection Distilled. 2013">Java Garbage Collection Distilled</a>,”
mechanical-sympathy.blogspot.co.uk, 16 July 2013.</p>

<p>[7] Peter Bailis and Kyle Kingsbury: “<a href="https://queue.acm.org/detail.cfm?id=2655736" title="P Bailis and K Kingsbury. The Network is Reliable. ACM Queue 12(7), 2014.">The Network is Reliable</a>,”
<em>ACM Queue</em>, volume 12, number 7, July 2014.
<a href="http://dx.doi.org/10.1145/2639988.2639988">doi:10.1145/2639988.2639988</a></p>

<p>[8] Mark Imbriaco: “<a href="https://github.com/blog/1364-downtime-last-saturday" title="Mark Imbriaco. Downtime last Saturday. 2012">Downtime last Saturday</a>,” github.com, 26 December 2012.</p>

<p>[9] Tushar Deepak Chandra and Sam Toueg:
“<a href="http://courses.csail.mit.edu/6.852/08/papers/CT96-JACM.pdf" title="TD Chandra and S Toueg. Unreliable Failure Detectors for Reliable Distributed Systems. JACM 43(2):225–267, 1996">Unreliable Failure Detectors for Reliable Distributed Systems</a>,”
<em>Journal of the ACM</em>, volume 43, number 2, pages 225–267, March 1996.
<a href="http://dx.doi.org/10.1145/226643.226647">doi:10.1145/226643.226647</a></p>

<p>[10] Michael J Fischer, Nancy Lynch, and Michael S Paterson:
“<a href="http://www.cs.princeton.edu/courses/archive/fall07/cos518/papers/flp.pdf" title="MJ Fischer, N Lynch, and MS Paterson. Impossibility of Distributed Consensus with One Faulty Process. JACM 32(2):374–382, 1985">Impossibility of Distributed Consensus with One Faulty Process</a>,”
<em>Journal of the ACM</em>, volume 32, number 2, pages 374–382, April 1985.
<a href="http://dx.doi.org/10.1145/3149.214121">doi:10.1145/3149.214121</a></p>

<p>[11] Maurice P Herlihy: “<a href="https://cs.brown.edu/~mph/Herlihy91/p124-herlihy.pdf" title="Maurice Herlihy. Wait-Free Synchronization. TOPLAS 13(1):124–149, 1991">Wait-Free Synchronization</a>,”
<em>ACM Transactions on Programming Languages and Systems</em>, volume 13, number 1, pages 124–149, January 1991.
<a href="http://dx.doi.org/10.1145/114005.102808">doi:10.1145/114005.102808</a></p>

<p>[12] Cynthia Dwork, Nancy Lynch, and Larry Stockmeyer:
“<a href="http://www.net.t-labs.tu-berlin.de/~petr/ADC-07/papers/DLS88.pdf" title="C Dwork, N Lynch, and L Stockmeyer. Consensus in the Presence of Partial Synchrony. JACM 35(2):288–323, 1988">Consensus in the Presence of Partial Synchrony</a>,”
<em>Journal of the ACM</em>, volume 35, number 2, pages 288–323, April 1988.
<a href="http://dx.doi.org/10.1145/42282.42283">doi:10.1145/42282.42283</a></p>

<p>[13] Christian Cachin, Rachid Guerraoui, and Luís Rodrigues:
<a href="http://www.distributedprogramming.net/" title="C Cachin, R Guerraoui, and L Rodrigues. Introduction to Reliable and Secure Distributed Programming, 2nd ed. Springer, 2011"><em>Introduction to Reliable and Secure Distributed Programming</em></a>,
Second Edition. Springer, February 2011. ISBN: 978-3-642-15259-7,
<a href="http://dx.doi.org/10.1007/978-3-642-15260-3">doi:10.1007/978-3-642-15260-3</a></p>


                ]]></content:encoded>
            </item>
        
            <item>
                <title>My year 2015 in review</title>
                <link>http://martin.kleppmann.com/2015/12/28/year-2015-review.html</link>
                <comments>http://martin.kleppmann.com/2015/12/28/year-2015-review.html#disqus_thread</comments>
                <pubDate>Mon, 28 Dec 2015 00:00:00 +0000</pubDate>
                <dc:creator>Martin Kleppmann</dc:creator>
                
                    <guid isPermaLink="true">http://martin.kleppmann.com/2015/12/28/year-2015-review.html</guid>
                
                <description><![CDATA[ I’ve had a pretty busy and interesting year 2015. Inspired by Caitie McCaffrey and Julia Evans, who recently wrote up reflections on their past year, I will try the same. A lot of my work this year has been to write and speak in public about technical topics, and I... ]]></description>
                <content:encoded><![CDATA[
                    <p>I’ve had a pretty busy and interesting year 2015. Inspired by
<a href="http://caitiem.com/2015/12/26/2015-a-year-in-review/">Caitie McCaffrey</a> and
<a href="http://jvns.ca/blog/2015/12/26/2015-year-in-review/">Julia Evans</a>,
who recently wrote up reflections on their past year, I will try the same.</p>

<p>A lot of my work this year has been to write and speak in public about technical topics, and I am
fortunate to have received a very positive response. Judging from what people have told me, both
online and in person, it seems that my ideas have been useful for people in designing their
software. This is very gratifying: although I find these topics interesting and useful (and hence
I wanted to talk about them), that doesn’t guarantee that other people will find them useful too.</p>

<h2 id="public-speaking">Public speaking</h2>

<p>I gave approximately 25 talks and lectures over the course of the year (I’m losing track of how many
I did exactly). Many of these were at major software engineering and data conferences in the UK, US,
Germany, Belgium, Sweden and Hungary, including
<a href="/2015/09/26/transactions-at-strange-loop.html">Strange Loop</a>,
<a href="/2015/11/04/transactions-at-code-mesh.html">Code Mesh</a>,
<a href="/2015/04/24/logs-for-data-infrastructure-at-craft.html">Craft</a>,
<a href="/2015/10/30/stream-processing-patterns-at-crunch.html">Crunch</a>,
<a href="/2015/06/02/change-capture-at-berlin-buzzwords.html">Berlin Buzzwords</a>,
<a href="/2015/11/13/change-data-capture-at-all-your-base.html">All Your Base</a>,
<a href="/2015/11/06/streams-as-team-interface-at-oredev.html">Øredev</a>,
<a href="/2015/09/30/data-liberation-with-kafka-at-strata.html">Strata New York</a> and
<a href="/2015/05/06/data-agility-at-strata.html">Strata London</a>.  A few were at internal events at various
organisations. See my <a href="/talks.html">talks archive</a> for the list of talks, and my
<a href="https://www.youtube.com/playlist?list=PLeKd45zvjcDHJxge6VtYUAbYnvd_VNQCx">YouTube playlist</a> for
the selection of talk recordings that made it onto YouTube.</p>

<p>Fairly few of those talks were verbatim repeats – I count at least 17 distinct talks over the
course of the year. Some of the talks shared material, of course (I don’t have anywhere near enough
new ideas to produce 17 entirely non-overlapping talks), but I like to think that there was at least
a nugget of something new and interesting in every one. I would get bored if I kept giving the same
talk over and over again, and it’s not much fun to listen to a speaker who’s bored of their own
talk!</p>

<p>Preparing all those talks and travelling to the various conferences has been quite time-consuming
and tiring. Thus, even though it has been a great experience, and I have enjoyed many great
conversations with people at those conferences, I will be reducing my conference activity to a more
sustainable level next year.</p>

<h2 id="book-writing">Book writing</h2>

<p>In 2015, we released chapters 4, 7 and 8 of my work-in-progress book,
<a href="http://dataintensive.net/">Designing Data-Intensive Applications</a>, as early release. This means
that two thirds of the planned 12 chapters are now available. However, work on chapter 9 has been
painfully slow, and I am painfully aware that it has now been over 6 months since we last released
a chapter.</p>

<p>The reason for the delay is twofold: I was too distracted by giving lots of talks, and the topic of
chapter 9 (consistency and consensus) is really hard. I wanted to make sure I really understood the
topic all the way to the core, with no niggling uncertanties or doubts, and so I spent a huge amount
of time researching the topic, reading many papers, and challenging myself to find intuitive and
correct explanations for things.</p>

<p>Frankly, so much nonsense has been written about the topic of distributed consistency that it has
been a bit of a challenge to find good source material. But the good news is that I think I have got
it figured out now, and making good headway with writing it up, so the release of chapter 9 should
not be too far away. And then the following chapters will be much easier, since they will cover more
straightfoward topics.</p>

<p>And by scaling back my speaking activity next year, I hope to get the rest of the book out quickly.</p>

<h2 id="new-job">New job</h2>

<p>After leaving LinkedIn in 2014, I took a 14-month sabbatical/funemployment to focus full-time on
book writing, speaking, blogging and open source. Since October 2015, I am once again employed,
namely as a postdoc on a research project at the
<a href="http://www.cl.cam.ac.uk/">University of Cambridge Computer Laboratory</a>.</p>

<p>The fact that I got a postdoc position may be surprising, since I actually don’t have a PhD.
However, the university was happy to regard my industrial experience as “equivalent to a PhD”,
whatever that means. Thumbs up to them for recognising my degree from the School of Hard Knocks.</p>

<p>So far I’m really enjoying being in academia, and the research project I’m on is super interesting.
I’ll be blogging and talking about it lots more in future, but for now I’ll just say this: my area
of focus is moving away from stream processing, and towards information security. There are big and
important problems to be solved at the intersection of data systems, distributed systems, infosec
and cryptography. Watch this space!</p>

<h2 id="non-book-writing">Non-book writing</h2>

<p>As a way of procrastinating from book-writing, I published three research papers in 2015, in three
different areas of computer science (namely distributed systems, cryptographic protocols, and data
management):</p>

<ul>
  <li>“<a href="http://arxiv.org/abs/1509.05393">A Critique of the CAP Theorem</a>” explains in detail why I think
the CAP theorem is terrible, and why we should all stop quoting it. In this paper I catalogue
various common misunderstandings and ambiguities, prove a more precisely formulated theorem, and
suggest a saner alternative to CAP.</li>
  <li>“<a href="/papers/mrsa-pass15.pdf">Strengthening public key authentication against key theft</a>” (with
<a href="http://cirw.in/">Conrad Irwin</a>) describes a cryptographic authentication protocol that is
somewhat robust against someone stealing your private key. We presented this paper at the
<a href="/2015/12/08/preventing-key-theft-at-passwords15.html">9th International Conference on Passwords</a>.</li>
  <li>“<a href="/papers/kafka-debull15.pdf">Kafka, Samza and the Unix philosophy of distributed data</a>” (with
<a href="https://twitter.com/jaykreps">Jay Kreps</a>) describes the design philosophy of Apache Kafka and
stream processing framework Apache Samza by analogy to Unix pipes. This article is a refined
version of my <a href="/2015/08/05/kafka-samza-unix-philosophy-distributed-data.html">prior blog post</a>
on the topic. It was invited to appear in the December 2015 issue of the
<a href="http://sites.computer.org/debull/bull_issues.html">IEEE Data Engineering Bulletin</a>.</li>
</ul>

<p>I also wrote up transcripts of several of my conference talks as oversized blog posts:</p>

<ul>
  <li><a href="/2015/01/29/stream-processing-event-sourcing-reactive-cep.html">Stream processing, Event sourcing, Reactive, CEP… and making sense of it all</a></li>
  <li><a href="/2015/03/04/turning-the-database-inside-out.html">Turning the database inside-out with Apache Samza</a></li>
  <li><a href="/2015/04/13/real-time-full-text-search-luwak-samza.html">Real-time full-text search with Luwak and Samza</a></li>
  <li><a href="/2015/04/23/bottled-water-real-time-postgresql-kafka.html">Bottled Water: Real-time integration of PostgreSQL and Kafka</a></li>
  <li><a href="/2015/05/27/logs-for-data-infrastructure.html">Using logs to build a solid data infrastructure (or: why dual writes are a bad idea)</a></li>
  <li><a href="/2015/08/05/kafka-samza-unix-philosophy-distributed-data.html">Kafka, Samza, and the Unix philosophy of distributed data</a></li>
</ul>

<p>An edited and updated version of those blog posts is planned to be published as a report by O’Reilly
in 2016. Complete with hand-drawn slides, of course.</p>

<p>Besides those transcripts, I published a few blog posts on topics other than stream processing:</p>

<ul>
  <li><a href="/2015/05/11/please-stop-calling-databases-cp-or-ap.html">Please stop calling databases CP or AP</a>
was a precursor to my <a href="http://arxiv.org/abs/1509.05393">CAP theorem paper</a>. It was a bit
contentious, but I stand by my point that the CP/AP categorisation is more confusing than helpful.</li>
  <li><a href="/2015/10/11/recurse-center-joy-of-learning.html">The Recurse Center and the joy of learning</a>
describes my experience of spending two weeks as a resident at the
<a href="https://www.recurse.com/">Recurse Center</a>, an educational retreat for programmers in NYC.</li>
  <li><a href="/2015/11/10/investigatory-powers-bill.html">The Investigatory Powers Bill would increase cybercrime</a> –
see politics section below. Apologies for the “cyber”, but I decided it was important to speak the
language of my target audience.</li>
</ul>

<h2 id="interviews">Interviews</h2>

<ul>
  <li>Werner Schuster <a href="http://www.infoq.com/interviews/kleppmann-data-infrastructure-logs-crdt">interviewed me for InfoQ</a>
about my <a href="/2015/05/27/logs-for-data-infrastructure.html">talk at Craft</a>, and about the future of
data systems design.</li>
  <li>Jim Brikman interviewed me for his book “<a href="http://www.hello-startup.net/">Hello, Startup</a>”
regarding the pros and cons of startup life.</li>
  <li>PwC Technology Forecast used some quotes from me in their report on “<a href="http://www.pwc.com/us/en/technology-forecast/2015/remapping-database-landscape/immutable-data-stores--rise.html">Remapping the database landscape: 
the rise of immutable data stores</a>”.</li>
  <li>The September 2015 issue of <a href="http://www.linuxjournal.com/">Linux Journal</a> republished some of my
older work on <a href="/2013/05/24/improving-security-of-ssh-private-keys.html">SSH private key encryption</a>.</li>
</ul>

<h2 id="politics">Politics</h2>

<p>The UK government has been pursuing some disastrous information security policies. I have
participated in the political process by adding a technical voice to the debate:</p>

<ul>
  <li>Early in 2015, I warned members of the House of Lords about the draft
<a href="https://en.wikipedia.org/wiki/Draft_Communications_Data_Bill">Communications Data Bill</a>
(“Snoopers’ Charter”), which was eventually withdrawn. However, an even worse proposal called the
<a href="https://en.wikipedia.org/wiki/Draft_Investigatory_Powers_Bill">Investigatory Powers Bill</a>
was introduced in November 2015, about which I contacted my MP.</li>
  <li>I wrote a <a href="/2015/11/10/investigatory-powers-bill.html">blog post</a> explaining why you should be
worried about the Investigatory Powers Bill even if you don’t care about privacy, and you are
happy with government services scanning all your communication. In a nutshell: the bill
mandates backdoors and security holes in software. It is a bad idea to make systems insecure by
design.</li>
  <li>I submitted <a href="http://data.parliament.uk/writtenevidence/committeeevidence.svc/evidencedocument/science-and-technology-committee/investigatory-powers-bill-technology-issues/written/25146.html">written evidence</a>
to the <a href="http://www.parliament.uk/business/committees/committees-a-z/commons-select/science-and-technology-committee/inquiries/parliament-2015/investigatory-powers-bill-technology-issues-inquiry-launch-15-16/publications/">House of Commons Science and Technology Committee</a>,
explaining the problems I see with the draft Investigatory Powers Bill.</li>
  <li>I also submitted similar <a href="http://data.parliament.uk/writtenevidence/committeeevidence.svc/evidencedocument/draft-investigatory-powers-bill-committee/draft-investigatory-powers-bill/written/26275.html">written evidence</a>
to the <a href="http://www.parliament.uk/documents/joint-committees/draft-investigatory-powers-bill/ipb-call-for-evidence.pdf">House of Commons and House of Lords joint committee on the draft
Investigatory Powers Bill</a>.</li>
</ul>

<p>The debate is still ongoing, but I hope the warnings from computer scientists and software
developers will be heard.</p>

<h2 id="open-source">Open Source</h2>

<p>This year I released <a href="https://github.com/confluentinc/bottledwater-pg">Bottled Water</a>, a tool for
Postgres that captures all the data written to a database, and replicates it to Kafka for use in
other systems (e.g. building search indexes or maintaining caches). Lots of people have started
playing with Bottled Water, and I have received several good pull requests and bug reports.</p>

<p>Bottled Water is still a very early-stage project, but I am optimistic about it. I plan to integrate
it with the new <a href="http://www.confluent.io/blog/how-to-build-a-scalable-etl-pipeline-with-kafka-connect">Kafka Connect</a>
framework (pull requests welcome!), which will make it easier to deploy. Similar ideas have also been
popping up for other databases, for example
<a href="https://developer.zendesk.com/blog/introducing-maxwell-a-mysql-to-kafka-binlog-processor">Maxwell for MySQL</a>.</p>

<p>Besides Bottled Water, I have made some small open source contributions to
<a href="http://avro.apache.org/">Apache Avro</a> and a few other projects. My
<a href="https://github.com/ept/warc-hadoop">WARC library</a> is now
<a href="https://issues.apache.org/jira/browse/NUTCH-2102">used in Apache Nutch</a>.
There are probably some more things that I have forgotten.</p>

<h2 id="conclusion">Conclusion</h2>

<p>Another year has passed, and it’s easy to forget everything one has done. So I find it motivating
to write down the major things that have happened, and see that actually it was quite a lot.</p>

<p>If anything, I have spread myself too thin, trying to do too many different things. Certainly my
progress with the book has not been as fast as I (or my editor!) would have liked. I am honoured
that so many conferences want me to speak, but unfortunately I will have to dial down my number of
talks, otherwise I’ll never get the book finished.</p>

<p>Thank you to <a href="http://confluent.io/">Confluent</a>, who sponsored a lot of my work in the last year on
a freelance basis. Thank you also to all the great people I’ve met, who have shared their ideas and
helped me understand things. Happy new year!</p>

<p><em>Update 7 Jan 2016</em>: added link to
<a href="http://data.parliament.uk/writtenevidence/committeeevidence.svc/evidencedocument/draft-investigatory-powers-bill-committee/draft-investigatory-powers-bill/written/26275.html">evidence I submitted to parliament</a>, which has now bene published.</p>

                ]]></content:encoded>
            </item>
        
            <item>
                <title>The Investigatory Powers Bill would increase cybercrime</title>
                <link>http://martin.kleppmann.com/2015/11/10/investigatory-powers-bill.html</link>
                <comments>http://martin.kleppmann.com/2015/11/10/investigatory-powers-bill.html#disqus_thread</comments>
                <pubDate>Tue, 10 Nov 2015 00:00:00 +0000</pubDate>
                <dc:creator>Martin Kleppmann</dc:creator>
                
                    <guid isPermaLink="true">http://martin.kleppmann.com/2015/11/10/investigatory-powers-bill.html</guid>
                
                <description><![CDATA[ As widely reported, the UK government proposed the draft of a new Investigatory Powers Bill last week. There has been much discussion of the bill already, but there are some important questions which I have not yet seen addressed. These open questions raise serious concerns about the effects that the... ]]></description>
                <content:encoded><![CDATA[
                    <p>As widely reported, the UK government proposed the draft of a new <a href="https://www.gov.uk/government/collections/draft-investigatory-powers-bill">Investigatory Powers
Bill</a> last week. There has been much discussion of the bill already, but there are some
important questions which I have not yet seen addressed.</p>

<p>These open questions raise serious concerns about the effects that the proposed law would have on
ordinary citizens. In this article I argue that if the bill is passed in its current form, its
effect will be exactly the opposite of that intended: it would leave citizens <em>more</em> exposed to the
risks of crime and terrorism, and not reduce those risks as intended.</p>

<h2 id="background-of-the-bill">Background of the bill</h2>

<p>The stated purpose of the Investigatory Powers Bill is to help fight crime and terrorism – in other
words, to uphold the rule of law and to keep us safe from aggression. If people break the law or
harm citizens (for example through paedophilia, murder, or acts of terrorism), our law enforcement
services require the means to find the perpetrators and hold them responsible. This is an important
public service: a country with ineffective law enforcement would quickly become dysfunctional. </p>

<p>There is a well-known tension between the ability of law enforcement services to get the information
they require to find and convict criminals, and the basic <a href="http://gilc.org/privacy/survey/intro.html">human right of privacy</a> for
innocent citizens. In order to do their job, the police and the intelligence services must
necessarily intrude on individuals’ privacy to some degree. For us as a society, it is important
that we decide how much privacy we are collectively <a href="http://www.theguardian.com/commentisfree/2015/nov/08/surveillance-bill-snoopers-charter-george-orwell">willing to sacrifice</a> for the
sake of helping law enforcement services do their job.</p>

<p>However, this article is not about the tension between security and privacy – it is only about
security. The Investigatory Powers Bill is supposed to increase our security by giving law
enforcement services the tools they need to catch criminals and terrorists. I will argue that, in
fact, the proposed bill <em>harms our security</em> by making us <em>more vulnerable</em> to attacks by criminals
and terrorists.</p>

<p>People who defend surveillance often say that a loss of privacy is <a href="http://www.theyworkforyou.com/debates/?id=2015-11-04a.969.0#g991.4">a price that many people are
willing to pay</a> for the sake of increased safety. However, what if the surveillance
measures actually make people <em>less</em> safe? Even if you do not care about privacy, you should be
strongly opposed to this bill, because it vastly increases the risks of cybercrime, as explained
below.</p>

<h2 id="encrypted-communication">Encrypted communication</h2>

<p>Encryption ensures that your information can only be read by the correct recipient, and not by any
random bystander. Whenever you send information over the internet, if that information has any
value whatsoever, it should be encrypted – otherwise anyone (e.g. people using the same coffee shop
wifi as you) can trivially steal it. Thus, almost all websites use encryption whenever passwords,
credit card numbers, online banking information, or similar sensitive information is involved. If we
didn’t encrypt this information, fraud and identity theft would be rampant. Encryption is a basic
necessity for the internet.</p>

<p>However, encryption can be applied at different levels. For example, in a chat or telephony app,
there are two options:</p>

<ul>
  <li><strong>Encryption in transit:</strong> In this case, the data is encrypted as it is transmitted between your
device and the service provider (e.g. a mobile network operator or a social network), but the
service provider handles the data in unencrypted form. This is illustrated in Figure 1a. In this
case, the service provider is able to read all of the communication, and you need to trust them to
safeguard your information appropriately.</li>
  <li><strong>End-to-end encryption:</strong> In this case, data is encrypted all the way between you and the person
you’re talking to. The service provider only passes on the messages, but it cannot see what you
are talking about (Figure 1b). There may not even <em>be</em> a service provider, because anybody can
write their own app that communicates over the internet, without requiring a centrally managed
service (Figure 1c).</li>
</ul>

<p>If law enforcement decides to investigate you, and encryption in transit is used, that makes life
very easy for law enforcement: they can serve a warrant to the service provider, and get them to
wiretap your communication without you ever knowing. With end-to-end encryption, surveillance is
still possible, but it’s more expensive: since the service provider cannot access the messages,
a warrant to the service provider is of no use. Instead, law enforcement must go to the people
communicating. For example, they can obtain a court order to seize your phone, or they can point
<a href="http://www.cl.cam.ac.uk/~mgk25/pet2004-fpd.pdf">microphones and antennas</a> at your house to listen to your communications, or they
can infiltrate the suspected gang.</p>

<p><img src="/2015/11/end-to-end.png" width="550" height="357" alt="Figure 1: The difference between encryption in transit and end-to-end encryption." /></p>

<p style="text-align: center;"><em>Figure 1: The difference between encryption in transit and end-to-end encryption.</em></p>

<h2 id="the-value-of-end-to-end-encryption">The value of end-to-end encryption</h2>

<p>Although encryption in transit is widely used, it has serious security problems. For example, the
service provider could be hacked by an adversary, or compromised by an insider, causing sensitive
information can be leaked. A fault in the service provider could cause data to be corrupted. For
these reasons, security experts are pushing towards widespread use of end-to-end encryption, which
reduces the exposure to such attacks.</p>

<p>The goal of end-to-end encryption is <em>not</em> to prevent legitimate access by law enforcement in cases
where it is justified for a criminal investigation. Rather, the goal is to defend systems against
adversaries who want to steal sensitive data or cause systems to malfunction. Such defence is
particularly critical in cases where human life is at stake, such as <a href="http://www.wired.com/2015/01/german-steel-mill-hack-destruction/">industrial control
systems</a>, <a href="http://www.wired.com/2015/07/hackers-remotely-kill-jeep-highway/">internet-connected cars</a>, or <a href="http://www.slideshare.net/MarieGMoe/2015-1021keynotehacklumariemoe">medical data and devices</a>. But
even in other domains, such as trade secrets and financial information in enterprises, in journalism
or in legal professions, it is crucial that sensitive information is adequately protected.</p>

<p>End-to-end encryption helps protect our own information against theft and manipulation by
adversaries – ranging from an individual disgruntled employee, to hostile foreign intelligence
services who may be spying or sabotaging for economic, political or military reasons. As more and
more aspects of the world are <a href="http://www.wsj.com/articles/SB10001424053111903480904576512250915629460">controlled by software</a>, and as increasingly many devices
are connected to the internet, a cyberattack against weakly secured systems could have catastrophic
consequences.  We will need all the defences we can get, and end-to-end encryption is going to be an
indispensable part of our security infrastructure.</p>

<h2 id="exceptional-access-to-encrypted-communication">Exceptional access to encrypted communication</h2>

<p>On the other hand, end-to-end encryption makes life harder for law enforcement services, because
they cannot simply serve a warrant to the service provider in order to obtain the content of the
communication. For this reason, politicians have recently attacked end-to-end encryption; for
example, in a speech in January 2015, <a href="https://embed.theguardian.com/embed/video/uk-news/video/2015/jan/12/david-cameron-spy-agencies-britain-video">PM David Cameron said</a>:</p>

<blockquote>
  <p>In our country, do we want to allow a means of communication between people which, even in
extremis, with a signed warrant from the home secretary personally, that we cannot read? […] The
question remains: are we going to allow a means of communications where it simply is not possible
to do that? My answer to that question is: no, we must not. The first duty of any government is to
keep our country and our people safe.</p>
</blockquote>

<p>The proposed Investigatory Powers Bill is an attempt to cast into law this principle outlined by the
Prime Minister. In particular, it places a duty on communications service providers to allow law
enforcement agencies to intercept communication when served with a warrant – even if the service
provider is outside the UK (§31). It also requires service providers to assist with hacking devices
and removing encryption if compelled by an order from the home secretary (§189).</p>

<p>The bill and related guidance notes from the government do not explain how these rules might be put
into practice, but they have <a href="http://www.telegraph.co.uk/news/uknews/terrorism-in-the-uk/11970391/Internet-firms-to-be-banned-from-offering-out-of-reach-communications-under-new-laws.html">been interpreted</a> as requiring services with end-to-end
encryption to have some kind of “backdoor” by which they <a href="http://www.independent.co.uk/life-style/gadgets-and-tech/news/investigatory-powers-bill-could-allow-government-to-ban-end-to-end-encryption-technology-powering-a6725311.html">can be broken</a>, if required.
Other sources say that the government <a href="http://uk.businessinsider.com/investigatory-powers-bill-wont-ban-end-to-end-encryption-2015-11">does not wish to ban end-to-end encryption</a>,
but in that case it is not clear what they <em>do</em> want, since the Prime Minister has reiterated his
plea that terrorists, paedophiles and criminals must not be allowed a “safe space” online.</p>

<p>Security professionals have no interest in making life easy for terrorists, paedophiles and
criminals. However, <em>any</em> technology can be used for both good and bad purposes. The government has
not explained which technologies would comply with the new rules, and which technologies would
violate them – and the text of the bill itself is very vague and ambiguous.</p>

<p>If the bill requires communication services to have some mechanism of obtaining the content of the
communication in response to a warrant, that means the service must somehow retain the ability to
decrypt the data when required. Provisions for such <em>exceptional access</em> (e.g. <a href="https://www.schneier.com/paper-key-escrow.html">key
escrow</a> or backdoors) are normally avoided in encryption products, because they
introduce serious security problems.</p>

<h2 id="exceptional-access-is-insecure">Exceptional access is insecure</h2>

<p>At first glance, it may seem reasonable to require that all encryption products must include
a provision for data to be decrypted by law enforcement agencies, as long as the decryption order is
protected with sufficient oversight to prevent abuse. However, on closer inspection, it turns out
that this proposal is deeply flawed.</p>

<p>The problem is laid out very clearly in <a href="http://dspace.mit.edu/handle/1721.1/97690">a recent report</a>, written by some of the biggest
names in cryptographic research and security engineering worldwide. To quote from the report:</p>

<blockquote>
  <p>Political and law enforcement leaders in the United States and United Kingdom … propose that
data storage and communications systems must be designed for exceptional access by law enforcement
agencies. These proposals are unworkable in practice, raise enormous legal and ethical questions,
and would undo progress on security at a time when Internet vulnerabilities are causing extreme
economic harm. As computer scientists with extensive security and systems experience, we believe
that law enforcement has failed to account for the risks inherent in exceptional access systems.</p>
</blockquote>

<p>The report lays out in <a href="http://dspace.mit.edu/handle/1721.1/97690">no uncertain terms</a>: there is no known method for securely
providing law enforcement with exceptional access to systems with end-to-end encryption. Any method
that provides exceptional access immediately exposes the system to attacks by malicious parties,
rendering the protection of encryption essentially worthless.</p>

<p>Exceptional access would probably require that government departments have some kind of master keys
that allowed them to decrypt any communication if required. Those master keys would obviously have
to be kept extremely secret: if they were to become public, the entire security infrastructure of
the internet would crumble into dust.</p>

<p>How good are government agencies at keeping secrets? Even just in the last few months, the OPM
failed to protect <a href="http://www.theguardian.com/technology/2015/jul/09/opm-hack-21-million-personal-information-stolen">millions of their own personnel records</a> from hackers, the email
account of <a href="http://motherboard.vice.com/read/hackers-release-alleged-ssn-numbers-stolen-from-cia-directors-aol-account">CIA Director John Brennan was hacked</a>, and the <a href="https://theintercept.com/2015/09/17/tsa-doesnt-really-care-luggage-locks-hacked/">master keys for TSA
locks</a> were accidentally posted on the internet. The US Air Force has been accused of
<a href="http://thebulletin.org/okinawa-missiles-october8826">accidentally broadcasting the launch codes for nuclear missiles</a> over radio in the 1960s.
These incidents do not fill me with confidence that any government would be able to handle
cryptographic master keys securely.</p>

<p>If the law enforcement services can remotely break into the device of a suspect, then sooner or
later criminals will find ways to use the same mechanism to break into devices and steal or destroy
your personal data. They will know your location, and the PIN for your burglar alarm, so they will
have an easy time breaking into your house. There is simply no technical mechanism that will allow
legitimate access by law enforcement, and which is also unbreakable by people who want to do you
harm.</p>

<p>I’ll say it again, to be absolutely clear: <em>any</em> mechanism that can allow law enforcement legitimate
access to data can <em>inevitably</em> be abused by hostile foreign intelligence services, and even
technically sophisticated individuals, to break into systems and gain unauthorised access to the
same data. There is <em>no known method</em> for making this secure. If we add provisions for exceptional
access to encryption products, we are simply shooting ourselves in the foot.</p>

<h2 id="retention-of-browsing-history">Retention of browsing history</h2>

<p>Another provision of the proposed Investigatory Powers Bill is that internet service providers
(ISPs) must retain a record of all the websites you visit (more specifically, all the IP addresses
you connect to) for one year. This appears to be another measure to weaken privacy while
strengthening security – but in fact, it is harmful to <em>both</em> privacy and security.</p>

<p>In order to maintain a record of every website you have visited in the last year, the ISP must store
that information somewhere accessible. Information that is stored somewhere accessible will sooner
or later be stolen by attackers. For example, just a few weeks ago, <a href="http://www.theguardian.com/business/2015/oct/23/talktalk-hacking-crisis-deepens-as-more-details-emerge">records of millions of TalkTalk
customers were stolen</a> due to a <a href="https://tommorris.org/posts/9396">SQL injection</a> (one of the most easily
preventable security issues – the fact that TalkTalk was vulnerable to such a simple bug casts
serious doubt on their competence in elementary software engineering practices). If ISPs are
required to store your browsing history, it is only a matter of time before it is stolen.</p>

<p>And stolen browsing history is a security problem. After the website <a href="http://www.theguardian.com/technology/2015/aug/19/ashley-madison-hackers-release-10gb-database-of-33m-infidelity-site-accounts">Ashley
Madison</a> (which helps married people have an affair) was hacked, millions of its
users found their real name, home address, email address and credit card numbers spewed all over the
internet. It did not take long before <a href="http://www.csoonline.com/article/2980631/data-breach/blackmail-rising-from-ashley-madison-breach.html">blackmailers started using the data</a>, threatening
users that <a href="http://www.zdnet.com/article/in-ashley-madisons-wake-heres-one-mans-story-of-sex-sorrow-and-extortion/">their spouse would be informed</a> unless a ransom was paid. Browsing history
retained by an ISP would carry the same blackmail risk.</p>

<p>The problem is not only extortion of money from the victims of blackmail, but also a security
problem. What if someone succeeds in blackmailing employees of an intelligence agency, or senior
civil servants, or a government official? If the victim fears for their reputation or repercussions
from the release of the sensitive information, the attacker gains power over the victim, which is
worrisome if the victim is in a position of power.</p>

<p>Increasingly, stolen personal information is being used for <a href="https://www.schneier.com/blog/archives/2015/11/the_rise_of_pol.html">politically motivated blackmail and
intimidation</a>. Even if you personally have never done anything embarrassing, and you have
nothing to hide, the fact that other people can be blackmailed is a risk to you if those people have
power over you.</p>

<p>“Give me six lines written by the most honest man in the world, and I will find enough in them to
hang him.” (Origin uncertain, attributed to <a href="https://en.wikiquote.org/wiki/Cardinal_Richelieu">Cardinal Richelieu</a>.) Or, to give a more
modern equivalent: “We kill people based on metadata.” (Former CIA and NSA director <a href="https://www.rt.com/usa/158460-cia-director-metadata-kill-people/">Michael
Hayden</a>.)</p>

<h2 id="we-must-make-systems-more-secure-not-less">We must make systems more secure, not less</h2>

<p>As <a href="https://embed.theguardian.com/embed/video/uk-news/video/2015/jan/12/david-cameron-spy-agencies-britain-video">David Cameron said</a>, “the first duty of any government is to keep our country
and our people safe.” The proposed Investigatory Powers Bill is supposed to make us more safe by
giving great powers to the law enforcement services. However, in this article I have argued that the
bill would in fact make us <em>significantly less safe</em>, by making internet security systems vulnerable
to cyberattacks, and by increasing the risk of blackmail.</p>

<p>The proposed bill, as it stands now, is too vague to allow any serious technical analysis to take
place. With regard to encryption technologies, it fails to specify what is allowed and what is not.
But the Prime Minister’s repeated assertion that we “make sure we do not allow terrorists safe
spaces to communicate with each other” implies a <a href="http://www.theguardian.com/world/2015/nov/10/surveillance-bill-dire-consequences-apple-tim-cook">worrisome weakening of security
technologies</a>.</p>

<p>Nobody wants to give criminals a safe space in which they can operate. However, the technologies
that help protect industrial control systems, cars, medical devices, lawyers, journalists and
businesses against attacks by malicious parties are <em>the same</em> as the technologies behind which
criminals can hide. Any technology can be used for good and bad.</p>

<p>It is not possible to eliminate “safe spaces” for criminals without also eliminating security from
the computer systems that our daily lives depend on. I am worried that the Investigatory Powers Bill
would effectively <a href="http://www.theguardian.com/technology/2015/nov/09/tech-firms-snoopers-charter-end-strong-encryption-britain-ip-bill">mandate computer systems to be insecure</a>, and thus leave our
infrastructure vulnerable to cyberattacks from people who want to do us harm. </p>

<p>According to the <a href="https://www.gov.uk/government/uploads/system/uploads/attachment_data/file/60943/the-cost-of-cyber-crime-full-report.pdf">government’s own report</a>, cybercrime is a Tier One risk to national
security, and already costs the UK £27bn per year. This is only going to get worse if we do not
improve the security of our computer systems. As internet-connected devices are increasingly used
for matters of life and death, the security of those devices becomes paramount, and breaches could
have catastrophic consequences. We need to do everything we can to <em>strengthen</em> the security of
those systems, not to weaken them.</p>

<p>I recognise that as systems become more secure, surveillance becomes more difficult for the
intelligence services. I acknowledge that secure communication systems may allow a terrorist plot or
a crime to succeed which may have been thwarted if surveillance was easy for law enforcement
services. But I argue that this risk is <a href="http://motherboard.vice.com/blog/youll-never-guess-how-many-terrorist-plots-the-nsas-domestic-spy-program-has-foiled">tiny</a> compared to the risk of an
insecure, vulnerable infrastructure in which terrorist cyberattackers could wreak havoc.</p>

<p>Aside from the proposed bill’s disregard for civil liberties, even if we consider only the security
implications of the bill, it is deeply worrisome. As more technical details of the proposal become
clear, we must carefully examine to what extent they leave us less secure than we were before.</p>

<p><em>Thank you to Alastair Beresford and Diana Vasile for reviewing a draft of this article.</em></p>

                ]]></content:encoded>
            </item>
        
            <item>
                <title>The Recurse Center and the joy of learning</title>
                <link>http://martin.kleppmann.com/2015/10/11/recurse-center-joy-of-learning.html</link>
                <comments>http://martin.kleppmann.com/2015/10/11/recurse-center-joy-of-learning.html#disqus_thread</comments>
                <pubDate>Sun, 11 Oct 2015 00:00:00 +0000</pubDate>
                <dc:creator>Martin Kleppmann</dc:creator>
                
                    <guid isPermaLink="true">http://martin.kleppmann.com/2015/10/11/recurse-center-joy-of-learning.html</guid>
                
                <description><![CDATA[ I have just spent an excellent two weeks as a resident at the Recurse Center (RC). As RC is a somewhat unusual institution, and several people have asked me about it, I would like to take this opportunity to reflect on what I have learnt. Recurse Center describes itself as... ]]></description>
                <content:encoded><![CDATA[
                    <p>I have just spent an excellent two weeks as a <a href="https://www.recurse.com/blog/85-more-new-residents-for-2015">resident</a> at the <a href="https://www.recurse.com/">Recurse Center</a> (RC).
As RC is a somewhat unusual institution, and several people have asked me about it, I would like to
take this opportunity to reflect on what I have learnt.</p>

<p>Recurse Center describes itself as an <em>educational retreat for programmers</em>. Anyone can apply, and
if you are accepted, you get to spend three months at the Recurse Center in New York. It is free to
attend. You are part of a <em>batch</em>, a group of people who all start at the same time. A new batch
starts every six weeks, so the first and second half of each batch overlaps with the preceding and
following batch, respectively. A batch consists of about 15 to 30 people.</p>

<p>What do you do during those three months? That is largely up to you. There is no curriculum, no exam
or dissertation. The goal is simply for you to become a better programmer during that time, through
self-directed study and practice. How you go about that goal is your choice, but RC provides support
structures to help you achieve the goal.</p>

<p>The three months give you time to focus, without the distraction of a job or project deadlines. This
is your chance to learn challenging new things: three months is long enough that you can build up
a significant amount of new skills, but short enough that great focus is needed.</p>

<p>At the end of the three months, RC will help you find a job at a good company, if you want one, but
there is no obligation to take a job. Alumni often stay involved with the RC community long after
their three months have come to an end.</p>

<h2 id="the-culture-of-recurse-center">The culture of Recurse Center</h2>

<p>The most striking thing about RC is the care with which the atmosphere and culture is designed.
Everything is geared towards providing a supportive environment in which everyone feels welcome, no
matter who they are or what background they come from.</p>

<p>Most technology companies and events I’ve seen are dominated by straight middle-class white and
Asian men. I count myself towards that group, and I don’t have any problem with white men <em>per se</em>,
but it’s an extremely biased sample of humanity, and such bias is not healthy for a community.
Fortunately, the Recurse Center is different. The people who go to RC are a much more representative
cross-section of humanity (in terms of ethnicity, gender, nationality, sexuality, social background,
etc) than most other venues in technology, which is wonderful and heartwarming.</p>

<p>I attribute this diversity directly to RC’s conscious efforts to create a safe space in which
everyone feels welcome. This includes explicit social rules to avoid bigoted (sexist, racist,
homophobic, transphobic, ageist, ableist, etc) language and ideas, and encouraging everybody to ask
questions and discuss technical topics without fear of being seen as stupid or clueless. Not knowing
something is celebrated as an opportunity to learn, not a fault to be embarrassed about.</p>

<p>In the batches I met at RC, some people had been programming for decades, and some only for a few
months. Some had built websites or mobile apps, some had built machine learning systems, others had
built low-level communication systems for satellites. Some people had non-computing backgrounds such
as oceanography, microbiology, religious studies, linguistics and poetry. But all had an
enthusiastic desire to learn.</p>

<h2 id="everyone-is-a-beginner-at-something">Everyone is a beginner at something</h2>

<p>One thing I had wondered about before coming to RC: if people have such vastly differing amounts of
programming experience, how can people productively help each other? If they are all in the same
room, won’t the beginners get intimidated and overwhelmed, while the experienced software engineers
get bored?</p>

<p>After working with a few members of the batch, I realised why the varying levels of experience are
not a problem for RC: everyone is a beginner at the thing they are exploring at RC. If you are an
experienced software developer, you don’t go to RC in order to keep doing the same things as you did
at your last job, using the same languages to build the same kind of application. No, you probably
join RC because you want to learn something completely different.</p>

<p>Your time at RC is an opportunity to “level up” your craft. If you’re an experienced web developer,
how about learning a functional language like <a href="https://www.haskell.org/">Haskell</a> or <a href="http://www.idris-lang.org/">Idris</a>? If you’re a veteran C++
hacker, maybe you want to learn about formal methods like <a href="https://coq.inria.fr/">Coq</a> or <a href="http://research.microsoft.com/en-us/um/people/lamport/tla/tla.html">TLA+</a>? Even if you’ve
been writing software for 20 years, you’re a complete newbie when you move so far out of your
comfort zone – so you’re actually not that different from someone who is trying to get their first
small piece of Python code to work. You’re a beginner too.</p>

<p>At RC, I learnt to appreciate that each individual person has a rich and interesting background, and
that it’s actually not particularly important what percentage of that background is in programming
activities versus non-programming activities. There is nothing about programming that makes it in
any way superior to, say, oceanography or poetry. All of us are just trying to learn something new,
and occasionally one person can help out another because they have more experience in one particular
area, but in another area those roles may well be reversed.</p>

<h2 id="the-joy-of-learning-for-its-own-sake">The joy of learning for its own sake</h2>

<p>Another thing that struck me about RC was the breadth of different things that people wanted to
learn: from fairly mainstream things like game programming in JavaScript, to obscure experimental
programming languages that I hadn’t even heard of.</p>

<p>Each person has their own reasons for their choice of technology to work on, and no justification is
required. Outside of the Recurse Center, people tend to focus on <em>marketable</em> skills, i.e. learning
things that are likely to help them in a job. Within RC, although getting a job is certainly a goal
for some people, the marketability of a technology does not seem to be a dominant factor in
Recursers’ choice of technology to work on. RC is not a training program for web developers or
mobile developers!</p>

<p>In practical terms, this means that some people will choose to work on things that are unlikely to
directly help them in a job, but which are simply <em>interesting</em> (e.g. creating a new programming
language). I find this non-utilitarian approach wonderful. Nobody will criticize your project for
not being “useful”: what matters is what you learn from doing it.</p>

<p>Recurse Center is not about startups. It is specifically about sharpening your technical skills, not
about building a product. During RC you will probably work on projects, because the best way of
deeply understanding an idea is to apply it to a concrete problem. However, the focus is on your
learning, not on the artifacts that you create as a side-effect of learning.</p>

<p>Only at the Recurse Center have I seen people genuinely giddy, bouncing up and down with excitement,
at the prospect of trying out some new language or technology. I don’t think I ever saw that while
studying computer science at university. This joy of learning something simply because it is
interesting, this intellectual curiosity, this childlike sense of wonder, is something we must
treasure.</p>

<h2 id="levelling-up-for-experienced-programmers">“Levelling up” for experienced programmers</h2>

<p>There is no doubt that the Recurse Center is ideal for beginner programmers: it is an incredibly
welcoming, supportive, encouraging environment where you can learn a lot in a short time. But is it
also worthwhile for experienced software engineers?</p>

<p>For the last year I have taken a “sabbatical” (read: been unemployed), doing only occasional bits of
consulting work, and otherwise focussing on on writing my <a href="http://dataintensive.net/">book</a>, doing <a href="http://arxiv.org/abs/1509.05393">research</a>, and giving
<a href="http://martin.kleppmann.com/talks.html">talks</a> about my work. It has been incredibly rewarding, because I have had the opportunity to
think through and understand topics in much greater depth than I could have done whilst working
a job and meeting project deadlines. I have worked hard during this year, and I feel that I have
grown substantially in my craft.</p>

<p>Now, I understand if such a completely self-organised sabbatical is not for everyone. But when
I look at RC, I see an opportunity for other experienced programmers to have a similar personal
growth experience as I have enjoyed. In the supportive environment of RC it is easier to leave your
comfortable software engineering job and set sail into uncharted waters.</p>

<p>If you want to help <em>create</em> the future of software development, rather than just coming along for
the ride, you will need to bring together ideas from very different areas of computer science, and
even incorporate ideas from outside of computer science. You will need to learn about things that do
not have much commercial value today, but which will make programming better for everyone in ten
years’ time.</p>

<p>It is unlikely that you will have the time to learn those things and take such a long-term view on
the side during your day job – you will need to make dedicated time. A sabbatical is a good option,
but it requires a lot of self-discipline to stay on track. The support structures of RC can help you
take your craft to the next level.</p>

<h2 id="conclusion">Conclusion</h2>

<p>If you’re a programmer (no matter what your level of experience), do take a look at the Recurse
Center <a href="https://www.recurse.com/about">about page</a>, <a href="https://www.recurse.com/faq">FAQ</a> and <a href="https://www.recurse.com/manual">user’s manual</a>. If the ideas there resonate with
you, I can warmly encourage you to apply.</p>

<p>If you’re at a company that hires software engineers, you should totally work with Recurse Center to
recruit Recursers. They are people who are intellectually curious, approach their work with joy, and
treat others with respect. The companies I have spoken to which recruit through RC have been very
happy with the people they hired.</p>

<p>I personally spent only two weeks at RC. I would have loved to stay for longer, but (a) my partner
can’t get three months off her job to stay in New York, and (b) I’ve just started a new dream job,
in which I will continue to have great freedom to explore, learn and teach (more on that in a future
blog post).</p>

<p>The thing that impressed me most about the Recurse Center is how they have succeeded in creating
a respectful, thoughtful, encouraging atmosphere that is welcoming to people from a very diverse set
of backgrounds. The tech industry would be vastly improved if more people would learn from RC’s
example. I, for my part, will try to carry that spirit of RC with me.</p>

                ]]></content:encoded>
            </item>
        
            <item>
                <title>Kafka, Samza, and the Unix philosophy of distributed data</title>
                <link>http://martin.kleppmann.com/2015/08/05/kafka-samza-unix-philosophy-distributed-data.html</link>
                <comments>http://martin.kleppmann.com/2015/08/05/kafka-samza-unix-philosophy-distributed-data.html#disqus_thread</comments>
                <pubDate>Wed, 05 Aug 2015 00:00:00 +0000</pubDate>
                <dc:creator>Martin Kleppmann</dc:creator>
                
                    <guid isPermaLink="true">http://martin.kleppmann.com/2015/08/05/kafka-samza-unix-philosophy-distributed-data.html</guid>
                
                <description><![CDATA[ This is an edited transcript of a talk I gave at the UK Hadoop Users Group. Video and slides are also available. This transcript was originally published on the Confluent blog. One of the things I realised while doing research for my book is that contemporary software engineering still has... ]]></description>
                <content:encoded><![CDATA[
                    <p><em>This is an edited transcript of a <a href="/2015/08/05/samza-unix-philosophy-at-huguk.html">talk</a> I gave at the
<a href="http://www.meetup.com/hadoop-users-group-uk/events/223836730/">UK Hadoop Users Group</a>.
<a href="https://www.youtube.com/watch?v=Gqdr0DiNh5g&amp;index=1&amp;list=PL5OOLwV_m9vaKzwGX7lM8oVT3aFw_CN5O">Video</a> and
<a href="https://speakerdeck.com/ept/kafka-samza-and-the-unix-philosophy-of-distributed-data">slides</a>
are also available. This transcript was originally published on the
<a href="http://www.confluent.io/blog/apache-kafka-samza-and-the-unix-philosophy-of-distributed-data">Confluent blog</a>.</em></p>

<p>One of the things I realised while doing research for <a href="http://dataintensive.net/">my book</a> is that
contemporary software engineering still has a lot to learn from the 1970s. As we’re in such
a fast-moving field, we often have a tendency of dismissing older ideas as irrelevant – and
consequently, we end up having to learn the same lessons over and over again, the hard way.
Although computers have got faster, data has got bigger and requirements have become more complex,
many old ideas are actually still highly relevant today.</p>

<p>In this blog post I’d like to highlight one particular set of old ideas that I think deserves more
attention today: the Unix philosophy. I’ll show how this philosophy is very different from the
design approach of mainstream databases, and explore what it would look like if modern distributed
data systems learnt a thing or two from Unix.</p>

<p><img src="/2015/08/unixphil-01.png" width="550" height="412" /></p>

<p>In particular, I’m going to argue that there are a lot of similarities between Unix pipes and Apache
Kafka, and that this similarity enables good architectural styles for large-scale applications. But
before we get into that, let me remind you of the foundations of the Unix philosophy. You’ve
probably seen the power of Unix tools before – but to get started, let me give you a concrete
example that we can talk about.</p>

<p>Say you have a web server that writes an entry to a log file every time it serves a request. For
example, using the nginx default access log format, one line of the log might look like this:</p>

<pre><code>216.58.210.78 - - [27/Feb/2015:17:55:11 +0000] "GET /css/typography.css HTTP/1.1"
200 3377 "http://martin.kleppmann.com/" "Mozilla/5.0 (Macintosh; Intel Mac OS X
10_9_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/40.0.2214.115 Safari/537.36"
</code></pre>

<p>(That is actually one line, it’s only broken up into multiple lines here for readability.) This line
of the log indicates that on 27 February 2015 at 17:55:11 UTC, the server received a request for the
file <code>/css/typography.css</code> from the client IP address 216.58.210.78. It then goes on to note various
other details, including the browser’s user-agent string.</p>

<p>Various tools can take these log files and produce pretty reports about your website traffic, but
for the sake of the exercise, let’s build our own, using basic Unix tools. Let’s determine the
5 most popular URLs on our website. To start with, we need to extract the path of the URL that was
requested, for which we can use <code>awk</code>.</p>

<p><code>awk</code> doesn’t know about the format of nginx logs – it just treats the log file as text. By
default, <code>awk</code> takes one line of input at a time, splits it by whitespace, and makes the
whitespace-separated components available as variables <code>$1</code>, <code>$2</code>, etc. In the nginx log example,
the requested URL path is the seventh whitespace-separated component:</p>

<p><img src="/2015/08/unixphil-03.png" width="550" height="412" /></p>

<p>Now that you’ve extracted the path, you can determine the 5 most popular pages on your website as
follows:</p>

<div class="highlight"><pre><code class="language-bash" data-lang="bash">awk <span class="s1">&#39;{print $7}&#39;</span> access.log <span class="p">|</span> <span class="c"># Split by whitespace, 7th field is request path</span>
    sort     <span class="p">|</span> <span class="c"># Make occurrences of the same URL appear consecutively in file</span>
    uniq -c  <span class="p">|</span> <span class="c"># Replace consecutive occurrences of the same URL with a count</span>
    sort -rn <span class="p">|</span> <span class="c"># Sort by number of occurrences, descending</span>
    head -n <span class="m">5</span>  <span class="c"># Output top 5 URLs</span></code></pre></div>

<p>The output of that series of commands looks something like this:</p>

<pre><code>4189 /favicon.ico
3631 /2013/05/24/improving-security-of-ssh-private-keys.html
2124 /2012/12/05/schema-evolution-in-avro-protocol-buffers-thrift.html
1369 /
 915 /css/typography.css
</code></pre>

<p><img src="/2015/08/unixphil-04.png" width="550" height="412" /></p>

<p>Although the above command line looks a bit obscure if you’re unfamiliar with Unix tools, it is
incredibly powerful. It will process gigabytes of log files in a matter of seconds, and you can
easily modify the analysis to suit your needs. For example, if you want to count top client IP
addresses instead of top pages, change the <code>awk</code> argument to <code>'{print $1}'</code>.</p>

<p>Many data analyses can be done in a few minutes using some combination of <code>awk</code>, <code>sed</code>, <code>grep</code>,
<code>sort</code>, <code>uniq</code> and <code>xargs</code>, and they perform
<a href="http://aadrake.com/command-line-tools-can-be-235x-faster-than-your-hadoop-cluster.html">surprisingly well</a>.
This is no coincidence: it is a direct result of the design philosophy of Unix.</p>

<p><img src="/2015/08/unixphil-05.png" width="550" height="412" /></p>

<p>The Unix philosophy is a set of principles that emerged gradually during the design and
implementation of Unix systems during the late 1960s and ’70s. There are various interpretations of
the Unix philosophy, but two points that particularly stand out were
<a href="https://archive.org/details/bstj57-6-1899">described</a> by Doug McIlroy, Elliot Pinson and Berk Tague
as follows in 1978:</p>

<ul>
  <li>Make each program do one thing well. To do a new job, build afresh rather than complicate old
programs by adding new “<a href="http://harmful.cat-v.org/cat-v/unix_prog_design.pdf">features</a>.”</li>
  <li>Expect the output of every program to become the input to another, as yet unknown, program.</li>
</ul>

<p>These principles are the foundation for chaining together programs into pipelines that can
accomplish complex processing tasks. The key idea here is that a program does not know or care
where its input is coming from, or where its output is going to: it may be a file, or another
program that’s part of the operating system, or another program written by someone else entirely.</p>

<p><img src="/2015/08/unixphil-06.png" width="550" height="165" /></p>

<p>The tools that come with the operating system are generic, but they are designed such that they can
be <em>composed</em> together into larger programs that can perform application-specific tasks.</p>

<p>The benefits that the designers of Unix derived from this design approach sound quite like the ideas
of the Agile and DevOps movements that appeared decades later: scripting and automation, rapid
prototyping, incremental iteration, being friendly to experimentation, and breaking down large
projects into manageable chunks. Plus ça change.</p>

<p><img src="/2015/08/unixphil-08.png" width="550" height="322" /></p>

<p>When you join two commands with the pipe character in your shell, the shell starts both programs at
the same time, and attaches the output of the first process to the second process’ input. This
attachment mechanism uses the <a href="http://linux.die.net/man/2/pipe"><code>pipe</code></a> syscall provided by the
operating system.</p>

<p>Note that this wiring is not done by the programs themselves, but by the shell – this allows them
to be <a href="http://en.wikipedia.org/wiki/Loose_coupling">loosely coupled</a>, and not worry about where
their input is coming from, or where their output is going.</p>

<p><img src="/2015/08/unixphil-09.png" width="550" height="412" /></p>

<p>The pipe had been invented in 1964 by Doug McIlroy, who first
<a href="http://doc.cat-v.org/unix/pipes/">described</a> it like this in an internal Bell Labs memo: “We should
have some ways of connecting programs like [a] garden hose – screw in another segment when it
becomes necessary to massage data in another way.” Dennis Richie later
<a href="http://cm.bell-labs.co/who/dmr/mdmpipe.html">wrote up his perspective</a> on the memo.</p>

<p><img src="/2015/08/unixphil-10.png" width="550" height="233" /></p>

<p>They also realised early that the inter-process communication mechanism (pipes) can look very
similar to the mechanism for reading and writing files. We now call this input redirection (using
the contents of a file as input to a process) and output redirection (writing the output of
a process to a file).</p>

<p>The reason that Unix programs can be composed so flexibly is that they all conform to the same
interface: most programs have one stream for input data (<code>stdin</code>) and two output streams (<code>stdout</code>
for regular output data, and <code>stderr</code> for errors and diagnostic messages).</p>

<p><img src="/2015/08/unixphil-12.png" width="550" height="311" /></p>

<p>Programs may also do other things besides reading <code>stdin</code> and writing <code>stdout</code>, such as reading and
writing files, communicating over the network, or drawing a graphical user interface. However, the
<code>stdin</code>/<code>stdout</code> communication is considered to be the main way how data flows from one Unix tool to
another.</p>

<p>And the great thing about the <code>stdin</code>/<code>stdout</code> interface is that anyone can implement it easily, in
any programming language. You can develop your own tool that conforms to this interface, and it will
play nicely with all the standard tools that ship as part of the operating system.</p>

<p><img src="/2015/08/unixphil-13.png" width="550" height="412" /></p>

<p>For example, when analysing a web server log file, perhaps you want to find out how many visitors
you have from each country. The log doesn’t tell you the country, but it does tell you the IP
address, which you can translate into a country using an IP geolocation database. Such a database
isn’t included with your operating system by default, but you can write your own tool that takes
IP addresses on <code>stdin</code> and outputs country codes on <code>stdout</code>.</p>

<p>Once you’ve written that tool, you can include it in the data processing pipeline we discussed
previously, and it will work just fine. This may seem painfully obvious if you’ve been working with
Unix for a while, but I’d like to emphasise how remarkable this is: your own code runs on equal
terms with the tools provided by the operating system.</p>

<p>Apps with graphical user interfaces or web apps cannot simply be extended and wired together like
this. You can’t just pipe Gmail into a separate search engine app, and post results to a wiki.
Today it’s an exception, not the norm, to have programs that work together as smoothly as Unix tools
do.</p>

<p><img src="/2015/08/unixphil-14.png" width="550" height="285" /></p>

<p>Change of scene. Around the same time as Unix was being developed, the
<a href="http://people.csail.mit.edu/tdanford/6830papers/codd-relational-model.pdf">relational data model</a>
was proposed, which in time became SQL, and was implemented in many popular databases. Many
databases actually run on Unix systems. Does that mean they also follow the Unix philosophy?</p>

<p><img src="/2015/08/unixphil-15.png" width="550" height="354" /></p>

<p>The dataflow in most database systems is very different from Unix tools. Rather than using <code>stdin</code>
and <code>stdout</code> as communication channels, there is a <em>database server</em>, and several <em>clients</em>. The
clients send queries to read or write data on the server, the server handles the queries and sends
responses to the clients. This relationship is fundamentally asymmetric: clients and servers are
distinct roles.</p>

<p><img src="/2015/08/unixphil-16.png" width="550" height="412" /></p>

<p>What about the composability and extensibility that we find in Unix systems? Clients can do anything
they like (since they are application code), but database servers are mostly in the business of
storing and retrieving your data. Letting you run arbitrary code is not their top priority.</p>

<p>That said, many databases do provide some ways of extending the database server with your own code.
For example, many relational databases let you write stored procedures in their own, rudimentary
procedural language such as <a href="http://www.oracle.com/technetwork/database/features/plsql/index.html">PL/SQL</a>
(and some let you run code in a general-purpose programming language such as
<a href="https://blog.heroku.com/archives/2013/6/5/javascript_in_your_postgres">JavaScript</a>). However, the
things you can do in stored procedures are limited.</p>

<p>Other extension points in some databases are support for custom data types (this was one of the
early <a href="http://db.cs.berkeley.edu/papers/ERL-M85-95.pdf">design goals of Postgres</a>), or pluggable
storage engines. Essentially, these are plugin APIs: you can run your code in the database server,
provided that your module adheres to a plugin API exposed by the database server for a particular
purpose.</p>

<p>This kind of extensibility is not the same as the arbitrary composability we saw with Unix tools.
The plugin API is totally controlled by the database server, and subordinate to it. Your extension
code is a guest in the database server’s home, not an equal partner.</p>

<p><img src="/2015/08/unixphil-17.png" width="550" height="338" /></p>

<p>A consequence of this design is that you can’t just pipe one database into another, even if they
have the same data model. Nor can you insert your own code into the database’s internal processing
pipelines (unless the server has specifically provided an extension point for you, such as
triggers).</p>

<p>I feel the design of databases is very self-centered. A database seems to assume that it’s the
centre of your universe: the only place where you might want to store and query your data, the
source of truth, and the destination for all queries. The closest you can get to piping data in and
out of it is through bulk-loading and bulk-dumping (backup) operations, but those operations don’t
really use any of the database’s features, such as query planning and indexes.</p>

<p>If a database was designed according to the Unix philosophy, it would be based on a small number of
core primitives that you could easily combine, extend and replace at will. Instead, databases are
tremendously complicated, monolithic beasts. While Unix acknowledges that the operating system will
never do everything you might want, and thus encourages you to extend it, databases try to implement
all the features you may need in a single program.</p>

<p><img src="/2015/08/unixphil-18.png" width="550" height="412" /></p>

<p>Perhaps that design is fine in simple applications where a single database is indeed sufficient.
However, many complex applications find that they have to use their data in
<a href="http://www.confluent.io/blog/using-logs-to-build-a-solid-data-infrastructure-or-why-dual-writes-are-a-bad-idea/">various different ways</a>:
they need fast random access for OLTP, big sequential scans for analytics, inverted indexes for
full-text search, graph indexes for connected data, machine learning systems for recommendation
engines, a push mechanism for notifications, various different cached representations of the data
for fast reads, and so on.</p>

<p>A general-purpose database may try to do all of those things in one product
(<a href="https://cs.brown.edu/~ugur/fits_all.pdf">“one size fits all”</a>), but in all likelihood it will not
perform as well as a tool that is specialized for one particular purpose. In practice, you can often
get the best results by combining various different data storage and indexing systems: for example,
you may take the same data and store it in a relational database for random access, in Elasticsearch
for full-text search, in a columnar format in Hadoop for analytics, and cached in a denormalized
form in memcached.</p>

<p>When you need to integrate different databases, the lack of Unix-style composability is a severe
limitation. (I’ve
<a href="http://www.confluent.io/blog/bottled-water-real-time-integration-of-postgresql-and-kafka/">done some work</a>
on piping data out of Postgres into other applications, but there’s still a long way to go before we
can simply pipe any database into any other database.)</p>

<p><img src="/2015/08/unixphil-19.png" width="550" height="412" /></p>

<p>We said that Unix tools are composable because they all implement the same interface of <code>stdin</code>,
<code>stdout</code> and <code>stderr</code> – and each of these is a <em>file descriptor</em>, i.e. a stream of bytes that you
can read or write like a file. This interface is simple enough that anyone can easily implement it,
but it is also powerful enough that you can use it for anything.</p>

<p>Because all Unix tools implement the same interface, we call it a <em>uniform interface</em>. That’s why
you can pipe the output of <code>gunzip</code> to <code>wc</code> without a second thought, even though those two tools
appear to have nothing in common. It’s like lego bricks, which all implement the same pattern of
knobbly bits and grooves, allowing you to stack any lego brick on any other, regardless of their
shape, size or colour.</p>

<p><img src="/2015/08/unixphil-20.png" width="550" height="412" /></p>

<p>The uniform interface of file descriptors in Unix doesn’t just apply to the input and output of
processes, but it’s a very broadly applied pattern. If you open a file on the filesystem, you get
a file descriptor. Pipes and unix sockets provide file descriptors that are a communication channel
to another process on the same machine. On Linux, the virtual files in <code>/dev</code> are the interfaces of
device drivers, so you might be talking to a USB port or even a GPU. The virtual files in <code>/proc</code>
are an API for the kernel, but since they’re exposed as files, you can access them with the same
tools as regular files.</p>

<p>Even a TCP connection to a process on another machine is a file descriptor, although the BSD sockets
API (which is most commonly used to establish TCP connections) is arguably not as Unixy as it could
be. <a href="http://www.catb.org/esr/writings/taoup/html/plan9.html">Plan 9</a> shows that even the network
could have been cleanly integrated into the same uniform interface.</p>

<p>To a first approximation, everything on Unix is a file. This uniformity means the logic of Unix
tools is separated from the wiring, making it more composable. <code>sed</code> doesn’t need to care whether
it’s talking to a pipe to another process, or a socket, or a device driver, or a real file on the
filesystem. It’s all the same.</p>

<p><img src="/2015/08/unixphil-21.png" width="550" height="412" /></p>

<p>A file is a <em>stream of bytes</em>, perhaps with an end-of-file (EOF) marker at some point, indicating
that the stream has ended (a stream can be of arbitrary length, and a process may not know in
advance how long its input is going to be).</p>

<p>A few tools (e.g. <code>gzip</code>) operate purely on byte streams, and don’t care about the structure of the
data. But most tools need to parse their input in order to do anything useful with it. For this,
most Unix tools use ASCII, with each record on one line, and fields separated by tabs or spaces, or
maybe commas.</p>

<p>Files are totally obvious to us today, which shows that a byte stream turned out to be a good
uniform interface. However, the implementors of Unix could have decided to do it very differently.
For example, it could have been a function callback interface, using a schema to pass records from
process to process. Or it could have been shared memory (like
<a href="http://www.tldp.org/LDP/lpg/node21.html">System V IPC</a> or
<a href="http://man7.org/linux/man-pages/man2/mmap.2.html">mmap</a>, which came along later). Or it could have
been a <em>bit</em> stream rather than a byte stream.</p>

<p>In a sense, a byte stream is a lowest common denominator – the simplest possible interface.
Everything can be expressed in terms of a stream of bytes, and it’s fairly agnostic to the transport
medium (pipe from another process, file on disk, TCP connection, tape, etc). But this is also a
disadvantage, as we shall discuss later.</p>

<p><img src="/2015/08/unixphil-22.png" width="550" height="412" /></p>

<p>We’ve seen that Unix developed some very good design principles for software development, and that
databases have taken a very different route. I would love to see a future in which we can learn from
both paths of development, and combine the best ideas from each.</p>

<p>How can we make 21st-century data systems better by learning from the Unix philosophy? In the rest
of this post I’d like to explore what it might look like if we bring the Unix philosophy to the
world of databases.</p>

<p><img src="/2015/08/unixphil-23.png" width="550" height="412" /></p>

<p>First, let’s acknowledge that Unix is not perfect. Although I think the simple, uniform interface of
byte streams was very successful at enabling an ecosystem of flexible, composable, powerful tools,
Unix has some limitations:</p>

<ul>
  <li>It’s designed for use on a single machine. As our applications get every more data and traffic,
and have higher uptime requirements, moving to distributed systems is becoming
<a href="http://queue.acm.org/detail.cfm?id=2482856">increasingly inevitable</a>. Although a TCP connection
can be made to look somewhat like a file, I don’t think that’s the right answer: it only works if
both sides of the connection are up, and it has
<a href="http://blog.netherlabs.nl/articles/2009/01/18/the-ultimate-so_linger-page-or-why-is-my-tcp-not-reliable">somewhat messy</a>
edge case semantics. TCP is good, but by itself it’s too low-level to serve as a distributed pipe
implementation.</li>
  <li>A Unix pipe is designed to have a single sender process, and a single recipient. You can’t use
pipes to send output to several processes, or to collect input from several processes. (You can
branch a pipeline with <a href="http://linux.die.net/man/1/tee"><code>tee</code></a>, but a pipe itself is always
one-to-one.)</li>
  <li>ASCII text (or rather, UTF-8) is great for making data easily explorable, but it quickly gets
messy. Every process needs to be set up with its own input parsing: first breaking the byte stream
into records (usually separated by newline, though some advocate <code>0x1e</code>, the <a href="https://ronaldduncan.wordpress.com/2009/10/31/text-file-formats-ascii-delimited-text-not-csv-or-tab-delimited-text/">ASCII record
separator</a>).
Then a record needs to be broken up into fields, like the <code>$7</code> in the <code>awk</code> example at the
beginning. Separator characters that appear in the data need to be escaped somehow. Even
a fairly simple tool like <a href="http://unixhelp.ed.ac.uk/CGI/man-cgi?xargs"><code>xargs</code></a> has about half
a dozen command-line options to specify how its input should be parsed. Text-based interfaces work
tolerably well, but in retrospect, I am pretty sure that a richer data model with
<a href="http://radar.oreilly.com/2014/11/the-problem-of-managing-schemas.html">explicit schemas</a>
would have worked better.</li>
  <li>Unix processes are generally assumed to be fairly short-running. For example, if a process
in the middle of a pipeline crashes, there is no way for it to resume processing from its input
pipe – the entire pipeline fails and must be re-run from scratch. That’s no problem if the
commands run only for a few seconds, but if an application is expected to run continuously for
years, better fault tolerance is needed.</li>
</ul>

<p>I think we can find a solution that overcomes these downsides, while retaining the Unix philosophy’s
benefits.</p>

<p><img src="/2015/08/unixphil-24.png" width="550" height="412" /></p>

<p>The cool thing is that this solution already exists, and is implemented in
<a href="http://kafka.apache.org/">Kafka</a> and <a href="http://samza.apache.org/">Samza</a>, two open source projects
that work together to provide distributed stream processing.</p>

<p>As you probably already know from
<a href="http://www.confluent.io/blog/stream-data-platform-1/">other</a>
<a href="http://www.confluent.io/blog/using-logs-to-build-a-solid-data-infrastructure-or-why-dual-writes-are-a-bad-idea/">posts</a>
on this blog, Kafka is a scalable distributed message broker, and Samza is a framework that lets you
write code to consume and produce data streams.</p>

<p><img src="/2015/08/unixphil-25.png" width="550" height="412" /></p>

<p>In fact, when you look at it through the Unix lens, Kafka looks quite like the pipe that connects
the output of one process to the input of another. And Samza looks quite like a standard library
that helps you read <code>stdin</code> and write <code>stdout</code> (and a few helpful additions, such as a deployment
mechanism,
<a href="http://samza.apache.org/learn/documentation/0.9/container/state-management.html">state management</a>,
metrics, and monitoring).</p>

<p>The style of stream processing jobs that you can write with Kafka and Samza closely follows the Unix
tradition of small, composable tools:</p>

<ul>
  <li>In Unix, the operating system kernel provides the pipe, a transport mechanism for getting a stream
of bytes from one process to another.</li>
  <li>In stream processing, Kafka provides publish-subscribe streams, a transport mechanism for getting
messages from one stream processing job to another.</li>
</ul>

<p><img src="/2015/08/unixphil-26.png" width="550" height="412" /></p>

<p>Kafka addresses the downsides of Unix pipes that we discussed previously:</p>

<ul>
  <li>The single-machine limitation is lifted: Kafka itself is distributed by default, and any stream
processors that use it can also be distributed across multiple machines.</li>
  <li>A Unix pipe connects exactly one process output with exactly one process input, whereas a stream
in Kafka can have many producers and many consumers. Many inputs is important for services that
are distributed across multiple machines, and many outputs makes Kafka more like a broadcast
channel. This is very useful, since it allows the same data stream to be consumed independently
for several different purposes (including monitoring and audit purposes, which are often outside
of the application itself). Kafka consumers can come and go without affecting other consumers.</li>
  <li>Kafka also provides good fault tolerance: data is replicated across multiple Kafka nodes, so if
one node fails, another node can automatically take over. If a stream processor node fails and is
restarted, it can resume processing at its last checkpoint.</li>
  <li>Rather than a stream of bytes, Kafka provides a stream of messages, which saves the first step of
input parsing (breaking the stream of bytes into a sequence of records). Each message is just an
array of bytes, so you can use your
<a href="http://martin.kleppmann.com/2012/12/05/schema-evolution-in-avro-protocol-buffers-thrift.html">favourite serialisation format</a>
for individual messages: JSON, XML, Avro, Thrift or Protocol Buffers are all reasonable choices.
It’s well worth <a href="http://www.confluent.io/blog/stream-data-platform-2/">standardising</a> on one
encoding, and Confluent provides particularly good
<a href="http://docs.confluent.io/1.0/schema-registry/docs/intro.html">schema management support for Avro</a>.
This allows applications to work with objects that have meaningful field names, and not have to
worry about input parsing or output escaping. It also provides good support for schema evolution
without breaking compatibility.</li>
</ul>

<p><img src="/2015/08/unixphil-27.png" width="550" height="412" /></p>

<p>There are a few more things that Kafka does differently from Unix pipes, which are worth calling out
briefly:</p>

<ul>
  <li>As mentioned, Unix pipes provide a byte stream, whereas Kafka provides a stream of messages. This
is especially important if several processes are concurrently writing to the same stream: in
a byte stream, the bytes from different writers can be interleaved, leading to an unparseable
mess. Since messages are coarser-grained and self-contained, they can be safely interleaved,
making it safe for multiple processes to concurrently write to the same stream.</li>
  <li>Unix pipes are just a small in-memory buffer, whereas Kafka durably writes all messages to disk.
In this regard, Kafka is less like a pipe, and more like one process writing to a temporary file,
while several other processes continuously read that file using <code>tail -f</code> (each consumer tails the
file independently). Kafka’s approach provides better fault tolerance, since it allows a consumer
to fail and restart without skipping messages. Kafka automatically splits those ‘temporary’ files
into segments and garbage-collects old segments on a configurable schedule.</li>
  <li>In Unix, if the consuming process of a pipe is slow to read the data, the buffer fills up and the
sending process is blocked from writing to the pipe. This is a kind of backpressure. In Kafka, the
producer and consumer are more decoupled: a slow consumer has its input buffered, so it doesn’t
slow down the producer or other consumers. As long as the buffer fits within Kafka’s available
disk space, the slow consumer can catch up later. This makes the system less sensitive to
individual slow components, and more robust overall.</li>
  <li>A data stream in Kafka is called a <em>topic</em>, and you can refer to it by name (which makes it more
like a Unix <a href="http://vincebuffalo.com/2013/08/08/the-mighty-named-pipe.html">named pipe</a>.
A pipeline of Unix programs is usually started all at once, so the pipes normally don’t need
explicit names. On the other hand, a long-running application usually has bits added, removed or
replaced gradually over time, so you need names in order to tell the system what you want to
connect to. Naming also helps with discovery and management.</li>
</ul>

<p>Despite those differences, I still think it makes sense to think of Kafka as Unix pipes for
distributed data. For example, one thing they have in common is that Kafka keeps messages in a fixed
order (like Unix pipes, which keep the byte stream in a fixed order). This is a
<a href="http://www.confluent.io/blog/using-logs-to-build-a-solid-data-infrastructure-or-why-dual-writes-are-a-bad-idea/">very useful property</a>
for event log data: the order in which things happened is often meaningful and needs to be preserved.
Other types of message broker, like AMQP and JMS, do not have this ordering property.</p>

<p><img src="/2015/08/unixphil-28.png" width="550" height="412" /></p>

<p>So we’ve got Unix tools and stream processors that look quite similar. Both read some input stream,
modify or transform it in some way, and produce an output stream that is somehow derived from the
input.</p>

<p>Importantly, the processing does not modify the input itself: it remains immutable. If you run <code>awk</code>
on some input file, the file remains unmodified (unless you explicitly choose to overwrite it).
Also, most Unix tools are <em>deterministic</em>, i.e. if you give them the same input, they always produce
the same output. This means you can re-run the same command as many times as you want, and gradually
iterate your way towards a working program. It’s great for experimentation, because you can always
go back to your original data if you mess up the processing.</p>

<p>This deterministic and side-effect-free processing looks a lot like functional programming. That
doesn’t mean you have to use a functional programming language like Haskell (although you’re welcome
to do so if you want), but you still get many of the benefits of functional code.</p>

<p><img src="/2015/08/unixphil-29.png" width="550" height="412" /></p>

<p>The Unix-like design principles of Kafka enable building composable systems at a large scale. In
a large organisation, different teams can each publish their data to Kafka. Each team can
independently develop and maintain stream processing jobs that consume streams and produce new
streams. Since a stream can have any number of independent consumers, no coordination is required to
set up a new consumer.</p>

<p>We’ve been calling this idea a <a href="http://www.confluent.io/blog/stream-data-platform-1/">stream data platform</a>.
In this kind of architecture, the data streams in Kafka act as the communication channel between
different teams’ systems. Each team focusses on making their particular part of the system do one
thing well. While Unix tools can be composed to accomplish a data processing task, distributed
streaming systems can be composed to comprise the
<a href="http://www.confluent.io/blog/stream-data-platform-1/">entire operation of a large organisation</a>.</p>

<p>A Unixy approach manages the complexity of a large system by encouraging loose coupling: thanks to
the uniform interface of streams, different components can be developed and deployed independently.
Thanks to the fault tolerance and buffering of the pipe (Kafka), when a problem occurs in one part
of the system, it remains localised. And
<a href="http://www.confluent.io/blog/stream-data-platform-2/">schema management</a> allows changes to data
structures to be made safely, so that each team can move fast without breaking things for other
teams.</p>

<p><img src="/2015/08/unixphil-30.png" width="550" height="412" /></p>

<p>To wrap up this post, let’s consider a real-life example of how this works at LinkedIn. As you may
know, companies can post their job openings on LinkedIn, and jobseekers can browse and apply for
those jobs. What happens if a LinkedIn member (user) views one of those job postings?</p>

<p>It’s very useful to know who has looked at which jobs, so the service that handles job views
publishes an event to Kafka, saying something like “member 123 viewed job 456 at time 789”. Now that
this information is in Kafka, it can be used for many
<a href="http://sites.computer.org/debull/A12june/A12JUN-CD.pdf">good purposes</a>:</p>

<ul>
  <li><strong>Monitoring systems</strong>: Companies pay LinkedIn to post their job openings, so it’s important that
the site is working correctly. If the rate of job views drops unexpectedly, alarms should go off,
because it indicates a problem that needs to be investigated.</li>
  <li><strong>Relevance and recommendations</strong>: It’s annoying for users to see the same thing over and over
again, so it’s good to track how many times the users has seen a job posting, and feed that into
the scoring process. Keeping track of who viewed what also allows for collaborative filtering
recommendations (people who viewed X also viewed Y).</li>
  <li><strong>Preventing abuse</strong>: LinkedIn doesn’t want people to be able to scrape all the jobs, submit
spam, or otherwise violate the terms of service. Knowing who is doing what is the first step
towards detecting and blocking abuse.</li>
  <li><strong>Job poster analytics</strong>: The companies who post their job openings want to see
<a href="https://engineering.linkedin.com/analytics/real-time-analytics-massive-scale-pinot">stats</a>
(in the style of Google Analytics) about who is viewing their postings, for example so that
they can test which wording attracts the best candidates.</li>
  <li><strong>Import into Hadoop and Data Warehouse</strong>: For LinkedIn’s internal business analytics, for senior
management’s dashboards, for crunching numbers that are reported to Wall Street, for evaluating
A/B tests, and so on.</li>
</ul>

<p>All of those systems are complex in their own right, and are maintained by different teams. Kafka
provides a fault-tolerant, scalable implementation of a pipe. A stream data platform based on Kafka
allows all of these various systems to be developed independently, and to be connected and composed
in a robust way.</p>

<p><em>If you enjoyed this post, you’ll love my book
<a href="http://dataintensive.net/">Designing Data-Intensive Applications</a>, published by
<a href="http://shop.oreilly.com/product/0636920032175.do">O’Reilly</a>.</em></p>

<p><em>Thank you to <a href="https://twitter.com/jaykreps">Jay Kreps</a>,
<a href="https://twitter.com/gwenshap">Gwen Shapira</a>,
<a href="http://www.michael-noll.com/">Michael Noll</a>,
<a href="http://www.ewencp.org/">Ewen Cheslack-Postava</a>,
<a href="https://www.linkedin.com/in/jasongustafson">Jason Gustafson</a>, and
<a href="https://twitter.com/jeff_hartley">Jeff Hartley</a>
for feedback on a draft of this post,
and thanks also to Jay for providing the LinkedIn job view example.</em></p>

                ]]></content:encoded>
            </item>
        
            <item>
                <title>Using logs to build a solid data infrastructure (or: why dual writes are a bad idea)</title>
                <link>http://martin.kleppmann.com/2015/05/27/logs-for-data-infrastructure.html</link>
                <comments>http://martin.kleppmann.com/2015/05/27/logs-for-data-infrastructure.html#disqus_thread</comments>
                <pubDate>Wed, 27 May 2015 00:00:00 +0000</pubDate>
                <dc:creator>Martin Kleppmann</dc:creator>
                
                    <guid isPermaLink="true">http://martin.kleppmann.com/2015/05/27/logs-for-data-infrastructure.html</guid>
                
                <description><![CDATA[ This is an edited transcript of a talk I gave at the Craft Conference 2015. The video and slides are also available. This transcript was originally published on the Confluent blog. It has also been translated into Japanese (part 1, part 2). How does your database store data on disk... ]]></description>
                <content:encoded><![CDATA[
                    <p><em>This is an edited transcript of a
<a href="https://martin.kleppmann.com/2015/04/24/logs-for-data-infrastructure-at-craft.html">talk</a> I gave
at the <a href="http://craft-conf.com/2015">Craft Conference 2015</a>. The
<a href="http://www.ustream.tv/recorded/61479591/theater">video</a> and
<a href="https://speakerdeck.com/ept/using-logs-to-create-a-solid-data-infrastructure">slides</a> are also
available. This transcript was originally published <a href="http://blog.confluent.io/2015/05/27/using-logs-to-build-a-solid-data-infrastructure-or-why-dual-writes-are-a-bad-idea/">on the Confluent
blog</a>.
It has also been translated into Japanese
(<a href="http://postd.cc/using-logs-to-build-a-solid-data-infrastructure-part-1/">part 1</a>,
<a href="http://postd.cc/using-logs-to-build-a-solid-data-infrastructure-or-why-dual-writes-are-a-bad-idea-part-2/">part 2</a>).</em></p>

<iframe width="550" height="346" src="http://www.ustream.tv/embed/recorded/61479591?v=3&amp;wmode=direct" scrolling="no" frameborder="0" style="border: 0px none transparent;"></iframe>

<p><em>How does your database store data on disk reliably? It uses a log.<br />
How does one database replica synchronise with another replica? It uses a log.<br />
How does a distributed algorithm like <a href="https://ramcloud.stanford.edu/raft.pdf">Raft</a> achieve consensus?
It uses a log. <br />
How does activity data get recorded in a system like <a href="http://kafka.apache.org/">Apache Kafka</a>? It uses a log.<br />
How will the data infrastructure of your application remain robust at scale? Guess what…</em></p>

<p><em>Logs are everywhere. I’m not talking about plain-text log files (such as syslog or log4j) – I mean
an append-only, totally ordered sequence of records. It’s a very simple structure, but it’s also
a bit strange at first if you’re used to normal databases. However, once you learn to think in terms
of logs, many problems of making large-scale data systems reliable, scalable and maintainable
suddenly become much more tractable.</em></p>

<p><em>Drawing from the experience of building scalable systems at LinkedIn and other startups, this talk
explores why logs are such a fine idea: making it easier to maintain search indexes and caches,
making your applications more scalable and more robust in the face of failures, and opening up your
data for richer analysis, while avoiding race conditions, inconsistencies and other ugly problems.</em></p>

<p><img src="/2015/05/logs-01.png" alt="Using logs to create a solid data infrastructure" width="550" height="413" /></p>

<p>Hello! I’m <a href="https://martin.kleppmann.com/">Martin Kleppmann</a>, and I work on large-scale data systems,
especially the kinds of systems that you find at internet companies. I used to work at LinkedIn,
contributing to an open source stream processing system called <a href="http://samza.apache.org/">Samza</a>.</p>

<p>In the course of that work, my colleagues and I learnt a thing or two about how to build applications
such that they are operationally robust, reliable and perform well. In particular, I got to work with
some fine people like <a href="https://twitter.com/jaykreps">Jay Kreps</a>, <a href="https://twitter.com/criccomini">Chris
Riccomini</a> and <a href="https://twitter.com/nehanarkhede">Neha Narkhede</a>.
They figured out a particular architectural style for applications, based on logs, that turns out
to work really well. In this talk I will describe that approach, and show how similar patterns
arise in various different areas of computing.</p>

<p>What I’m going to talk about today isn’t really new — some people have known about these ideas for
a long time. However, they aren’t as widely known as they should be. If you work on a non-trivial
application, something with more than just one database, you’ll probably find these ideas very
useful.</p>

<p><a href="http://dataintensive.net/"><img src="/2015/05/logs-02.png" alt="Designing Data-Intensive Applications" width="550" height="308" /></a></p>

<p>At the moment, I’m taking a sabbatical to write a book for O’Reilly, called
“<a href="http://dataintensive.net/">Designing Data-Intensive Applications</a>”. This book is an attempt to
collect the important fundamental lessons we’ve learnt about data systems in the last few
decades, covering the architecture of databases, caches, indexes, batch processing and stream
processing.</p>

<p>The book is not about any one particular database or tool – it’s about the whole range of
different tools and algorithms that are used in practice, and their trade-offs, their pros
and cons. This talk is partly based on my research for the book, so if you find it
interesting, you can find more detail and background in the book. The first seven chapters
are currently <a href="http://dataintensive.net/">available in early release</a>.</p>

<p><img src="/2015/05/logs-03.png" alt="Stereotypical three-tier architecture" width="550" height="413" /></p>

<p>Anyway, let’s get going. Let’s assume that you’re working on a web application. In the
simplest case, it probably has the stereotypical three-tier architecture: you have some
clients (which may be web browsers, or mobile apps, or both), which make requests to a web
application running on your servers. The web application is where your application code or
“business logic” lives.</p>

<p>Whenever the application wants to remember something for the future, it stores it in a database.
And whenever the application wants to look up something that it stored previously, it queries
the database. This approach is simple to understand and works pretty well.</p>

<p><img src="/2015/05/logs-04.png" alt="Web app with DB, cache, search, graph index, message queue and workers" width="550" height="413" /></p>

<p>However, things usually don’t stay so simple for long. Perhaps you get more users, making more
requests, your database gets slow, and you add a cache to speed it up – perhaps memcached or Redis,
for example. Perhaps you need to add full-text search to your application, and the basic search
facility built into your database is not good enough, so you end up setting a separate indexing
service such as Elasticsearch or Solr. </p>

<p>Perhaps you need to do some graph operations that are not efficient on a relational or document
database, for example for social features or recommendations, so you add a separate graph index
to your system. Perhaps you need to move some expensive operations out of the web request flow,
and into an asynchronous background process, so you add a message queue which lets you send jobs
to your background workers.</p>

<p>And it gets worse…</p>

<p><img src="/2015/05/logs-05.png" alt="Web app with a horrendous mess of storage services" width="550" height="413" /></p>

<p>By now, other parts of the system are getting slow again, so you add another cache. More caches
always make things faster, right? But now you have a lot of systems and services, so you need
to add metrics and monitoring so that you can see whether they are actually working, and the
metrics system is another system in its own right. </p>

<p>Next, you want to send notifications, such as email or push notifications to your users, so you
chain a notification system off the side of the job queue for background workers, and it perhaps
needs some kind of database of its own to keep track of stuff. But now you’re generating a lot
of data that needs to be analysed, and you can’t have your business analysts running big
expensive queries on your main database, so you add Hadoop or a data warehouse, and load the
data from the database into it.</p>

<p>Now that your business analytics are working, you find that your search system is no longer
keeping up… but you realise that since you have all the data in HDFS anyway, you could
actually build your search indexes in Hadoop and push them out to the search servers, and the
system just keeps getting more and more complicated…</p>

<p>…and the result is complete and utter insanity.</p>

<p><img src="/2015/05/logs-06.png" alt="Insanity" width="550" height="413" /></p>

<p>How did we get into that state? How did we end up with such complexity, where everything is
calling everything else, and nobody understands what is going on?</p>

<p>It’s not that any particular decision we made along the way was bad. There is no one database
or tool that can do everything that our application requires – we use the best tool for the
job, and for an application with a variety of features that implies using a variety of tools.</p>

<p>Also, as a system grows, you need a way of decomposing it into smaller components in order
to keep it manageable. That’s what microservices are all about. But if your system becomes
a tangled mess of interdependent components, that’s not manageable either.</p>

<p><img src="/2015/05/logs-07.png" alt="Same data in different form" width="550" height="413" /></p>

<p>Simply having many different storage systems is not a problem in itself: if they were all
independent from each other, it wouldn’t be a big deal. The real trouble here is that many
of them end up containing the same data, or related data, but in different forms.</p>

<p>For example, the documents in your full-text indexes are typically also stored in a database,
because search indexes are not intended to be used as systems of record. The data in your
caches is a duplicate of data in some database (perhaps joined with other data, or rendered
into HTML, or something) – that’s the definition of a cache.</p>

<p>Also, denormalization is just another form of duplicating data, similar to caching – if some
value is too expensive to recompute on reads, you may store that value somewhere, but now you
need to also keep it up-to-date when the underlying data changes. Materialized aggregates,
such as the count, sum or average of a bunch of records (which you often get in metrics or
analytics systems) are again a form of redundant data.</p>

<p>I’m not saying that this duplication of data is bad – far from it. Caching, indexing and
other forms of redundant data are often essential for getting good performance on reads.
However, keeping the data in sync between all these various different representations and
storage systems becomes a real challenge.</p>

<p><img src="/2015/05/logs-08.png" alt="Our challenge: data integration" width="550" height="413" /></p>

<p>For lack of a better term I’m going to call this the problem of “data integration”. With
that I really just mean <em>“making sure that the data ends up in all the right places”</em>.
Whenever a piece of data changes in one place, it needs to change correspondingly in all
the other places where there is a copy or derivative of that data.</p>

<p>So how do we keep these different data systems in sync? There are a few different techniques.</p>

<p>A popular approach is so-called <em>dual writes</em>:</p>

<p><img src="/2015/05/logs-09.png" alt="Dual writes" width="550" height="356" /></p>

<p>Dual writes is simple: it’s your application code’s responsibility to update data in all the
right places. For example, if a user submits some data to your web app, there’s some code
in the web app that first writes the data to your database, then invalidates or refreshes
the appropriate cache entries, then re-indexes the document in your full-text search index,
and so on. (Or maybe it does those things in parallel – doesn’t matter for our purposes.)</p>

<p>The dual writes approach is popular because it’s easy to build, and it more or less works at
first. But I’d like to argue that it’s a really bad idea, because it has some fundamental
problems. The first problem is race conditions.</p>

<p>The following diagram shows two clients making dual writes to two datastores. Time flows
from left to right, following the black arrows:</p>

<p><img src="/2015/05/logs-10.png" alt="Race condition with dual writes" width="550" height="413" /></p>

<p>Here, the first client (teal) is setting the key X to be some value A. They first make a
request to the first datastore – perhaps that’s the database, for example – and set X=A.
The datastore responds saying the write was successful. Then the client makes a request to
the second datastore – perhaps that’s the search index – and also sets X=A.</p>

<p>At the same time as this is happening, another client (red) is also active. It wants to
write to the same key X, but it wants to set the key to a different value B. The client
proceeds in the same way: it first sends a request X=B to the first datastore, and then
sends a request X=B to the second datastore.</p>

<p>All these writes are successful. However, look at what value is stored in each database over
time:</p>

<p><img src="/2015/05/logs-11.png" alt="Race condition with dual writes" width="550" height="413" /></p>

<p>In the first datastore, the value is first set to A by the teal client, and then set to B
by the red client, so the final value is B.</p>

<p>In the second datastore, the requests arrive in a different order: the value is first set
to B, and then set to A, so the final value is A. Now the two datastores are inconsistent
with each other, and they will permanently remain inconsistent until sometime later
someone comes and overwrites X again.</p>

<p>An the worst thing: you probably won’t even notice that your database and your search
indexes have gone out of sync, because no errors occurred. You’ll probably only realize
six months later, while you’re doing something completely different, that your database
and your indexes don’t match up, and you’ll have no idea how that could have happened.</p>

<p>That alone should be enough to put anyone off dual writes. But wait, there’s more…</p>

<p><img src="/2015/05/logs-12.png" alt="Updating denormalized data" width="550" height="303" /></p>

<p>Let’s look at denormalized data. Say, for example, you have an application where users
can send each other messages or emails, and you have an inbox for each user. When a new
message is sent, you want to do two things: add the message to the list of messages in
the user’s inbox, and also increment the user’s count of unread messages.</p>

<p>You keep a separate counter because you display it in the user interface all the time, and
it would be too slow to query the number of unread messages by scanning over the list of
messages every time you need to display the number. However, this counter is denormalized
information: it’s derived from the actual messages in the inbox, and whenever the messages
change, you also need to update the counter accordingly.</p>

<p>Let’s keep this one simple: one client, one database. Think about what happens over time:
first the client inserts the new message into the recipient’s inbox. Then the client
makes a request to increment the unread counter.</p>

<p><img src="/2015/05/logs-13.png" alt="Update of denormalized data fails" width="550" height="369" /></p>

<p>However, just in that moment, something goes wrong – perhaps the database goes down, or
a process crashes, or the network gets interrupted, or someone unplugs the wrong network
cable. Whatever the reason, the update to the unread counter fails.</p>

<p>Now your database is inconsistent: the message has been added to the inbox, but the
counter hasn’t been updated. And unless you periodically recompute all your counter
values from scratch, or undo the insertion of the message, it will forever remain
inconsistent.</p>

<p>Of course, you could argue that this problem was solved decades ago by <em>transactions</em>:
atomicity, the “A” in “ACID”, means that if you make several changes within one
transaction, they either all happen or none happen. The purpose of atomicity is to solve
precisely this issue – if something goes wrong during your writes, you don’t have to
worry about a half-finished set of changes making your data inconsistent.</p>

<p><img src="/2015/05/logs-14.png" alt="Wrapping two writes in a transaction" width="550" height="413" /></p>

<p>The traditional approach of wrapping the two writes in a transaction works fine in
databases that support it, but many of the new generation of databases don’t, so you’re
on your own.</p>

<p>Also, if the denormalized information is stored in a different database – for example,
if you keep your emails in a database but your unread counters in Redis – then you lose
the ability to tie the writes together into a single transaction. If one write succeeds,
and the other fails, you’re going to have a difficult time clearing up the inconsistency.</p>

<p>Some systems support distributed transactions, based on
<a href="http://the-paper-trail.org/blog/consensus-protocols-two-phase-commit/">2-phase commit</a>
for example. However, many datastores nowadays don’t support it, and even if they did,
it’s <a href="http://adrianmarriott.net/logosroot/papers/LifeBeyondTxns.pdf">not clear</a> whether
distributed transactions are a good idea in the first place. So we have to assume that
with dual writes, the application has to deal with partial failure, which is difficult.</p>

<p><img src="/2015/05/logs-08.png" alt="Our challenge: data integration" width="550" height="413" /></p>

<p>So, back to our original question. How do we make sure that all the data ends up in all
the right places? How do we get a copy of the same data to appear in several different
storage systems, and keep them all consistently in sync as the data changes?</p>

<p>As we saw, dual writes isn’t the solution, because it can introduce inconsistencies due
to race conditions and partial failures. How can we do better?</p>

<p><img src="/2015/05/logs-15.png" alt="Stupidly simple solution: totally ordered sequence of records" width="550" height="413" /></p>

<p>I’m a fan of stupidly simple solutions. The great thing about simple solutions is that
you have a chance of understanding them and convincing yourself that they’re correct.
And in this case, the simplest solution I can see is to do all your writes in a fixed
order, and to store them in that fixed order.</p>

<p>If you do all your writes sequentially, without any concurrency, then you have removed
the potential for race conditions. Moreover, if you write down the order in which you
make your writes, it becomes much easier to recover from partial failures, as I will
show later.</p>

<p>So, the stupidly simple solution that I propose looks like this: whenever anyone wants
to write some data, we append that write to the end of a sequence of records. That
sequence is totally ordered, it’s append-only (we never modify existing records, only
ever add new records at the end), and it’s persistent (we store it durably on disk).</p>

<p>The picture above shows an example of such a data structure: moving left to right, it
records that we first wrote X=5, then we wrote Y=8, then we wrote X=6, and so on.</p>

<p><img src="/2015/05/logs-16.png" alt="The ubiquitous log" width="550" height="303" /></p>

<p>That data structure has a name: we call it a <em>log</em>.</p>

<p>The interesting thing about logs is that they pop up in many different areas of
computing. Although it may seem like a stupidly simple idea that can’t possibly work,
it actually turns out to be incredibly powerful.</p>

<p><img src="/2015/05/logs-17.png" alt="One line from nginx access log" width="550" height="159" /></p>

<p>When I say “logs”, the first thing you probably think of is textual application logs
of the style you might get from Log4j or Syslog. For example, the above is one line from
an nginx server’s access log, telling me that some IP addresses requested a certain file
at a certain time. It also includes the referrer, the user-agent, the response code and
a few other things.</p>

<p>Sure, that’s one kind of log, but when I talk about logs here I mean something more
general. I mean any kind of data structure of totally ordered records that is
append-only and persistent. Any kind of append-only file.</p>

<p><img src="/2015/05/logs-18.png" alt="Logs are everywhere: DB storage engines" width="550" height="413" /></p>

<p>In the rest of this talk, I’d like to run through a few examples of how logs are used in
practice. It turns out that logs are already present in the databases and systems you use
every day. And once we understand how logs are used in various different systems, we’ll be
in a better position to understand how they can help us solve the problem of data
integration.</p>

<p>I’d like to talk about four different places where logs are used, and the first is in
the internals of database storage engines.</p>

<p><img src="/2015/05/logs-19.png" alt="B-tree example" width="550" height="291" /></p>

<p>Do you remember <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.219.7269">B-Trees</a>
from your algorithms classes? They are a very widely used data structure for storage
engines – almost all relational databases, and many non-relational databases, use them.</p>

<p>To summarize them briefly: a B-Tree consists of <em>pages</em>, which are fixed-size blocks on
disk, typically 4 or 8 kB in size. When you want to look up a particular key, you start
with one page, which is at the root of the tree. The page contains pointers to other
pages, and each pointer is tagged with a range of keys: for example, if your key is
between 0 and 100, you follow the first pointer; if your key is between 100 and 300, you
follow the second pointer; and so on.</p>

<p>The pointer takes you to another page, which further breaks down the key range into
sub-ranges. And eventually you end up at the page containing the particular key you’re
looking for.</p>

<p>Now what happens if you need to insert a new key/value pair into a B-tree? You have to
insert it into the page whose key range contains the key you’re inserting. If there is
enough spare space in that page, no problem. But if the page is full, it needs to be
split into two separate pages.</p>

<p><img src="/2015/05/logs-20.png" alt="B-tree node split" width="550" height="413" /></p>

<p>When you split a page, you need to write at least three pages to disk: the two pages
that are the result of the split, and the parent page (to update the pointers to the
split pages). However, these pages may be stored at various different locations on
disk.</p>

<p>This raises the question: what happens if the database crashes (or the power goes out, or
something else goes wrong) halfway through the operation, after only some of those pages
have been written to disk? In that case, you have the old (pre-split) data in some pages,
and the new (post-split) data in other pages, and that’s bad news. You’re most likely
going to end up with dangling pointers or pages that nobody is pointing to. In other
words, you’ve got a corrupted index.</p>

<p>Now, storage engines have been doing this for decades, so how do they make B-trees
reliable? The answer is that they use a
<a href="http://db.csail.mit.edu/madden/html/aries.pdf">write-ahead log</a> (WAL).</p>

<p>A write-ahead log is a particular kind of log, i.e. an append-only file on disk. Whenever
the storage engine wants to make any kind of change to the B-tree, it must <em>first</em> write
the change that it intends to make to the WAL. Only after it has been written to the WAL,
and durably written to disk, it is allowed to modify the actual B-tree.</p>

<p>This makes the B-tree reliable: if the database crashes while data was being appended to
the WAL, no problem, because the B-tree hasn’t been touched yet. And if it crashes while
the B-tree is being modified, no problem, because the WAL contains the information about
what changes were about to happen. When the database comes back up after the crash, it
can use the WAL to repair the B-tree and get it back into a consistent state.</p>

<p>This is our first example to show that logs are a really neat idea.</p>

<p><img src="/2015/05/logs-21.png" alt="Log-structured storage" width="550" height="413" /></p>

<p>Now, storage engines didn’t stop with B-trees. Some clever folks realized that if we’re
writing everything to a log anyway, we might as well use the log as the primary storage
medium. This is known as <a href="http://www.cs.umb.edu/~poneil/lsmtree.pdf">log-structured storage</a>,
which is used in <a href="http://blog.cloudera.com/blog/2012/06/hbase-io-hfile-input-output/">HBase</a>
and <a href="http://jonathanhui.com/how-cassandra-read-persists-data-and-maintain-consistency">Cassandra</a>,
and a variant appears in <a href="http://basho.com/assets/bitcask-intro.pdf">Riak</a>.</p>

<p>In log-structured storage we don’t always keep appending to the same file, because it
would become too large and it would be too difficult to find the key we’re looking for.
Instead, the log is broken into <em>segments</em>, and from time to time the storage engine
merges segments and discards duplicate keys. Segments may also be internally sorted by
key, which can make it easier to find the key you’re looking for, and also simplifies
merging. However, these segments are still logs: they are only written sequentially, and
they are immutable once written.</p>

<p>As you can see, logs play an important role in storage engines.</p>

<p><img src="/2015/05/logs-22.png" alt="Logs are everywhere: DB replication" width="550" height="413" /></p>

<p>Let’s move on to the second example where logs are used: database replication.</p>

<p>Replication is a feature that you find in many databases: it allows you to keep a copy of
the same data on several different nodes. That can be useful for spreading the load, and
it also means that if one node dies, you can fail over to another one.</p>

<p><img src="/2015/05/logs-23.png" alt="Leader-follower replication" width="550" height="413" /></p>

<p>There are a few different ways of implementing replication, but a common choice is to
designate one node as the <em>leader</em> (also known as <em>primary</em> or <em>master</em>), and the other
replicas as <em>followers</em> (also known as <em>standby</em> or <em>slave</em>). I don’t like the
master/slave terminology, so I’m going to stick with leader/follower.</p>

<p>Whenever a client wants to write something to the database, it needs to talk to the
leader. Read-only clients can use either the leader or the follower (although the
follower is typically asynchronous, so it may have slightly out-of-date information
if the latest writes haven’t yet been applied).</p>

<p>When clients write data to the leader, how does that data get to the followers? Big
surprise: they use a log! They use a <em>replication log</em>, which may in fact be the same
as the write-ahead log (this is what Postgres does, for example) or it may be a separate
replication log (MySQL does this).</p>

<p><img src="/2015/05/logs-24.png" alt="Follower applies writes in order of replication log" width="550" height="413" /></p>

<p>The replication log works as follows: whenever some data is written to the leader, it
is also appended to the replication log. The followers read that log in the order it was
written, and apply each of the writes to their own copy of the data. As a result, each
follower processes the same writes in the same order as the leader, and thus it ends up
with a copy of the same data.</p>

<p>Even if the writes happen concurrently on the reader, the log still contains the writes
in a total order. Thus, the log actually <em>removes</em> the concurrency from the writes – it
“<a href="https://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying">squeezes all the non-determinism out of the stream of
writes</a>”,
and on the follower there’s no doubt about the order in which the writes happened.</p>

<p>So what about the dual-writes race condition we discussed earlier?</p>

<p><img src="/2015/05/logs-11.png" alt="Race condition with dual writes" width="550" height="413" /></p>

<p>This race condition cannot happen with leader-based replication, because clients don’t
write directly to the followers. The only writes processed by followers are the ones
they receive from the replication log. And since the log fixes the order of those writes,
there is no ambiguity over which one happened first.</p>

<p><img src="/2015/05/logs-13.png" alt="Update of denormalized data fails" width="550" height="369" /></p>

<p>And what about the second problem with dual writes that we discussed earlier? This could
still happen: a follower could successfully process the first write from a transaction,
but fail to process the second write from the transaction (perhaps because the disk is
full, or the network is interrupted).</p>

<p><img src="/2015/05/logs-25.png" alt="Network interruption between leader and follower" width="550" height="413" /></p>

<p>If the network between the leader and the follower is interrupted, the replication log
cannot flow from the leader to the follower. This could lead to an inconsistent replica,
as we discussed previously. How does database replication recover from such errors and
avoid becoming inconsistent?</p>

<p>Notice that the log has a very nice property: because the leader only ever appends to it,
we can give each record in the log a sequential number that is always increasing (which we
might call <em>log position</em> or <em>offset</em>). Furthermore, followers only process it in
sequential order (from left to right, i.e. in order of increasing log position), so we can
describe a follower’s current state with a single number: the position of the latest
record it has processed.</p>

<p>When you know a follower’s current position in the log, you can be sure that all the
prior records in the log have already been processed, and none of the subsequent records
have been processed.</p>

<p>This is great, because it makes error recovery quite simple. If a follower becomes
disconnected from the leader, or it crashes, the follower just needs to store the log
position up to which it has processed the replication log. When the follower recovers,
it reconnects to the leader, and asks for the replication log starting from the last
offset that it previously processed. Thus, the follower can catch up on all the writes
that it missed while it was disconnected, without losing any data or receiving
duplicates.</p>

<p>The fact that the log is totally ordered makes this recovery much simpler than if you
had to keep track of every write individually.</p>

<p><img src="/2015/05/logs-26.png" alt="Logs are everywhere: distributed consensus" width="550" height="413" /></p>

<p>The third example of logs in practice is in a different area: distributed consensus.</p>

<p><img src="/2015/05/logs-27.png" alt="Examples of consensus" width="550" height="344" /></p>

<p>Achieving consensus is one of the well-known and often-discussed problems in
distributed systems. It is important, but it is also surprisingly difficult to solve.</p>

<p>An example of consensus in the real world would be trying to get a group of friends to
agree on where to go for lunch. This is a distinctive feature of a
<a href="https://www.goodreads.com/quotes/71510-the-history-of-every-major-galactic-civilization-tends-to-pass">sophisticated civilization</a>,
and can be a surprisingly difficult problem, especially if some of your friends are easily
distractible (so they don’t always respond to your questions) or if they are fussy eaters.</p>

<p>Closer to our usual domain of computers, an example of where you might want consensus is
in a distributed database system: for instance, you may require all your database nodes to
agree on which node is the leader for a particular partition (shard) of the database.</p>

<p>It’s pretty important that they all agree on who’s leader: if two different nodes both
think they are leader, they may both accept writes from clients. Later, when one of them
finds out that it was wrong and it wasn’t leader after all, the writes that it accepted
may be lost. This situation is known as <em>split brain</em>, and it can cause
<a href="https://aphyr.com/posts/284-call-me-maybe-mongodb">nasty data loss</a>.</p>

<p><img src="/2015/05/logs-28.png" alt="The Raft consensus protocol" width="550" height="413" /></p>

<p>There are a few different algorithms for implementing consensus.
<a href="http://www.read.seas.harvard.edu/~kohler/class/08w-dsi/chandra07paxos.pdf">Paxos</a> is
perhaps the most well-known, but there are also <a href="http://labs.yahoo.com/files/ZAB.pdf">Zab</a>
(used by <a href="https://zookeeper.apache.org/">Zookeeper</a>),
<a href="http://ramcloud.stanford.edu/raft.pdf">Raft</a> and <a href="http://arxiv.org/abs/1309.5671">others</a>.
These algorithms are quite tricky and have some non-obvious
<a href="http://www.cl.cam.ac.uk/~ms705/pub/papers/2015-osr-raft.pdf">subtleties</a>. In this talk,
I will just very briefly sketch one part of the Raft algorithm.</p>

<p>In a consensus system, there are a number of nodes (three in this diagram) which are in
charge of agreeing what the value of a particular variable should be. A client proposes
a value, for example X=8 (which may mean that node X is the leader for partition 8), by
sending it to one of the Raft nodes. That node collects votes from the other nodes. If
a majority of nodes agree that the value should be X=8, the first node is allowed to
commit the value.</p>

<p>When that value is committed, what happens? In Raft, that value is appended to the end of
a log. Thus, what Raft is doing is not just getting the nodes to agree on one particular
value – it’s actually building up a log of values that have been agreed over time. All
Raft nodes are guaranteed to have exactly the same sequence of committed values in their
log, and clients can consume this log.</p>

<p><img src="/2015/05/logs-29.png" alt="Raft commits a value by appending it to a log" width="550" height="413" /></p>

<p>Once the newly agreed value has been committed, appended to the log and replicated to the
other nodes, the client that originally proposed the value X=8 is sent a response saying
that the system succeeded in reaching consensus, and that the proposed value is now part
of the Raft log.</p>

<p>(As a theoretical aside, the problems of consensus and <em>atomic broadcast</em> – that is,
creating a log with exactly-once delivery – are
<a href="http://courses.csail.mit.edu/6.852/08/papers/CT96-JACM.pdf">reducible to each other</a>.
This means Raft’s use of a log is not just a convenient implementation detail, but also
reflects a fundamental property of the consensus problem it is solving.)</p>

<p><img src="/2015/05/logs-30.png" alt="Logs are everywhere: Kafka" width="550" height="413" /></p>

<p>Ok. We’ve seen that logs really are a recurring theme in surprisingly many areas of
computing: storage engines, database replication and consensus. As the fourth and final
example, I’d like to talk about <a href="http://kafka.apache.org/">Apache Kafka</a>, another system
that is built around the idea of logs. The interesting thing about Kafka is that it
it doesn’t hide the log from you. Rather than treating the log as an implementation
detail, Kafka exposes it to you, so that you can build applications around it.</p>

<p>You may have heard of Kafka before. It’s an open source project that was originally
developed at LinkedIn, and is now a lively Apache project with many different
contributors and users.</p>

<p><img src="/2015/05/logs-31.png" alt="Kafka producers and consumers" width="550" height="413" /></p>

<p>The typical use of Kafka is as a message broker (message queue) – so it is somewhat
comparable to AMQP, JMS and other messaging systems. Kafka has two types of clients:
<em>producers</em> (which send messages to Kafka) and <em>consumers</em> (which subscribe to streams
of messages in Kafka).</p>

<p>For example, producers may be your web servers or mobile apps, and the types of messages
they send to Kafka might be logging information – e.g. events that indicate which user
clicked which link at which point in time. The consumers are various processes that need
to find out about stuff that is happening: for example, to generate analytics, to monitor
for unusual activity, to generate personalized recommendations for users, and so on.</p>

<p><img src="/2015/05/logs-32.png" alt="Kafka architecture: a partitioned log" width="550" height="367" /></p>

<p>The thing that makes Kafka interestingly different from other message brokers is that
it is structured as a log. In fact, it has many logs! Data streams in Kafka are split
into <em>partitions</em>, and each partition is a log (a totally ordered sequence of messages).
Different partitions are completely independent from each other, so there is no ordering
guarantee across different partitions. This allows different partitions to be handled
on different servers, which is important for the scalability of Kafka.</p>

<p>Each partition is stored on disk and replicated across several machines, so it is
durable and can tolerate machine failure without data loss. Producing and consuming
logs is very similar to what we saw previously in the context of database replication:</p>

<ul>
  <li>Every message that is sent to Kafka is appended to the end of a partition. That is the
only write operation supported by Kafka: appending to the end of a log. It’s not
possible to modify past messages.</li>
  <li>Within each partition, messages have a monotonically increasing <em>offset</em> (log
position). To consume messages from Kafka, a client reads messages sequentially,
starting from a particular offset. That offset is managed by the consumer.</li>
</ul>

<p>Let’s return to the data integration problem from the beginning of this talk.</p>

<p><img src="/2015/05/logs-05.png" alt="Web app with a horrendous mess of storage services" width="550" height="413" /></p>

<p>Say you have this tangle of different datastores, caches and indexes that need to be kept
in sync with each other. Now that we have seen a bunch of examples of practical
applications of logs, can we use what we’ve learnt to figure out how to build these
systems in a better way?</p>

<p><img src="/2015/05/logs-33.png" alt="Stop doing dual writes!" width="550" height="413" /></p>

<p>Firstly, we need to stop doing dual writes. As discussed, it’s probably going to make your
data inconsistent, unless you have very carefully thought about the potential race
conditions and partial failures that can occur in your application.</p>

<p>And note this inconsistency isn’t just a kind of “eventual consistency” that is often
quoted in asynchronous systems. What I’m talking about here is permanent inconsistency –
if you’ve written two different values to two different datastores, due to a race
condition or partial failure, that difference won’t simply resolve itself. You’d have to
take explicit actions to search for data mismatches and resolve them (which is difficult,
since the data is constantly changing).</p>

<p>We need a better approach than dual writes for keeping different datastores in sync.</p>

<p><img src="/2015/05/logs-34.png" alt="Instead, embrace the log" width="550" height="413" /></p>

<p>What I propose is this: rather than having the application write directly to the various
datastores, the application only appends the data to a log (such as Kafka). All the
different representations of this data – your databases,
<a href="https://www.facebook.com/note.php?note_id=23844338919&amp;id=9445547199">your caches</a>,
your indexes – are constructed by consuming the log in sequential order.</p>

<p>Each datastore that needs to be kept in sync is an independent consumer of the log. Every
consumer takes the data in the log, one record at a time, and writes it to its own
datastore. The log guarantees that the consumers all see the records in the same order; by
applying the writes in the same order, the problem of race conditions is gone.  This looks
very much like the database replication we saw earlier!</p>

<p>And what about the problem of partial failure? What if one of your stores has a problem
and can’t accept writes for a while?</p>

<p><img src="/2015/05/logs-13.png" alt="Update of denormalized data fails" width="550" height="369" /></p>

<p>That problem is also solved by the log: each consumer keeps track of the log position up
to which it has processed the log. When the error in the datastore-writing consumer is
resolved, it can resume processing records in the log from the last position it previously
reached. That way, a datastore won’t lose any updates, even if it’s offline for a while.
This is great for decoupling parts of your system: even if there is a problem in one
datastore, the rest of the system remains unaffected.</p>

<p>The log, the stupidly simple idea of putting your writes in a total order, strikes again.</p>

<p>Just one problem remains: the consumers of the log all update their datastores
asynchronously, so they are eventually consistent. Reading from them is like reading from
a database follower: they may be a little behind the latest writes, so you don’t have
a guarantee of
<a href="http://www.allthingsdistributed.com/2007/12/eventually_consistent.html">read-your-writes</a>
(and certainly not <a href="https://aphyr.com/posts/313-strong-consistency-models">linearizability</a>).</p>

<p>I think that can be overcome by layering a
<a href="http://research.microsoft.com/pubs/199947/Tango.pdf">transaction protocol</a> on top of the
log, but that’s a researchy area which so far hasn’t been widely implemented in
production systems. For now, a better option is to extract the log from a database:</p>

<p><img src="/2015/05/logs-35.png" alt="Using change data capture" width="550" height="413" /></p>

<p>This approach is called <em>change data capture</em>, which I
<a href="http://blog.confluent.io/2015/04/23/bottled-water-real-time-integration-of-postgresql-and-kafka/">wrote about recently</a>
(and <a href="https://github.com/confluentinc/bottledwater-pg">implemented on PostgreSQL</a>). As
long as you’re only writing to a single database (not doing dual writes), and getting the
log of writes from the database (in the order in which they were committed to the DB),
then this approach works just as well as making your writes to the log directly.</p>

<p>As this database in front of the log applies writes synchronously, you can use it to make
reads that require “immediate consistency” (linearizability), and enforce constraints
(e.g. requiring that account balances never go negative). Going via a database also means
that you don’t need to trust the log as your system of record (which may be a scary
prospect if it’s implemented with a new technology) – if you have an existing database
that you know and like, and you can extract a change log from that database, you can still
get all the advantages of a log-oriented architecture. I’ll be talking more about this topic in an
<a href="http://martin.kleppmann.com/2015/06/02/change-capture-at-berlin-buzzwords.html">upcoming conference talk</a>.</p>

<p>To close, I’d like to leave you with a thought experiment:</p>

<p><img src="/2015/05/logs-36.png" alt="Thought experiment: could you make all your writes through a log?" width="550" height="413" /></p>

<p>Most APIs we work with have endpoints for both reading and writing. In RESTful terms,
<code>GET</code> is for reading (i.e. side-effect-free operations) and <code>POST</code>, <code>PUT</code> and <code>DELETE</code>
are for writing. These endpoints for writing are ok if you only have one system you’re
writing to, but if you have more than one such system, you quickly end up with dual
writes and all their aforementioned problems.</p>

<p>Imagine a system with an API in which you eliminate all the endpoints for writing. Imagine
that you keep all the <code>GET</code> requests, but prohibit any <code>POST</code>, <code>PUT</code> or <code>DELETE</code>. Instead,
the only way you can send writes into the system is by appending them to a log, and having
the system consume that log. (The log must be outside of the system, so that you can have
several consumers for the same log.)</p>

<p>For example, imagine a variant of Elasticsearch in which you cannot write documents
through the REST API, but only write documents by sending them to Kafka. Elasticsearch
would internally include a Kafka consumer that takes documents and adds them to the index.
This would actually simplify some of the internals of Elasticsearch, since it would no
longer have to worry about concurrency control, and replication would be simpler to
implement. And it would sit neatly alongside other tools that may be consuming the same
log.</p>

<p>My favorite feature of this log-oriented architecture is this: if you want to build a new
derived datastore, you can just start a new consumer at the beginning of the log, and
churn through the history of the log, applying all the writes to your datastore. When you
reach the end, you’ve got a new view onto your dataset, and you can keep it up-to-date by
simply continuing to consume the log!</p>

<p>This makes it really easy to try out new ways of presenting your existing data, for
example to index it another way. You can build experimental new indexes or views onto your
data without interfering with any of the existing data. If the result is good, you can
shift users to read from the new view; if it isn’t, you can just discard it again. This
gives you tremendous freedom to experiment and adapt your application.</p>

<h2 id="further-reading">Further reading</h2>

<ul>
  <li>Many of the ideas in this talk were previously laid out by Jay Kreps:
“<a href="http://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying">The Log: What every software engineer should know about real-time data’s unifying
abstraction</a>,”
16 December 2013. (An edited version was published as an
<a href="http://shop.oreilly.com/product/0636920034339.do">ebook by O’Reilly Media</a>, September 2014.)</li>
  <li>This talk arose from research I did for my own book,
“<a href="http://dataintensive.net">Designing Data-Intensive Applications</a>,” to appear with O’Reilly Media in 2015.</li>
  <li>For a more detailed vision of deriving materialised views from a log, see my previous talk
“<a href="https://martin.kleppmann.com/2015/03/04/turning-the-database-inside-out.html">Turning the database inside-out with Apache
Samza</a>,” at <em>Strange Loop</em>, 18 Sep 2014.</li>
  <li>Pat Helland has also observed that immutability and append-only datasets are a recurring pattern
at many levels of the stack: see “<a href="http://www.cidrdb.org/cidr2015/Papers/CIDR15_Paper16.pdf">Immutability Changes Everything</a>,”
at <em>7th Biennial Conference on Innovative Data Systems Research</em> (CIDR), January 2015. </li>
  <li>LinkedIn’s approach to building derived data systems based on totally ordered logs is described in
Shirshanka Das, Chavdar Botev, Kapil Surlaker, et al.: “<a href="http://www.socc2012.org/s18-das.pdf">All Aboard the Databus!</a>,”
at <em>ACM Symposium on Cloud Computing</em> (SoCC), October 2012.</li>
  <li>Facebook’s Wormhole has a lot of similarities to Databus. See Yogeshwer Sharma, Philippe Ajoux, Petchean
Ang, et al.: “<a href="https://www.usenix.org/system/files/conference/nsdi15/nsdi15-paper-sharma.pdf">Wormhole: Reliable Pub-Sub to Support Geo-replicated Internet
Services</a>,” at <em>12th USENIX
Symposium on Networked Systems Design and Implementation</em> (NSDI), May 2015.</li>
  <li>If you need transactional semantics (e.g. linearizability), you can add a transaction protocol on top of the
asynchronous log. I like the one described in Mahesh Balakrishnan, Dahlia Malkhi, Ted Wobber, et al.:
“<a href="http://research.microsoft.com/pubs/199947/Tango.pdf">Tango: Distributed Data Structures over a Shared Log</a>,”
at <em>24th ACM Symposium on Operating Systems Principles</em> (SOSP), pages 325–340, November 2013. </li>
  <li>Write-ahead logs are described in many places. For a detailed discussion, see
C Mohan, Don Haderle, Bruce G Lindsay, Hamid Pirahesh, and Peter Schwarz:
“<a href="http://db.csail.mit.edu/madden/html/aries.pdf">ARIES: A Transaction Recovery Method Supporting Fine-Granularity Locking and Partial Rollbacks Using
Write-Ahead Logging</a>,”
<em>ACM Transactions on Database Systems</em> (TODS), volume 17, number 1, pages 94–162, March 1992.</li>
  <li>The log-structured storage approach used in Cassandra and HBase appears in
Patrick O’Neil, Edward Cheng, Dieter Gawlick, and Elizabeth O’Neil:
“<a href="http://www.cs.umb.edu/~poneil/lsmtree.pdf">The Log-Structured Merge-Tree (LSM-Tree)</a>,”
<em>Acta Informatica</em>, volume 33, number 4, pages 351–385, June 1996.</li>
  <li>For an analysis of the Raft consensus algorithm, and some subtle correctness requirements, see
Heidi Howard, Malte Schwarzkopf, Anil Madhavapeddy, and Jon Crowcroft:
“<a href="http://www.cl.cam.ac.uk/~ms705/pub/papers/2015-osr-raft.pdf">Raft Refloated: Do We Have Consensus?</a>,”
<em>ACM SIGOPS Operating Systems Review</em>, volume 49, number 1, pages 12–21, January 2015.</li>
</ul>

                ]]></content:encoded>
            </item>
        
            <item>
                <title>Please stop calling databases CP or AP</title>
                <link>http://martin.kleppmann.com/2015/05/11/please-stop-calling-databases-cp-or-ap.html</link>
                <comments>http://martin.kleppmann.com/2015/05/11/please-stop-calling-databases-cp-or-ap.html#disqus_thread</comments>
                <pubDate>Mon, 11 May 2015 00:00:00 +0000</pubDate>
                <dc:creator>Martin Kleppmann</dc:creator>
                
                    <guid isPermaLink="true">http://martin.kleppmann.com/2015/05/11/please-stop-calling-databases-cp-or-ap.html</guid>
                
                <description><![CDATA[ In his excellent blog post Notes on Distributed Systems for Young Bloods, Jeff Hodges recommends that you use the CAP theorem to critique systems. A lot of people have taken that advice to heart, describing their systems as “CP” (consistent but not available under network partitions), “AP” (available but not... ]]></description>
                <content:encoded><![CDATA[
                    <p>In his excellent blog post <a href="http://www.somethingsimilar.com/2013/01/14/notes-on-distributed-systems-for-young-bloods/">Notes on Distributed Systems for Young Bloods</a>, Jeff Hodges
recommends that you use the <a href="http://henryr.github.io/cap-faq/">CAP theorem</a> to critique systems. A lot of people have taken
that advice to heart, describing their systems as “CP” (consistent but not available under network
partitions), “AP” (available but not consistent under network partitions), or sometimes “CA”
(meaning “I still haven’t read <a href="http://codahale.com/you-cant-sacrifice-partition-tolerance/">Coda’s post from almost 5 years ago</a>”).</p>

<p>I agree with all of Jeff’s other points, but with regard to the CAP theorem, I must disagree. The
CAP theorem is too simplistic and too widely misunderstood to be of much use for characterizing
systems. Therefore I ask that we retire all references to the CAP theorem, stop talking about the
CAP theorem, and put the poor thing to rest. Instead, we should use more precise terminology to
reason about our trade-offs.</p>

<p>(Yes, I realize the irony of writing a blog post about the very topic that I am asking people to
stop writing about. But at least it gives me a URL that I can give to people when they ask why
I don’t like them talking about the CAP theorem. Also, apologies if this is a bit of a rant, but
at least it’s a rant with lots of literature references.)</p>

<h2 id="cap-uses-very-narrow-definitions">CAP uses very narrow definitions</h2>

<p>If you want to refer to CAP as a <em>theorem</em> (as opposed to a vague hand-wavy concept in your
database’s marketing materials), you have to be precise. Mathematics requires precision. The proof
only holds if you use the words with the same meaning as they are used in <a href="http://webpages.cs.luc.edu/~pld/353/gilbert_lynch_brewer_proof.pdf">the proof</a>.
And the proof uses very particular definitions:</p>

<ul>
  <li>
    <p><em>Consistency</em> in CAP actually means <a href="http://cs.brown.edu/~mph/HerlihyW90/p463-herlihy.pdf">linearizability</a>, which is a very specific (and very
strong) notion of consistency. In particular it has got nothing to do with the C in ACID, even
though that C also stands for “consistency”. I explain the meaning of linearizability below.</p>
  </li>
  <li>
    <p><em>Availability</em> in CAP is defined as “every request received by a non-failing [database] node in
the system must result in a [non-error] response”. It’s not sufficient for <em>some</em> node to be able
to handle the request: <em>any</em> non-failing node needs to be able to handle it. Many so-called
“highly available” (i.e. low downtime) systems actually do not meet this definition of
availability.</p>
  </li>
  <li>
    <p><em>Partition Tolerance</em> (terribly mis-named) basically means that you’re communicating over an
<a href="http://henryr.github.io/cap-faq/">asynchronous network</a> that may delay or drop messages. The internet and all our
datacenters <a href="https://aphyr.com/posts/288-the-network-is-reliable">have this property</a>, so you don’t really have any choice in this
matter.</p>
  </li>
</ul>

<p>Also note that the CAP theorem doesn’t just describe any old system, but a very specific model of
a system:</p>

<ul>
  <li>
    <p>The CAP system model is a single, read-write register – that’s all. For example, the CAP theorem
says nothing about transactions that touch multiple objects: they are simply out of scope of the
theorem, unless you can somehow reduce them down to a single register.</p>
  </li>
  <li>
    <p>The only fault considered by the CAP theorem is a network partition (i.e. nodes remain up, but
the network between some of them is not working). That kind of fault absolutely
<a href="https://aphyr.com/posts/288-the-network-is-reliable">does happen</a>, but it’s not the only kind of thing that can go wrong: nodes can
crash or be rebooted, you can run out of disk space, you can hit a bug in the software, etc. In
building distributed systems, you need to consider a much wider range of trade-offs, and focussing
too much on the CAP theorem leads to ignoring other important issues.</p>
  </li>
  <li>
    <p>Also, the CAP theorem says nothing about latency, which people <a href="http://dbmsmusings.blogspot.co.uk/2010/04/problems-with-cap-and-yahoos-little.html">tend to care about more</a>
than availability. In fact, CAP-available systems are allowed to be arbitrarily slow to respond,
and can still be called “available”. Going out on a limb, I’d guess that your users wouldn’t call
your system “available” if it takes 2 minutes to load a page. </p>
  </li>
</ul>

<p>If your use of words matches the precise definitions of the proof, then the CAP theorem applies to
you. But if you’re using some other notion of consistency or availability, you can’t expect the CAP
theorem to still apply. Of course, that doesn’t mean you can suddenly do impossible things, just by
redefining some words! It just means that you can’t turn to the CAP theorem for guidance, and you
cannot use the CAP theorem to justify your point of view.</p>

<p>If the CAP theorem doesn’t apply, that means you have to think through the trade-offs yourself. You
can reason about consistency and availability using your own definitions of those words, and you’re
welcome to prove your own theorem. But please don’t call it CAP theorem, because that name is
already taken.</p>

<h2 id="linearizability">Linearizability</h2>

<p>In case you’re not familiar with linearizability (i.e. “consistency” in the CAP sense), let me
explain it briefly. The <a href="http://cs.brown.edu/~mph/HerlihyW90/p463-herlihy.pdf">formal definition</a> is not entirely straightforward, but
the key idea, stated informally, is this:</p>

<blockquote>
  <p>If operation B started after operation A successfully completed, then operation B must see the
the system in the same state as it was on completion of operation A, or a newer state.</p>
</blockquote>

<p>To make this more tangible, consider an example of a system that is <em>not</em> linearizable. See the
following diagram (sneak preview from an unreleased chapter of <a href="http://dataintensive.net/">my book</a>):</p>

<p><img src="/2015/05/linearizability.png" width="550" height="391" alt="Illustration of a non-linearizable sequence of events" /></p>

<p>This diagram shows Alice and Bob, who are in the same room, both checking their phones to see the
outcome of the <a href="http://www.bbc.co.uk/sport/0/football/28181689">2014 football world cup final</a>. Just after the final score is announced,
Alice refreshes the page, sees the winner announced, and excitedly tells Bob about it. Bob
incredulously hits <em>reload</em> on his own phone, but his request goes to a database replica that is
lagging, and so his phone shows that the game is still ongoing.</p>

<p>If Alice and Bob had hit reload at the same time, it wouldn’t have been surprising if they had got
two different query results, because they don’t know at exactly what time their respective requests
were processed by the server. However, Bob knows that he hit the reload button (initiated his query)
<em>after</em> he heard Alice exclaim the final score, and therefore he expects his query result to be at
least as recent as Alice’s. The fact that he got a stale query result is a violation of
linearizability.</p>

<p>Knowing that Bob’s request happened strictly after Alice’s request (i.e. that they were not
concurrent) depends on the fact that Bob heard about Alice’s query result through a separate
communication channel (in this case, IRL audio). If Bob hadn’t heard from Alice that the game was
over, he wouldn’t have known that the result of his query was stale.</p>

<p>If you’re building a database, you don’t know what kinds of backchannel your clients may have. Thus,
if you want to provide linearizable semantics (CAP-consistency) in your database, you need to make
it appear as though there is only a single copy of the data, even though there may be copies
(replicas, caches) of the data in multiple places.</p>

<p>This is a fairly expensive guarantee to provide, because it requires a lot of coordination. Even the
CPU in your computer <a href="http://www.cl.cam.ac.uk/~pes20/weakmemory/x86tso-paper.tphols.pdf">doesn’t provide linearizable access to your local RAM</a>! On modern
CPUs, you need to use an explicit <a href="http://mechanical-sympathy.blogspot.co.uk/2011/07/memory-barriersfences.html">memory barrier instruction</a> in order to get
linearizability. And even testing whether a system provides linearizability is <a href="https://github.com/aphyr/knossos">tricky</a>.</p>

<h2 id="cap-availability">CAP-Availability</h2>

<p>Let’s talk briefly about the need to give up either linearizability or availability in the case of
a network partition.</p>

<p>Let’s say you have replicas of your database in two different datacenters. The exact method of
replication doesn’t matter for now – it may be single-leader (master/slave), multi-leader
(master/master) or quorum-based replication (<a href="http://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf">Dynamo-style</a>). The requirement of replication
is that whenever data is written to data in one datacenter, it also has to be written to the replica
in the other datacenter. Assuming that clients only connect to one datacenter, there must be
a network link between the two datacenters over which the replication happens.</p>

<p>Now assume that network link is interrupted – that’s what we mean with a <em>network partition</em>. What
happens?</p>

<p><img src="/2015/05/cap-availability.png" width="550" height="251" alt="A network partition between two datacenters" /></p>

<p>Clearly you can choose one of two things:</p>

<ol>
  <li>
    <p>The application continues to be allowed to write to the database, so it remains fully available
in both datacenters. However, as long as the replication link is interrupted, any changes that
are written in one datacenter will not appear in the other datacenter. This violates
linearizability (in terms of the previous example, Alice could be connected to DC 1 and Bob
could be connected to DC 2).</p>
  </li>
  <li>
    <p>If you don’t want to lose linearizability, you have to make sure you do all your reads and
writes in one datacenter, which you may call the <em>leader</em>. In the other datacenter (which cannot
be up-to-date, due to the failed replication link), the database must stop accepting reads and
writes until the network partition is healed and the database is in sync again. Thus, although
the non-leader database has not failed, it cannot process requests, so it is not CAP-available.</p>
  </li>
</ol>

<p>(And this, by the way, is essentially the proof of the CAP theorem. That’s all there is to it. This
example uses two datacenters, but it applies equally to network problems within a single datacenter.
I just find it easier to think about when I imagine it as two datacenters.)</p>

<p>Note that in our notionally “unavailable” situation in option 2, we’re still happily processing
requests in one of the datacenters. So if a system chooses linearizability (i.e. it is not
CAP-available), that doesn’t necessarily mean that a network partition automatically leads to an
outage of the application. If you can shift all clients to using the leader datacenter, the clients
will in fact see no downtime at all.</p>

<p>Availability in practice <a href="http://blog.thislongrun.com/2015/04/cap-availability-high-availability-and_16.html">does not quite correspond</a> to CAP-availability. Your
application’s availability is probably measured with some SLA (e.g. 99.9% of well-formed requests
must return a successful response within 1 second), but such an SLA can be met both with
CAP-available and CAP-unavailable systems.</p>

<p>In practice, multi-datacenter systems <em>are</em> often designed with asynchronous replication, and thus
non-linearizable. However, the reason for that choice is often the latency of wide-area networks,
not just wanting to tolerate datacenter and network failures.</p>

<h2 id="many-systems-are-neither-linearizable-nor-cap-available">Many systems are neither linearizable nor CAP-available</h2>

<p>Under the CAP theorem’s strict definitions of consistency (linearizability) and availability, how
do systems fare?</p>

<p>For example, take any replicated database with a single leader, which is the standard way of setting
up replication in most relational databases. In this configuration, if a client is partitioned from
the leader, it cannot write to the database. Even though it may be able to read from a follower (a
read-only replica), the fact that it cannot write means any single-leader setup is not
CAP-available. Never mind that such configurations are often marketed as “high availability”.</p>

<p>If single-leader replication is not CAP-available, does that make it “CP”? Wait, not so fast. If you
allow the application to make reads from a follower, and the replication is asynchronous (the
default in most databases), then a follower may be a little behind the leader when you read from it.
In this case, your reads will not be linearizable, i.e. not CAP-consistent.</p>

<p>Moreover, databases with <a href="http://research.microsoft.com/pubs/69541/tr-95-51.pdf">snapshot isolation</a>/MVCC are intentionally non-linearizable,
because enforcing linearizability would reduce the level of concurrency that the database can offer.
For example, <a href="http://drkp.net/papers/ssi-vldb12.pdf">PostgreSQL’s SSI</a> provides <em>serializability</em> but not
<em>linearizability</em>, and <a href="http://www.researchgate.net/publication/220225203_Making_snapshot_isolation_serializable/file/e0b49520567eace81f.pdf">Oracle provides neither</a>. Just because a database is branded “ACID”
doesn’t mean it meets the CAP theorem’s definition of consistency.</p>

<p>So these systems are neither CAP-consistent nor CAP-available. They are neither “CP” nor “AP”, they
are just “P”, whatever that means. (Yes, the “two out of three” formulation <em>does</em> allow you to pick
only one out of three, or even none out of three!)</p>

<p>What about “NoSQL”? Take MongoDB, for example: it has a single leader per shard (or at least it’s
supposed to, if it’s not in split-brain mode), so it’s not CAP-available by the argument above. And
Kyle <a href="https://aphyr.com/posts/322-call-me-maybe-mongodb-stale-reads">recently showed</a> that it allows non-linearizable reads even at the highest
consistency setting, so it’s not CAP-consistent either.</p>

<p>And the <a href="http://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf">Dynamo</a> derivatives like Riak, Cassandra and Voldemort, which are often called “AP” since
they optimize for high availability? It depends on your settings. If you accept a single replica for
reads and writes (R=W=1), they are indeed CAP-available. However, if you require quorum reads and
writes (R+W&gt;N), and you have a network partition, clients on the minority side of the partition
cannot reach a quorum, so quorum operations are not CAP-available (at least temporarily, until the
database sets up additional replicas on the minority side).</p>

<p>You sometimes see people people claiming that quorum reads and writes guarantee linearizability, but
I think it would be unwise to rely on it – subtle combinations of features such as sloppy quorums
and read repair can lead to <a href="http://basho.com/riaks-config-behaviors-part-3/">tricky edge cases</a> in which deleted data is resurrected,
or the number of replicas of a value falls below the original W (violating the quorum condition), or
the number of replica nodes increases above the original N (again violating the quorum condition).
All of these lead to non-linearizable outcomes.</p>

<p>These are not bad systems: people successfully use them in production all the time. However, so far
we haven’t been able to rigorously classify them as “AP” or “CP”, either because it depends on the
particular operation or configuration, or because the system meets neither of the CAP theorem’s
strict definitions of consistency or availability.</p>

<h2 id="case-study-zookeeper">Case study: ZooKeeper</h2>

<p>What about ZooKeeper? It uses a <a href="http://web.stanford.edu/class/cs347/reading/zab.pdf">consensus algorithm</a>, so people generally regard it as a
<a href="http://www.knewton.com/tech/blog/2014/12/eureka-shouldnt-use-zookeeper-service-discovery/">clear-cut case of choosing consistency over availability</a> (i.e. a “CP system”).</p>

<p>However, if you look at the <a href="http://zookeeper.apache.org/doc/r3.4.6/zookeeperProgrammers.html#ch_zkGuarantees">ZooKeeper docs</a>, they make quite clear that ZooKeeper
by default <em>does not</em> provide linearizable reads. Each client is connected to one of the server
nodes, and when you make a read, you see only the data on that node, even if there are more
up-to-date writes on another node. This makes reads much faster than if you had to assemble a quorum
or contact the leader for every read, but it also means that ZooKeeper by default <em>does not</em> meet
the CAP theorem’s definition of consistency.</p>

<p>It is possible to make linearizable reads in ZooKeeper by <a href="http://mail-archives.apache.org/mod_mbox/zookeeper-user/201303.mbox/%3CCAJwFCa0Hoekc14Zy6i0LyLj=eraF8JimqMZadohoKQJNTMtYSg@mail.gmail.com%3E">preceding a read with a <code>sync</code>
command</a>. That isn’t the default though, because it comes with a performance penalty.
People do use <code>sync</code>, but usually not all the time.</p>

<p>What about ZooKeeper availability? Well, ZK requires a <a href="http://www.tcs.hut.fi/Studies/T-79.5001/reports/2012-deSouzaMedeiros.pdf">majority quorum</a> in
order to reach consensus, i.e. in order to process writes. If you have a partition with a majority
of the nodes on one side and a minority on the other, then the majority side continues to function,
but the nodes on the minority side can’t process writes, even though the nodes are up. Thus, writes
in ZK are not CAP-available under a partition (even though the majority side can continue to process
writes).</p>

<p>To add to the fun, ZooKeeper 3.4.0 added a <a href="http://zookeeper.apache.org/doc/r3.4.6/zookeeperAdmin.html#Experimental+Options%2FFeatures">read-only mode</a>, in which nodes on the
minority side of a partition can continue serving read requests – no quorum needed! This read-only
mode <em>is</em> CAP-available. Thus, ZooKeeper by default is neither CAP-consistent (CP) nor CAP-available
(AP) – it’s really just “P”. However, you can optionally make it CP by calling <code>sync</code> if you want,
and for reads (but not for writes) it’s actually AP, if you turn on the right option.</p>

<p>But this is irritating. Calling ZooKeeper “not consistent”, just because it’s not linearizable by
default, really badly misrepresents its features. It actually provides an excellent level of
consistency! It provides <a href="http://web.stanford.edu/class/cs347/reading/zab.pdf">atomic broadcast</a> (which is <a href="http://courses.csail.mit.edu/6.852/08/papers/CT96-JACM.pdf">reducible to consensus</a>)
combined with the session guarantee of <a href="http://www-i2.informatik.rwth-aachen.de/i2/fileadmin/user_upload/documents/Seminar_MCMM11/Causal_memory_1996.pdf">causal consistency</a> – which is <a href="http://arxiv.org/pdf/1302.0309.pdf">stronger</a>
than <a href="http://www.researchgate.net/profile/Douglas_Terry3/publication/3561300_Session_guarantees_for_weakly_consistent_replicated_data/links/02e7e52cdbe60a6cb4000000.pdf">read your writes, monotonic reads</a> and <a href="http://research.microsoft.com/pubs/157411/ConsistencyAndBaseballReport.pdf">consistent prefix reads</a> combined.
The documentation says that it provides <a href="http://research-srv.microsoft.com/en-us/um/people/lamport/pubs/multi.pdf">sequential consistency</a>, but it’s under-selling
itself, because ZooKeeper’s guarantees are in fact much stronger than sequential consistency.</p>

<p>As ZooKeeper demonstrates, it is quite reasonable to have a system that is neither CAP-consistent
nor CAP-available in the presence of partitions, and by default isn’t even linearizable in the
<em>absence</em> of partitions. (I guess that would be PC/EL in <a href="http://dbmsmusings.blogspot.co.uk/2010/04/problems-with-cap-and-yahoos-little.html">Abadi’s PACELC framework</a>, but
I don’t find that any more enlightening than CAP.)</p>

<h2 id="cpap-a-false-dichotomy">CP/AP: a false dichotomy</h2>

<p>The fact that we haven’t been able to classify even one datastore as unambiguously “AP” or “CP”
should be telling us something: those are simply not the right labels to describe systems.</p>

<p>I believe that we should stop putting datastores into the “AP” or “CP” buckets, because:</p>

<ul>
  <li>
    <p>Within one piece of software, you may well have various operations with <a href="http://groups.csail.mit.edu/tds/papers/Gilbert/Brewer2.pdf">different consistency
characteristics</a>.</p>
  </li>
  <li>
    <p>Many systems are neither consistent nor available under the CAP theorem’s definitions. However,
I’ve never heard anyone call their system just “P”, presumably because it looks bad. But it’s not
bad – it may be a perfectly reasonable design, it just doesn’t fit one of the two CP/AP buckets.</p>
  </li>
  <li>
    <p>Even though most software doesn’t neatly fit one of those two buckets, people try to shoehorn
software into one of the two buckets anyway, thereby inevitably changing the meaning of
“consistency” or “availability” to whatever definition suits them. Unfortunately, if the meaning
of the words is changed, the CAP theorem no longer applies, and thus the CP/AP distinction is
rendered completely meaningless.</p>
  </li>
  <li>
    <p>A huge amount of subtlety is lost by putting a system in one of two buckets. There are many
considerations of fault-tolerance, latency, simplicity of programming model, operability, etc.
that feed into the design of a distributed systems. It is simply not possible to encode this
subtlety in one bit of information. For example, even though ZooKeeper has an “AP” read-only mode,
this mode still provides a total ordering of historical writes, which is a vastly stronger
guarantee than the “AP” in a system like Riak or Cassandra – so it’s ridiculous to throw them
into the same bucket.</p>
  </li>
  <li>
    <p>Even Eric Brewer <a href="http://cs609.cs.ua.edu/CAP12.pdf">admits</a> that CAP is misleading and oversimplified. In 2000, it was meant
to start a discussion about trade-offs in distributed data systems, and it did that very well. It
wasn’t intended to be a breakthrough formal result, nor was it meant to be a rigorous
classification scheme for data systems. 15 years later, we now have a much greater range of tools
with different consistency and fault-tolerance models to choose from. CAP has served its purpose,
and now it’s time to move on.</p>
  </li>
</ul>

<h2 id="learning-to-think-for-yourself">Learning to think for yourself</h2>

<p>If CP and AP are unsuitable to describe and critique systems, what should you use instead? I don’t
think there is one right answer. Many people have thought hard about these problems, and proposed
terminology and models to help us understand problems. To learn about those ideas, you’ll have to go
deeper into the literature.</p>

<ul>
  <li>
    <p>A good starting point is Doug Terry’s paper in which he <a href="http://research.microsoft.com/pubs/157411/ConsistencyAndBaseballReport.pdf">explains various different levels of
eventual consistency using Baseball examples</a>. It’s very readable and clear, even if
(like me) you’re not American and have no clue about Baseball.</p>
  </li>
  <li>
    <p>If you’re interested in transaction isolation models (which is not the same as consistency of
distributed replicas, but somewhat related), my little project <a href="http://martin.kleppmann.com/2014/11/25/hermitage-testing-the-i-in-acid.html">Hermitage</a> may be relevant.</p>
  </li>
  <li>
    <p><a href="http://arxiv.org/pdf/1302.0309.pdf"><img src="/2014/11/isolation-levels.png" width="250" height="101" style="float: right; margin: 0 0 1em 1em;" /></a>The connections between
replica consistency, transaction isolation and availability are explored by <a href="http://arxiv.org/pdf/1302.0309.pdf">Peter Bailis et
al.</a> (That paper also explains the meaning of that hierarchy of consistency levels which
Kyle Kingsbury <a href="https://aphyr.com/posts/322-call-me-maybe-mongodb-stale-reads">likes to show</a>.)</p>
  </li>
  <li>
    <p>When you’ve read those, you should be ready to dive deeper into the literature. I’ve scattered
a ton of links to papers throughout this post. Do take a look at them: a number of experts have
already figured out a lot of stuff for you.</p>
  </li>
  <li>
    <p>As a last resort, if you can’t face reading the original papers, I suggest you take a look at
<a href="http://dataintensive.net/">my book</a>, which summarizes the most important ideas in an approachable manner. (See,
I tried <em>very hard</em> not to make this post a sales pitch.)</p>
  </li>
  <li>
    <p>If you want to know more specifically about using ZooKeeper correctly, 
<a href="http://shop.oreilly.com/product/0636920028901.do">Flavio Junqueira and Benjamin Reed’s book</a> is good.</p>
  </li>
</ul>

<p>Whatever way you choose to learn, I encourage you to be curious and patient – this stuff doesn’t
come easy. But it’s rewarding, because you learn to reason about trade-offs, and thus figure out
what kind of architecture works best for your particular application. But whatever you do, please
stop talking about CP and AP, because they just don’t make any sense.</p>

<p><em>Thank you to <a href="https://aphyr.com/">Kyle Kingsbury</a> and <a href="https://twitter.com/skamille">Camille Fournier</a>
for comments on a draft of this post. Any errors or unpalatable opinions are mine, of course.</em></p>

                ]]></content:encoded>
            </item>
        
            <item>
                <title>Bottled Water: Real-time integration of PostgreSQL and Kafka</title>
                <link>http://martin.kleppmann.com/2015/04/23/bottled-water-real-time-postgresql-kafka.html</link>
                <comments>http://martin.kleppmann.com/2015/04/23/bottled-water-real-time-postgresql-kafka.html#disqus_thread</comments>
                <pubDate>Thu, 23 Apr 2015 00:00:00 +0000</pubDate>
                <dc:creator>Martin Kleppmann</dc:creator>
                
                    <guid isPermaLink="true">http://martin.kleppmann.com/2015/04/23/bottled-water-real-time-postgresql-kafka.html</guid>
                
                <description><![CDATA[ This post was originally published on the Confluent blog. Writing to a database is easy, but getting the data out again is surprisingly hard. Of course, if you just want to query the database and get some results, that’s fine. But what if you want a copy of your database... ]]></description>
                <content:encoded><![CDATA[
                    <p><em>This post was originally published
<a href="http://blog.confluent.io/2015/04/23/bottled-water-real-time-integration-of-postgresql-and-kafka/">on the Confluent blog</a>.</em></p>

<p>Writing to a database is easy, but getting the data out again is surprisingly hard.</p>

<p>Of course, if you just want to query the database and get some results, that’s fine. But what if you
want a copy of your database contents in some other system — for example, to make it searchable in
Elasticsearch, or to pre-fill caches so that they’re nice and fast, or to load it into a data
warehouse for analytics, or if you want to migrate to a different database technology?</p>

<p>If your data never changed, it would be easy. You could just take a snapshot of the database (a full
dump, e.g. a backup), copy it over, and load it into the other system. The problem is that the data
in the database is constantly changing, and so the snapshot is already out-of-date by the time
you’ve loaded it. Even if you take a snapshot once a day, you still have one-day-old data in the
downstream system, and on a large database those snapshots and bulk loads can become very expensive.
Not really great.</p>

<p>So what do you do if you want a copy of your data in several different systems?</p>

<p>One option is for your application to do so-called “dual writes”. That is, every time your
application code writes to the database, it also updates/invalidates the appropriate cache entries,
reindexes the data in your search engine, sends it to your analytics system, and so on:</p>

<p><img src="/2015/04/bottledwater-01.png" alt="Application-managed dual writes" width="550" height="412" /></p>

<p>However, as I explain in <a href="http://martin.kleppmann.com/2014/10/28/staying-agile-at-span.html">one of my talks</a>,
the dual-writes approach is really problematic. It suffers from race conditions and reliability
problems. If slightly different data gets written to two different datastores (perhaps due to a bug
or a race condition), the contents of the datastores will gradually drift apart — they will become
more and more inconsistent over time. Recovering from such gradual data corruption is difficult.</p>

<p>If you rebuild a cache or index from a snapshot of a database, that has the advantage that any
inconsistencies get <a href="http://nathanmarz.com/blog/how-to-beat-the-cap-theorem.html">blown away</a> when
you rebuild from a new database dump. However, on a large database, it’s slow and inefficient to
process the entire database dump once a day (or more frequently). How could we make it fast?</p>

<p>Typically, only a small part of the database changes between one snapshot and the next. What if you
could process only a “diff” of what changed in the database since the last snapshot? That would also
be a smaller amount of data, so you could take such diffs more frequently. What if you could take
such a “diff” every minute? Every second? 100 times a second?</p>

<p>When you take it to the extreme, the changes to a database become a stream. Every time someone
writes to the database, that is a message in the stream. If you apply those messages to a database
in exactly the same order as the original database committed them, you end up with an exact copy of
the database. And if you think about it, this is exactly
<a href="http://blog.confluent.io/2015/03/04/turning-the-database-inside-out-with-apache-samza/">how database replication works</a>.</p>

<p>The replication approach to data synchronization works much better than dual writes. First, you
write all your data to one database (which is probably what you’re already doing anyway). Next, you
extract two things from that database:</p>

<ul>
  <li>a <strong>consistent snapshot</strong> at one point in time, and</li>
  <li>a <strong>real-time stream of changes</strong> from that point onwards.</li>
</ul>

<p>You can load the snapshot into the other systems (for example your search indexes or caches), and
then apply the real-time changes on an ongoing basis. If this pipeline is well tuned, you can
probably get a latency of less than a second, so your downstream systems remain very almost
up-to-date. And since the stream of changes provides ordering of writes, race conditions are
<a href="https://martin.kleppmann.com/2015/04/24/logs-for-data-infrastructure-at-craft.html">much less of a problem</a>.</p>

<p>This approach to building systems is sometimes called
<a href="http://en.wikipedia.org/wiki/Change_data_capture">Change Data Capture</a> (CDC), though the tools for
doing it are currently not very good. However, at some companies, CDC has become a key building
block for applications — for example, LinkedIn built <a href="http://www.socc2012.org/s18-das.pdf">Databus</a>
and Facebook built
<a href="https://code.facebook.com/posts/188966771280871/wormhole-pub-sub-system-moving-data-through-space-and-time/">Wormhole</a>
for this purpose.</p>

<p>I am excited about change capture because it allows you to unlock the value in the data you already
have. You can feed the data into a
<a href="http://blog.confluent.io/2015/02/25/stream-data-platform-1/">central hub of data streams</a>, where it
can readily be combined with event streams and data from other databases in real-time. This approach
makes it much easier to experiment with new kinds of analysis or data format, it allows gradual
migration from one system to another with minimal risk, and it is much more robust to data
corruption: if something goes wrong, you can always rebuild a datastore from the snapshot and the
stream.</p>

<p><img src="/2015/04/bottledwater-02.png" alt="Using change capture to drive derived data stores" width="550" height="412" /></p>

<h2 id="getting-the-real-time-stream-of-changes">Getting the real-time stream of changes</h2>

<p>Getting a consistent snapshot of a database is a common feature, because you need it in order to
take backups. But getting a real-time stream of changes has traditionally been an overlooked feature
of databases. Oracle
<a href="http://www.oracle.com/us/products/middleware/data-integration/goldengate/overview/index.html">GoldenGate</a>, the
<a href="https://dev.mysql.com/doc/refman/5.7/en/binary-log.html">MySQL binlog</a>, the
<a href="http://www.manuel-schoebel.com/blog/meteorjs-and-mongodb-replica-set-for-oplog-tailing">MongoDB oplog</a> or the
<a href="http://guide.couchdb.org/draft/notifications.html">CouchDB changes feed</a> do something like
this, but they’re not exactly easy to use correctly. More recently, a few databases such as
<a href="http://rethinkdb.com/blog/realtime-web/">RethinkDB</a> or
<a href="https://www.firebase.com/docs/web/guide/retrieving-data.html">Firebase</a> have oriented themselves
towards real-time change streams.</p>

<p>However, today we will talk about <strong>PostgreSQL</strong>. It’s an old-school database, but it’s good. It is
very stable, has good performance, and is <a href="https://vimeo.com/61044807">surprisingly full-featured</a>.</p>

<p>Until recently, if you wanted to get a stream of changes from Postgres, you had to use triggers.
This is possible (see below), but it is fiddly, requires schema changes and doesn’t perform very
well. However, Postgres 9.4 (released in December 2014) introduced a new feature that changes
everything: <a href="http://www.postgresql.org/docs/9.4/static/logicaldecoding.html">logical decoding</a>
(which I explain in more detail below).</p>

<p>With logical decoding, change data capture for Postgres suddenly becomes much more appealing. So,
when this feature was released, I set out to build a change data capture tool for Postgres that
would take advantage of the new facilities. <a href="http://confluent.io/">Confluent</a> sponsored me to work
on it (thank you Confluent!), and today we are releasing an alpha version of this tool as open
source. It is called <a href="https://github.com/confluentinc/bottledwater-pg">Bottled Water</a>.</p>

<p><img src="/2015/04/bottledwater-03.png" alt="Bottled Water: Data streams freshly bottled at source" width="550" height="412" /></p>

<h2 id="introducing-bottled-water">Introducing Bottled Water</h2>

<p>Logical decoding takes the database’s write-ahead log (WAL), and gives us access to row-level change
events: every time a row in a table is inserted, updated or deleted, that’s an event. Those events
are grouped by transaction, and appear in the order in which they were committed to the database.
Aborted/rolled-back transactions do not appear in the stream. Thus, if you apply the change events
in the same order, you end up with an exact, transactionally consistent copy of the database.</p>

<p>The Postgres logical decoding is well designed: it even creates a consistent snapshot that is
coordinated with the change stream. You can use this snapshot to make a point-in-time copy of the
entire database (without locking — you can continue writing to the database while the copy is being
made), and then use the change stream to get all writes that happened since the snapshot.</p>

<p>Bottled Water uses these features to copy all the data in a database, and encodes it in the
efficient binary <a href="http://avro.apache.org/">Avro format</a>. The encoded data is sent to
<a href="http://kafka.apache.org/">Kafka</a> — each table in the database becomes a Kafka topic, and each row
in the database becomes a message in Kafka.</p>

<p>Once the data is in Kafka, you can easily write a Kafka consumer that does whatever you need: send
it to Elasticsearch, or populate a cache, or process it in a <a href="http://samza.apache.org/">Samza</a> job,
or load it into HDFS with <a href="http://confluent.io/docs/current/camus/docs/intro.html">Camus</a>… the
possibilities are endless.</p>

<h2 id="why-kafka">Why Kafka?</h2>

<p>Kafka is a messaging system, best known for transporting high-volume activity events, such as web
server logs and user click events. In Kafka, such events are typically retained for a certain time
period and then discarded. Is Kafka really a good fit for database change events? We don’t want
database data to be discarded!</p>

<p>In fact, Kafka is a perfect fit — the key is Kafka’s
<a href="http://kafka.apache.org/documentation.html#compaction">log compaction feature</a>, which was designed
precisely for this purpose. If you enable log compaction, there is no time-based expiry of data.
Instead, every message has a key, and Kafka retains the latest message for a given key indefinitely.
Earlier messages for a given key are eventually garbage-collected. This is quite similar to new
values overwriting old values in a key-value store.</p>

<p>Bottled Water identifies the primary key (or
<a href="http://michael.otacoo.com/postgresql-2/postgres-9-4-feature-highlight-replica-identity-logical-replication/">replica identity</a>)
of each table in Postgres, and uses that as the key of the messages sent to Kafka. The value of the
message depends on the kind of event:</p>

<ul>
  <li>For inserts and updates, the message value contains all of the row’s fields, encoded as Avro.</li>
  <li>For deletes, the message value is set to null. This causes Kafka to remove the message during log
compaction, so its disk space is freed up.</li>
</ul>

<p>With log compaction, you don’t need one system to store the snapshot of the entire database and
another system for the real-time messages — they can live perfectly well within the same system.
Bottled Water writes the initial snapshot to Kafka by turning every single row in the database into
a message, keyed by primary key, and sending them all to the Kafka brokers. When the snapshot is
done, every row that is inserted, updated or deleted similarly turns into a message.</p>

<p>If a row frequently gets updated, there will be many messages with the same key (because each update
turns into a message). Fortunately, Kafka’s log compaction will sort this out, and garbage-collect
the old values, so that we don’t waste disk space. On the other hand, if a row never gets updated or
deleted, it just stays unchanged in Kafka forever — it never gets garbage-collected.</p>

<p>Having the full database dump and the real-time stream in the same system is tremendously powerful.
If you want to rebuild a downstream database from scratch, you can start with an empty database,
start consuming the Kafka topic from the beginning, and scan through the whole topic, writing each
message to your database. When you reach the end, you have an up-to-date copy of the entire
database. What’s more, you can continue keeping it up-to-date by simply continuing to consume the
stream. Building alternative views onto your data was never easier!</p>

<p>The idea maintaining a copy of your database in Kafka surprises people who are more familiar with
traditional enterprise messaging and its limitations. Actually, this use case is exactly why Kafka
is built around a
<a href="https://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying">replicated log abstraction</a>:
it makes this kind of large-scale data retention and distribution possible. Downstream systems can
reload and re-process data at will, without impacting the performance of the upstream database that
is serving low-latency queries.</p>

<h2 id="why-avro">Why Avro?</h2>

<p>The data extracted from Postgres could be encoded as JSON, or Protobuf, or Thrift, or any number of
formats. However, I believe Avro is the best choice. Gwen Shapira has written about the
<a href="http://radar.oreilly.com/2014/11/the-problem-of-managing-schemas.html">advantages of Avro</a> for
schema management, and I’ve got a
<a href="http://martin.kleppmann.com/2012/12/05/schema-evolution-in-avro-protocol-buffers-thrift.html">blog post</a>
comparing it to Protobuf and Thrift. The
<a href="http://blog.confluent.io/2015/02/25/stream-data-platform-2/">Confluent stream data platform guide</a>
gives some more reasons why Avro is good for data integration.</p>

<p>Bottled Water inspects the schema of your database tables, and automatically generates an Avro
schema for each table. The schemas are automatically registered with
<a href="http://confluent.io/docs/current/schema-registry/docs/index.html">Confluent’s schema registry</a>,
and the schema version is embedded in the messages sent to Kafka. This means it “just works” with
the stream data platform’s
<a href="http://confluent.io/docs/current/schema-registry/docs/serializer-formatter.html">serializers</a>: you
can work with the data from Postgres as meaningful application objects and rich datatypes, without
writing a lot of tedious parsing code.</p>

<p>The translation of Postgres datatypes into Avro is already fairly comprehensive, covering all the
common datatypes, and providing a lossless and sensibly typed conversion. I intend to extend it to
support all of Postgres’ built-in datatypes (of which there are many!) — it’s some effort, but it’s
worth it, because good schemas for your data are tremendously important.</p>

<p><img src="/2015/04/bottledwater-04.png" alt="Inside the bottle factory" width="550" height="316" /></p>

<h2 id="the-logical-decoding-output-plugin">The logical decoding output plugin</h2>

<p>An interesting property of Postgres’ logical decoding feature is that it does not define a wire
format in which change data is sent over the network to a consumer. Instead, it defines an
<a href="http://www.postgresql.org/docs/9.4/static/logicaldecoding-output-plugin.html">output plugin API</a>,
which receives a function call for every insert, update or delete. Bottled Water uses this API to
read data in the database’s internal format, and serializes it to Avro.</p>

<p>The output plugin must be written in C using the Postgres extension mechanism, and loaded into the
database server as a shared library. This requires superuser privileges and filesystem access on the
database server, so it’s not something to be undertaken lightly. I understand that many a database
administrator will be scared by the prospect of running custom code inside the database server.
Unfortunately, this is the only way logical decoding can currently be used.</p>

<p>At the moment, the logical decoding plugin must be installed on the leader database. In principle,
it would be possible to have it run on a separate follower, so that it cannot impact other clients,
but the current implementation in Postgres does not allow this. This limitation will hopefully be
lifted in future versions of Postgres.</p>

<p><img src="/2015/04/bottledwater-05.png" alt="Bottled Water architecture" width="550" height="412" /></p>

<h2 id="the-client-daemon">The client daemon</h2>

<p>Besides the plugin (which runs inside the database server), Bottled Water consists of a client
program which you can run anywhere. It connects to the Postgres server and to the Kafka brokers,
receives the Avro-encoded data from the database, and forwards it to Kafka.</p>

<p>The client is also written in C, because it’s easiest to use the Postgres client libraries that way,
and because some code is shared between the plugin and the client. It’s fairly lightweight and
doesn’t need to write to disk.</p>

<p>What happens if the client crashes, or gets disconnected from either Postgres or Kafka? No problem.
It keeps track of which messages have been published and acknowledged by the Kafka brokers. When the
client restarts after an error, it replays all messages that haven’t been acknowledged. Thus, some
messages could appear twice in Kafka, but no data should be lost.</p>

<h2 id="related-work">Related work</h2>

<p>Various other people are working on similar problems:</p>

<ul>
  <li><a href="https://github.com/xstevens/decoderbufs">Decoderbufs</a> is an experimental Postgres plugin by
<a href="https://twitter.com/xstevens">Xavier Stevens</a> that decodes the change stream into a Protocol
Buffers format. It only provides the logical decoding plugin part of the story — it doesn’t have
the consistent snapshot or client parts (Xavier mentions he has written a client which reads from
Postgres and writes to Kafka, but it’s not open source).</li>
  <li><a href="https://github.com/xstevens/pg_kafka">pg_kafka</a> (also from Xavier) is a Kafka producer client in
a Postgres function, so you could potentially produce to Kafka from a trigger.</li>
  <li><a href="https://wiki.postgresql.org/wiki/PGQ_Tutorial">PGQ</a> is a Postgres-based queue implementation, and
<a href="https://wiki.postgresql.org/wiki/SkyTools">Skytools Londiste</a> (developed at Skype) uses it to
provide trigger-based replication. <a href="https://bucardo.org/wiki/Bucardo">Bucardo</a> is another
trigger-based replicator. I get the impression that trigger-based replication is somewhat of
a hack, requiring schema changes and fiddly configuration, and incurring significant overhead.
Also, none of these projects seems to be endorsed by the PostgreSQL core team, whereas logical
decoding is fully supported.</li>
  <li><a href="http://sqoop.apache.org/">Sqoop</a> recently added support for
<a href="https://issues.apache.org/jira/browse/SQOOP-1852">writing to Kafka</a>. To my knowledge, Sqoop can
only take full snapshots of a database, and not capture an ongoing stream of changes. Also, I’m
unsure about the transactional consistency of its snapshots.</li>
  <li>For those using MySQL, <a href="https://twitter.com/lorax_james">James Cheng</a> has put together a list of
<a href="https://github.com/wushujames/mysql-cdc-projects/wiki">change capture projects</a> that get data
from MySQL into Kafka. AFAIK, they all focus on the binlog parsing piece and don’t do the
consistent snapshot piece.</li>
</ul>

<h2 id="status-of-bottled-water">Status of Bottled Water</h2>

<p>At present, Bottled Water is alpha-quality software. It’s more than a proof of concept — quite a bit
of care has gone into its design and implementation — but it hasn’t yet been tested in any
real-world scenarios. It’s definitely not ready for production use right now, but with some testing
and tweaking it will hopefully become production-ready in future.</p>

<p>We’re releasing it as open source now in the hope of getting feedback from the community. Also,
a few people who heard I was working on this have been bugging me to release it :-)</p>

<p>The <a href="https://github.com/confluentinc/bottledwater-pg/blob/master/README.md">README</a> has more
information on how to get started. Please let us know how you get on! Also, I’ll be talking more
about Bottled Water at
<a href="http://berlinbuzzwords.de/session/change-data-capture-magic-wand-we-forgot">Berlin Buzzwords</a> in
June — hope to see you there.</p>

                ]]></content:encoded>
            </item>
        
            <item>
                <title>Real-time full-text search with Luwak and Samza</title>
                <link>http://martin.kleppmann.com/2015/04/13/real-time-full-text-search-luwak-samza.html</link>
                <comments>http://martin.kleppmann.com/2015/04/13/real-time-full-text-search-luwak-samza.html#disqus_thread</comments>
                <pubDate>Mon, 13 Apr 2015 00:00:00 +0000</pubDate>
                <dc:creator>Martin Kleppmann</dc:creator>
                
                    <guid isPermaLink="true">http://martin.kleppmann.com/2015/04/13/real-time-full-text-search-luwak-samza.html</guid>
                
                <description><![CDATA[ This is an edited transcript of a talk given by Alan Woodward and Martin Kleppmann at FOSDEM 2015. It was originally published on the Confluent blog, and has been translated into Korean. Traditionally, search works like this: you have a large corpus of documents, and users write ad-hoc queries to... ]]></description>
                <content:encoded><![CDATA[
                    <p>This is an edited transcript of a
<a href="https://fosdem.org/2015/schedule/event/searching_over_streams_with_luwak_and_apache_samza/">talk</a>
given by <a href="https://twitter.com/romseygeek">Alan Woodward</a> and
<a href="https://martin.kleppmann.com/">Martin Kleppmann</a> at <a href="https://fosdem.org/2015/">FOSDEM 2015</a>.
It was originally published
<a href="http://blog.confluent.io/2015/04/13/real-time-full-text-search-with-luwak-and-samza/">on the Confluent blog</a>,
and has been
<a href="http://www.slideshare.net/myunghyunlee/realtime-fulltext-search-with-luwak-and-samza">translated into Korean</a>.</p>

<p><em>Traditionally, search works like this: you have a large corpus of documents, and users write ad-hoc
queries to find documents within that corpus. Documents may change from time to time, but on the
whole, the corpus is fairly stable.</em></p>

<p><em>However, with fast-changing data, it can be useful to turn this model on its head, and search over
a stream of documents as they appear. For example, companies may want to detect whenever they are
mentioned in a feed of news articles, or a Twitter user may want to see a continuous stream of
tweets for a particular hashtag.</em></p>

<p><em>In this talk, we describe open source tools that enable search on streams:
<a href="https://github.com/flaxsearch/luwak">Luwak</a> is a <a href="http://lucene.apache.org/">Lucene</a>-based library
for running many thousands of queries over a single document, with optimizations that make this
process efficient. <a href="http://samza.apache.org/">Samza</a> is a stream processing framework based on
<a href="http://kafka.apache.org/">Kafka</a>, allowing real-time computations to be distributed across
a cluster of machines. We show how Luwak and Samza can be combined into an efficient and scalable
streaming search engine.</em></p>

<p><img src="/2015/04/streamsearch-01.png" alt="Searching over streams with Luwak &amp; Samza" width="550" height="412" /></p>

<p>In this talk we’re going to discuss some work that we’ve been doing in the area of full-text search
on streams. Perhaps you already know about normal search engines like Elasticsearch and Solr, but as
we’ll see, searching on streams is quite a different problem, with some interesting challenges.</p>

<p>Searching on streams becomes important when you’re dealing with real-time data that is rapidly
changing. We’ll see some examples later of when you might need it.</p>

<p><img src="/2015/04/streamsearch-02.png" alt="What is a stream?" width="550" height="197" /></p>

<p>But first of all, we should define what we mean with a stream. For our purposes, we’ll say that
a stream is an append-only, totally ordered sequence of records (also called events or messages).
For example, a log file is a stream: each record is a line of text with some structure, perhaps some
metadata like a timestamp or severity, perhaps an exception stack trace. Every log record is
appended to the end of the file.</p>

<p><img src="/2015/04/streamsearch-03.png" alt="Appending to a log, and tailing it" width="550" height="243" /></p>

<p>There are a few ways you can read the content of a stream. For example, you can start at the
beginning of the file and read the entire file sequentially. Or you can use <code>tail -f</code> to watch the
file for any new records that are appended, and be notified when new data appears.</p>

<p>We call a process that writes to a stream a <em>“producer”</em>, and a process that reads from the stream
a <em>“consumer”</em>.</p>

<p><img src="/2015/04/streamsearch-04.png" alt="How do you search a stream?" width="550" height="239" /></p>

<p>Now say you’ve got some data in a stream, such as a log file, and you want to do full-text search on
it. How do you go about doing that?</p>

<p><img src="/2015/04/streamsearch-05.png" alt="Put the contents of a log file in an index" width="550" height="412" /></p>

<p>The traditional approach is to load everything into a big search index, perhaps something like
Elasticsearch or Solr. <a href="http://www.elasticsearch.org/overview/elkdownloads/">ELK</a> (Elasticsearch,
Logstash and Kibana) is a currently trendy way of setting this up. That way you have the entire
history of the stream searchable, and people can write any queries they want to search the index.</p>

<p><img src="/2015/04/streamsearch-06.png" alt="Partitioned indexes for different time periods" width="550" height="412" /></p>

<p>But what happens as new records are appended to the stream? You need to add them to an index in
order to make them searchable. For example, you could imagine creating different indexes for
different time periods: one for historical data, one for yesterday, one for the last hour, one for
the last minute…</p>

<p>And this is basically what people mean when they talk about “near-real-time” search: create an index
for documents that appeared very recently, and send any queries to that index as well as the older
historical indexes.</p>

<p>Let’s talk about some examples of this kind of search in practice.</p>

<p><img src="/2015/04/streamsearch-07.png" alt="Example: Twitter search" width="550" height="431" /></p>

<p>Take Twitter, for example. If you type something in the search box, you’ll see a list of tweets that
match your search query, ranked by recency. The index includes
<a href="https://blog.twitter.com/2014/building-a-complete-tweet-index">all public tweets ever written</a>,
broken down my time period, similar to the diagram above.</p>

<p>But if you stay on that page for a while, notice that something happens: a bar appears at the top of
the page, saying that there are new results for your query. What happened here? When you typed your
search query, it seems that Twitter didn’t forget about the query the moment they returned the
results to you. Rather, they must have <em>remembered</em> the query, and <em>continued to search the stream</em>
of tweets for any new matches for your query. When new matches appear in the stream, they send
a notification to your browser.</p>

<p>In this case, the stream we’re searching is Twitter’s so-called <em>firehose</em> of Tweets. I don’t know
how they’ve implemented that. Perhaps they group tweets into batches — say, create an index for 10
seconds worth of tweets, and then run the queries from all open search sessions against that index.
But somehow they are doing full-text search on a stream.</p>

<p><img src="/2015/04/streamsearch-08.png" alt="Example: Google alerts" width="550" height="277" /></p>

<p>Another example is Google Alerts. This is a feature of Google where you can register some search
queries with their system, and they send you an email notification when new web pages matching your
query are published. For example, you might set up an alert for your name or company name, so that
you find out when people write stuff about you.</p>

<p>Google internally has a stream of new web pages being discovered by its crawler, and Google Alerts
allows you to register a query against that stream. Google remembers the query, and runs the query
against every new document that is discovered and added to its index.</p>

<p><img src="/2015/04/streamsearch-09.png" alt="Comparing after-the-fact search and streaming search" width="550" height="412" /></p>

<p>So it seems that we can divide search into two categories:</p>

<ul>
  <li>In one case, you put all the documents in a big index, and people can search that index by writing
ad-hoc queries. We could call that <em>“after-the-fact search”</em>, because it’s searching a repository
of historical documents that we received at some point in the past.</li>
  <li>In the other case, you register the queries in advance, and then the system checks each document
that appears on a stream, to see whether it matches any of the registered queries. This is
<em>streaming search</em>.</li>
</ul>

<p>It often makes sense to combine these two: for example, in the Twitter case, both types of search
are happening. You first get after-the-fact search results from the last 7 days, but while you have
the search page open, your query is also registered for a stream search, so that you can follow the
stream of tweets that match your query.</p>

<p>You might have seen this pattern in
<a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-percolate.html">Elasticsearch Percolator</a>,
for example. The streaming search approach we’re describing here is similar to Percolator, but we
think that it will scale better.</p>

<p><img src="/2015/04/streamsearch-10.png" alt="One document, many queries" width="550" height="412" /></p>

<p>So, how do you actually implement streaming search? Well, we saw earlier that for near-real-time
search, you construct a small index for recently added documents. We can take that approach to the
extreme: for each new document that appears on a stream, we create a new index, containing just that
one document. Then we can run through all of the registered queries, test each query against the
one-document index, and output a list of all the queries that match the new document. A downstream
system can then take those matches and notify the owners of the queries.</p>

<p><img src="/2015/04/streamsearch-11.png" alt="Does it scale?" width="550" height="412" /></p>

<p>However, the question is: how efficient is this going to be when you have lots of queries, or very
complex queries? If you only have a few hundred registered queries, you can pretty quickly run
through all of those queries for every new document on the stream — but if you have hundreds of
thousands of queries, it can get quite slow.</p>

<p>Also, if you have very complex queries, even just executing a single query can take a long time.
I (Alan) work with some clients who provide
<a href="http://en.wikipedia.org/wiki/Media_monitoring">media monitoring</a> services (also known as <em>clipping
companies</em>). They collect feeds of newspaper articles and other news from around the world, and
their clients are companies who want to know whenever they are mentioned in the news. The media
monitoring companies construct one big query for each client, and those queries can become really
huge — a query might be hundreds of kilobytes long! They contain a large number of terms, have lots
of nested boolean operators, and lots of exclusions (negated search terms).</p>

<p>To give just one example, in the UK there’s a magazine called “<a href="http://www.which.co.uk/">Which?</a>”.
If you simply search for the term “which”, you match a huge number of documents, since that’s such
a common word in English. They have to construct really complex queries to filter out most of the
noise.</p>

<p>So, if you have a large number of queries, or very complex queries, the streaming search becomes
slow. We need to find ways of optimizing that. Observe this: the fastest query is a query that you
never execute. So, if we can figure out which queries are definitely <em>not</em> going to match a given
document, we can skip those queries entirely, and potentially save ourselves a lot of work.</p>

<p><img src="/2015/04/streamsearch-12.png" alt="Flax Luwak" width="550" height="281" /></p>

<p>Which brings us to <a href="https://github.com/flaxsearch/luwak">Luwak</a>, a library that
<a href="http://www.flax.co.uk/">we (Flax)</a> wrote in order to do efficient streaming search. Luwak is open
source and builds upon Apache Lucene. It works the other way round from a normal search index: with
a normal index, you first add documents to the index, and then you query it. Luwak turns this on its
head: you first register queries with Luwak, and then match documents against them. Luwak tells you
which of the registered queries match the document.</p>

<p>Let’s go a bit into the detail of how Luwak optimizes this matching process.</p>

<p><img src="/2015/04/streamsearch-13.png" alt="Index of queries" width="550" height="412" /></p>

<p>As I said, we want some way of figuring out which queries are definitely <em>not</em> going to match
a document, so that we don’t need to bother executing those queries. In order to do this, we can do
something unusual: we can index the <em>queries</em>! In search engines you normally index documents, but
in this case we’re going to index the queries.</p>

<p>Let’s say we have three registered queries: Q1 is <code>“WHEELS” NEAR “BUS”</code>, Q2 is <code>“WHEELS” NEAR
“CAR”</code>, and Q3 is <code>“WHEELS” OR “BUMPERS”</code>. First observe this: in a conjunction query (that is,
a query like A AND B AND C), <em>all</em> the search terms must appear in the document for the query to
match. (An operator like NEAR is a specialized form of AND that has an additional proximity
restriction.) For example, a document must contain both “wheels” and “bus” in order to match Q1; if
a document doesn’t contain the word “bus”, there’s no chance it can match Q1.</p>

<p>That means, we can arbitrarily pick one term from a conjunction and check whether the document
contains that term. If the document doesn’t contain the term, we can be sure that the document won’t
match the conjunction either.</p>

<p>On the other hand, a disjunction query (with an OR operator, like Q3 for example) matches if <em>any</em>
of the search terms appear in the document. For example, if a document doesn’t contain “wheels”, it
may nevertheless match Q3 if it contains “bumpers”. In this case, we must extract <em>all</em> of the terms
from the disjunction; if any one of those terms appears in the document, we have to test the full
query.</p>

<p>We can now take those terms that we extracted from queries (for example “bus” from Q1, “car” from
Q2, and “bumpers” and “wheels” from Q3), and build an index of those terms. As I said, this is an
index of <em>queries</em>, not of documents. The index is a dictionary from terms to queries: it maps terms
to queries containing that term.</p>

<p><img src="/2015/04/streamsearch-14.png" alt="Document disjunction" width="550" height="412" /></p>

<p>Now that we’ve taken all our registered queries and indexed them, we can move on to the next step:
processing the stream of documents. For each document, we want to find all the queries that match.
How do we go about doing this?</p>

<p>The trick is to take each document, and turn it into a <em>query</em>. (Previously we created an index of
queries. Now we’re turning a document into a query. How upside-down!) Namely, we take all the words
(terms) that appear in the document, and construct a disjunction (OR) query from all of those words.
Intuitively, this is saying: “find me all the queries that match any of the words in this document”.
 Creating an inverted index from a single document automatically gives us this list of terms.</p>

<p><img src="/2015/04/streamsearch-15.png" alt="Selecting candidate queries" width="550" height="412" /></p>

<p>Now that we have created an index of queries, and turned a document into a query, we can figure out
which queries match the document. First, we run the document disjunction query against the index of
queries. This will tell us which queries <em>may</em> match the document.</p>

<p>In our example, we created the query index by extracting the term “bus” from Q1, the term “car” from
Q2, and the terms “bumpers” and “wheels” from Q3. Also, we turned the document “The wheels on the
bus go round and round” into the disjunction query:</p>

<pre><code>“and” OR “bus” OR “go” OR “on” OR “round” OR “the” OR “wheels”
</code></pre>

<p>Running that disjunction query against the query index, we get a hit on the terms “bus” (Q1) and
“wheels” (Q3), but the terms “bumpers” (Q3) and “car” (Q2) don’t appear in the document. Therefore
we can conclude that Q1 and Q3 <em>might</em> match the document, but Q2 definitely <em>doesn’t</em> match the
document.</p>

<p>The next step is then to run queries Q1 and Q3 against the document index, to see whether they
really do match. But we don’t need to run Q2, because we’ve already established that it definitely
doesn’t match.</p>

<p>This whole process of indexing queries may seem a bit complicated, but it is a really powerful
optimization if you have a large number of queries. It can cut out 99% of the queries you would
otherwise have to execute, and thus massively speed up searching on streams. As I said, the fastest
query is one that you never even execute.</p>

<p><img src="/2015/04/streamsearch-16.png" alt="Query decomposition" width="550" height="412" /></p>

<p>Besides indexing queries, there are other optimizations we can make. One particular optimization
that we’ve found useful: if you have a big query that contains an OR operator at the top level, you
can break that big query into smaller ones. That is especially useful if one of the subqueries is
simple (fast to execute), and another one is complex (slow to execute).</p>

<p>Say we have a document that will match on the simple subquery, but not on the complex one.  In the
normal case, the whole query is run against the document, so we still pay the price for executing
the complex subquery, even though it doesn’t match. If we decompose it into its constituent parts,
however, then only the simple subquery will be selected, and we can avoid the performance hit of
running the complex one.</p>

<p><img src="/2015/04/streamsearch-17.png" alt="Term frequency analysis and phrase query analysis" width="550" height="260" /></p>

<p>We said earlier that when you’re indexing the queries, you can make some arbitrary choices about
which terms to extract. For example, for the query <em>“car” AND “bumpers”</em>, you could choose either
“car” or “bumpers” as the term to use in the query index. Which one should you choose?</p>

<p>It’s helpful to know how often each term occurs in your documents. For example, perhaps “car” is
quite a common term, but “bumpers” is much more rare. In that case, it would be better to use
“bumpers” in the query index, because it’s less likely to appear in documents. Only the small number
of documents containing the term “bumpers” would then need to be matched against the query <em>“car”
AND “bumpers”</em>, and you save yourself the effort of executing the query for the large number of
documents that contain “car” but not “bumpers”.</p>

<p><img src="/2015/04/streamsearch-18.png" alt="Query term extraction" width="550" height="412" /></p>

<p>Another, more advanced optimization technique considers several different possibilities of
extracting terms from queries. Take the query tree above, containing five terms and four boolean
operators. Remember the rule for extracting terms that we established earlier: extract any one of
the children of a conjunction (AND), but extract all of the children of a disjunction (OR). This
means there are three different combinations of terms that you could extract from the above query
tree:</p>

<p><img src="/2015/04/streamsearch-19.png" alt="Three ways of extracting terms from a query" width="550" height="107" /></p>

<p>Say you have a document “term1 term2 term3”: this document does not match the query (because neither
of the required term4 or term5 appears). However, in the first two combinations above (term1 and
term2 extracted, or term1 and term3 extracted), the document would nevertheless be selected to be
matched against the query. In the third combination above (term4 and term5 extracted), the document
wouldn’t be selected, because we can tell from the query index that it is definitely not going to
match the query.</p>

<p>Can we make the query pre-selection more precise? Yes, we can! Rather than just extracting one
(arbitrary) set of terms from the query, we can extract <em>several</em> sets of terms from the same query,
like the three above, and index them into separate fields (let’s call them _a, _b and _c). You then
run your document disjunction against <em>all</em> those fields, and the set of queries you need to run is
the intersection of those results — a conjunction of disjunctions, if you like.</p>

<p>The document “term1 term2 term3”, which we previously turned into a simple disjunction of terms, now
turns into something like this:</p>

<pre><code>_a:(term1 OR term2 OR term3) AND
_b:(term1 OR term2 OR term3) AND
_c:(term1 OR term2 OR term3)
</code></pre>

<p>The first two terms of the conjunction match, but the third doesn’t, and so we don’t select this
query. It’s still an approximation — you still need to execute the full query to be sure whether it
matches or not — but with these optimizations you can further reduce the number of queries you need
to execute.</p>

<p>Fortunately, Luwak has implemented all of these optimizations already, and they’re available for you
to use today.</p>

<p>However, as described so far, Luwak runs on a single machine. At some point, you may have so many
queries or such high throughput of documents that a single machine is not enough, even after you
have applied all of these optimizations. Which brings us to the second half of this talk: scaling
stream search across a cluster of machines.</p>

<p><em>(At this point, <a href="http://martin.kleppmann.com/">Martin</a> took over from
<a href="https://twitter.com/romseygeek">Alan</a>)</em></p>

<p><img src="/2015/04/streamsearch-20.png" alt="Kafka and Samza" width="550" height="412" /></p>

<p>Rather than inventing our own distributed computing framework — which would be likely to go wrong,
because distributed systems are hard — we’re going to build on a robust foundation. We’re going to
use <a href="http://kafka.apache.org/">Apache Kafka</a> and <a href="http://samza.apache.org/">Apache Samza</a>, two open
source projects that originated at LinkedIn.</p>

<p>I’ll start by giving a bit of background about Kafka, and then talk about how we embedded Luwak
inside Samza in order to scale out search on streams.</p>

<p><img src="/2015/04/streamsearch-21.png" alt="A partitioned stream in Kafka" width="550" height="412" /></p>

<p>Kafka is a kind of <em>message broker</em> or <em>message queue</em> — that is, it takes messages that originate
in one process (a <em>producer</em>), and delivers them to another process (a <em>consumer</em>). It does so in
a scalable, fault-tolerant manner.</p>

<p>The way it works is simple but remarkably effective. You can imagine Kafka as one big, append-only
file. Whenever a producer wants to send a new message to a stream, it simply appends it to the end
of the file. That’s the only way how you can write to Kafka: by appending to the end of a file.</p>

<p>A consumer is like a <code>tail -f</code> on that file, just like what we described at the beginning of this
talk. Each consumer reads the messages in the file sequentially, and each consumer has a current
<em>position</em> or <em>offset</em> in this file. Thus, it knows that all messages before that position have
already been read, and all messages after that position have not yet been read. This makes Kafka
very efficient: the brokers don’t have to keep track of which consumer has seen which messages,
because the consumers themselves just need to store their current position.</p>

<p>In order to scale across multiple machines, a Kafka stream is also <em>partitioned</em>. That means,
there’s not just one append-only file, but several. Each partition is completely independent from
the others, so different partitions can live on different machines.</p>

<p><img src="/2015/04/streamsearch-22.png" alt="Replication over Kafka broker nodes" width="550" height="412" /></p>

<p>In addition, Kafka provides replication, i.e. maintaining a copy of the same data on multiple
machines. This is important for fault tolerance — so that if one machine dies, you don’t lose any
data, and everything keeps running.</p>

<p>Kafka does this using a leader/follower model. Each partition of a stream has a leader on one broker
node, and a configurable number of followers on other broker nodes. All the new messages for
a partition go to its leader, and Kafka replicates them from the leader to the followers. If
a broker node goes down, and it was the leader for some partition, then one of the followers for
that partition becomes the new leader.</p>

<p>Every message in Kafka has a <em>key</em> and a <em>value</em> (which can be arbitrary byte strings). The key is
used for two different things: firstly, it determines which partition the message should go to (we
make sure that all the messages with the same key go to the same partition, by choosing the
partition based on a hash of the key). Secondly, it is used for <em>compaction</em>.</p>

<p><img src="/2015/04/streamsearch-23.png" alt="Kafka changelog compaction" width="550" height="412" /></p>

<p>Compaction is an exception to Kafka’s otherwise strict append-only model. You don’t have to use
compaction, but if you do turn it on, then Kafka keeps track of the keys in the stream. And if there
are several messages with the same key, then Kafka is allowed to throw away older messages with that
key — only the newest message for a given key is guaranteed to be retained.</p>

<p>In the picture above, there are originally three messages with key A, and compaction discards two of
them. In effect, this means that later messages can “overwrite” earlier messages with the same key.
This overwriting doesn’t happen immediately, but at some later time: compaction is done in
a background thread, a bit like garbage collection.</p>

<p>A Kafka stream with compaction is thus similar to a database with a key-value data model. If a key
is never overwritten, it is never discarded, so it stays in the stream forever. With compaction, we
can thus keep a complete history of key-value pairs:</p>

<p><img src="/2015/04/streamsearch-24.png" alt="Keeping complete history of events in Kafka" width="550" height="412" /></p>

<p>Without compaction, the stream would keep growing forever (the size of the stream is proportional to
the number of messages ever sent). But with compaction, the size of the stream is proportional to
the number of distinct keys — just like a database. If you can imagine storing all the keys and
values in a database, you can equally well store all the keys and values in a Kafka stream.</p>

<p>Why is this useful? Well, if we want to use Luwak in a reliable manner, there is a problem we need
to solve: when you register a query with Luwak, it is only kept in memory. Thus, whenever the Luwak
process is restarted, it needs to reload its list of queries from stable storage into memory.</p>

<p>You could use a database for this, but using a Kafka stream has a big advantage: Luwak can consume
the stream of queries, so it gets notified whenever someone registers a new query, modifies
a registered query, or unregisters (deletes) a query. We simply use the query ID as the message key,
and the query string as the message value (or a null value when a query is unregistered). And stream
compaction ensures that the query stream doesn’t get too big.</p>

<p><img src="/2015/04/streamsearch-25.png" alt="Re-processing historical events from stream" width="550" height="412" /></p>

<p>Now, whenever Luwak starts up, it can jump to the very beginning of the queries stream, and consume
it from beginning to end. All queries in that stream are loaded into memory, so that Luwak knows
what queries it should apply to documents. Only once it has finished consuming the stream, Luwaks
starts processing documents and matching them against queries. We call the query stream a
<a href="http://samza.apache.org/learn/documentation/0.8/container/streams.html#bootstrapping">bootstrap stream</a>,
because it’s used to bootstrap (initialize) the in-memory state of the stream consumer.</p>

<p>This brings us to <a href="http://samza.apache.org/">Samza</a>, a framework for writing stream processing jobs
on top of Kafka. A basic Samza job is very simple: you write a bit of code (implementing a Java
interface called <a href="http://samza.apache.org/learn/documentation/0.8/api/overview.html">StreamTask</a>)
and tell it which stream you want to consume, and Samza calls the process() method on your code for
every message that is consumed from the input stream. The code can do whatever it wants, including
sending messages to an output stream.</p>

<p><img src="/2015/04/streamsearch-26.png" alt="Samza takes a Kafka stream an input, produces another as output" width="550" height="412" /></p>

<p>As the input stream from Kafka is split into partitions, Samza creates a separate StreamTask for
each partition, and each StreamTask processes the messages in its corresponding partition
sequentially. Although a StreamTask only processes input from one partition, it can send output to
any partition of its output streams.</p>

<p>This partitioning model allows a Samza job to have two or more input streams, and “join” them
together:</p>

<p><img src="/2015/04/streamsearch-27.png" alt="Joining streams in Samza" width="550" height="412" /></p>

<p>By default, if a job has two input streams (say A and B), Samza sends partition 1 of stream A and
partition 1 of stream B to the same StreamTask 1; it sends partition 2 of A and partition 2 of B to
the same StreamTask 2; and so on. This is illustrated in the picture above. Note this only really
works if both input streams have the same number of partitions.</p>

<p>This allows the stream join to scale, by partitioning both input streams in the same way. For
example, if both streams are partitioned by user ID (i.e. using the user ID as the Kafka message
key), then you can be sure that all the messages for a particular user ID on both input streams are
routed to the same StreamTask. That StreamTask can then keep whatever state it needs in order to
join the streams together.</p>

<p>How do we bring full-text search on streams into this processing model?</p>

<p><img src="/2015/04/streamsearch-28.png" alt="Integration of Luwak with Samza" width="550" height="412" /></p>

<p>Alan and I hacked together a proof-of-concept called
<a href="https://github.com/romseygeek/samza-luwak">Samza-Luwak</a> to test using Luwak inside a Samza job. It
works as follows:</p>

<p>There are two input streams (Kafka topics): one for queries, and one for documents. The query stream
is a bootstrap stream with compaction, as described above. Whenever a user wants to register, modify
or unregister a query, they send a message to the queries stream. The Samza job consumes this
stream, and whenever a message appears, it updates Luwak’s in-memory roster of queries.</p>

<p>The documents stream contains the things that should be matched by the queries (the tweets if you’re
building Twitter search, the web pages from the crawler if you’re building Google Alerts, etc). The
Samza job also consumes the documents stream, and whenever a document appears, it is matched against
the index of registered queries, as described previously. It then sends a message to an output
stream, indicating which queries matched.</p>

<p>How do we distribute this matching process across multiple machines? The problem is that Samza’s
default partitioning model actually doesn’t do what we need. As I said previously, Samza by default
sends partition 1 of all the input streams to task 1, partition 2 of all the input streams to task
2, and so on:</p>

<p><img src="/2015/04/streamsearch-29.png" alt="Joining by co-partitioning two streams" width="550" height="412" /></p>

<p>This is a good partitioning model if you’re doing an
<a href="https://en.wikipedia.org/wiki/Join_(SQL)#Equi-join">equi-join</a>, because you can set up the
partitioning of the input streams such that each tasks only needs to know about its own input
partitions, and can ignore all the other partitions. This allows you to increase parallel processing
and scale the computation simply by creating more partitions.</p>

<p>However, full-text search is different. We’re not doing an equi-join on a particular field, we’re
trying to find matches involving arbitrarily complicated boolean expressions. In general, we don’t
know in advance which documents are going to match which queries. (The query index tells us
<em>approximately</em> which queries might match, but it’s not precise enough to use for partitioning.)</p>

<p>We still want to partition the query and document streams, because that will allow the system to
scale. But we also want to be able to match every document against every possible query. In other
words, we need to make sure that every query partition is joined with every document partition:</p>

<p><img src="/2015/04/streamsearch-31.png" alt="Cartesian product join" width="550" height="412" /></p>

<p>If you think about it, what we need is a
<a href="https://en.wikipedia.org/wiki/Cartesian_product">cartesian product</a> of query partitions and
document partitions. We want to create a separate StreamTask for every possible combination of
a query partition and a document partition. For example, in the picture above, StreamTask 8 is
responsible for handling query partition 4 and document partition 2.</p>

<p>This gives us exactly the semantics we need: every query is sent to multiple tasks (one task per
document partition), and conversely, every document is sent to multiple tasks (one task per query
partition). Each task can independently do its work on its own partitions, and afterwards you just
need to combine all the matches for each document. The dataflow is similar to the scatter-gather
approach you get in distributed search engines.</p>

<p>Unfortunately, this mode of streaming joins is not yet supported in Samza, but it’s being worked on
(you can track it under <a href="https://issues.apache.org/jira/browse/SAMZA-353">SAMZA-353</a> if you’re
interested). Once this feature is in place, you’ll be able to perform full-text search on streams at
arbitrary scale, simply by adding new partitions and adding more machines to the cluster. Combining
the clever indexing of Luwak with the scalability of Kafka and Samza — isn’t that cool?</p>

<p><em>If you want to play with Samza, there’s a quickstart project
“<a href="http://samza.apache.org/startup/hello-samza/0.8/">hello-samza</a>”, and you can find our
proof-of-concept integration of Samza and Luwak
<a href="https://github.com/romseygeek/samza-luwak">on Github</a>.</em></p>

<p><em><a href="https://twitter.com/romseygeek">Alan Woodward</a> is a director of <a href="http://www.flax.co.uk/">Flax</a>,
a consultancy specializing in open source search engines. He is a committer on
<a href="http://lucene.apache.org/">Lucene/Solr</a>, and developer of
<a href="https://github.com/flaxsearch/luwak">Luwak</a>. He previously worked on the enterprise search product
Fast ESP.</em></p>

<p><em><a href="https://martin.kleppmann.com/">Martin Kleppmann</a> is a committer on
<a href="http://samza.apache.org/">Apache Samza</a> and author of the upcoming O’Reilly book
<a href="http://dataintensive.net/">Designing Data-Intensive Applications</a>. He previously co-founded
<a href="https://rapportive.com/">Rapportive</a>, which was acquired by <a href="https://www.linkedin.com/">LinkedIn</a>.</em></p>

                ]]></content:encoded>
            </item>
        
            <item>
                <title>Turning the database inside-out with Apache Samza</title>
                <link>http://martin.kleppmann.com/2015/03/04/turning-the-database-inside-out.html</link>
                <comments>http://martin.kleppmann.com/2015/03/04/turning-the-database-inside-out.html#disqus_thread</comments>
                <pubDate>Wed, 04 Mar 2015 00:00:00 +0000</pubDate>
                <dc:creator>Martin Kleppmann</dc:creator>
                
                    <guid isPermaLink="true">http://martin.kleppmann.com/2015/03/04/turning-the-database-inside-out.html</guid>
                
                <description><![CDATA[ This is an edited and expanded transcript of a talk I gave at Strange Loop 2014. This transcript was originally published on the Confluent blog. The video recording (embedded below) has been watched over 8,000 times. For those of you who prefer reading, I thought it would be worth writing... ]]></description>
                <content:encoded><![CDATA[
                    <p>This is an edited and expanded transcript of a
<a href="/2014/09/18/turning-database-inside-out-at-strange-loop.html">talk</a>
I gave at <a href="https://thestrangeloop.com/archive/2014">Strange Loop 2014</a>.
This transcript was originally published on the
<a href="http://blog.confluent.io/2015/03/04/turning-the-database-inside-out-with-apache-samza/">Confluent blog</a>. The
<a href="https://www.youtube.com/watch?v=fU9hR3kiOK0&amp;list=PLeKd45zvjcDHJxge6VtYUAbYnvd_VNQCx">video recording</a>
(embedded below) has been watched over 8,000 times. For those of you who prefer reading, I thought
it would be worth writing down the talk.</p>

<iframe width="550" height="309" src="https://www.youtube.com/embed/fU9hR3kiOK0?rel=0" frameborder="0" allowfullscreen=""></iframe>

<p><em>Databases are global, shared, mutable state. That’s the way it has been since the 1960s, and no
amount of NoSQL has changed that. However, most self-respecting developers have got rid of mutable
global variables in their code long ago. So why do we tolerate databases as they are?</em></p>

<p><em>A more promising model, used in some systems, is to think of a database as an always-growing
collection of immutable facts. You can query it at some point in time — but that’s still old,
imperative style thinking. A more fruitful approach is to take the streams of facts as they come in,
and functionally process them in real-time.</em></p>

<p><em>This talk introduces Apache Samza, a distributed stream processing framework developed at LinkedIn.
At first it looks like yet another tool for computing real-time analytics, but it’s more than that.
Really it’s a surreptitious attempt to take the database architecture we know, and turn it inside
out.</em></p>

<p><em>At its core is a distributed, durable commit log, implemented by Apache Kafka. Layered on top are
simple but powerful tools for joining streams and managing large amounts of data reliably.</em></p>

<p><em>What do we have to gain from turning the database inside out? Simpler code, better scalability,
better robustness, lower latency, and more flexibility for doing interesting things with data. After
this talk, you’ll see the architecture of your own applications in a new light.</em></p>

<p><img src="/2015/03/insideout-01.png" alt="Turning the database inside-out with Apache Samza" width="550" height="363" /></p>

<p>This talk is about database architecture and application architecture. It’s somewhat related to an
open source project I’ve been working on, called <a href="http://samza.apache.org/">Apache Samza</a>. I’m
<a href="http://martin.kleppmann.com/">Martin Kleppmann</a>, and I was until recently at LinkedIn working on
Samza. At the moment I’m taking a sabbatical to write a book for O’Reilly, called
<a href="http://dataintensive.net/">Designing Data-Intensive Applications</a>.</p>

<p>Let’s talk about databases. What I mean is not any particular brand of database — I don’t mind
whether you’re using relational, or NoSQL, or whatever. I’m really talking about the general concept
of a database, as we use it when building applications.</p>

<p>Take, for example, the stereotypical web application architecture:</p>

<p><img src="/2015/03/insideout-02.png" alt="Three-tier architecture: client, backend, database" width="550" height="412" /></p>

<p>You have a client, which may be a web browser or a mobile app, and that client talks to some kind of
server-side system (which you may call a “backend” or whatever you like). The backend typically
implements some kind of business logic, performs access control, accepts input, produces output.
When the backend needs to remember something for the future, it stores that data in a database, and
when it needs to look something up, it queries a database. That’s all very familiar stuff.</p>

<p>The way we typically build these sorts of applications is that we make the backend layer
<em>stateless</em>. That has a lot of advantages: you can scale out the backend by just running more
processes in parallel, and you can route any request to any backend instance (they are all equally
well qualified to handle the request), so it’s easy to spread the load across multiple machines. Any
state that is required to handle a request will be looked up from the database on each request. That
also works nicely with HTTP, since HTTP is a stateless protocol.</p>

<p>However, the big problem with this approach is: the state has to go <em>somewhere</em>, and so we have to
put it in the database. We are now using the database as a kind of gigantic, global, shared, mutable
state. It’s like a global variable that’s shared between all your application servers. It’s exactly
the kind of horrendous thing that, in shared-memory concurrency, we’ve been trying to get rid of for
ages. <a href="https://en.wikipedia.org/wiki/Actor_model">Actors</a>,
<a href="http://clojure.com/blog/2013/06/28/clojure-core-async-channels.html">channels</a>,
<a href="https://gobyexample.com/goroutines">goroutines</a>, etc. are all attempts to get away from
shared-memory concurrency, avoiding the problems of locking, deadlock, concurrent modifications,
race conditions, and so on.</p>

<p>We’re trying to get away from shared-memory concurrency, but with databases we’re still stuck with
this big, shared, mutable state. So it’s worth thinking about this: if we’re trying to get rid of
shared memory in our single-process application architecture, what would happen if we tried to get
rid of this shared mutable state on a whole-system level?</p>

<p><img src="/2015/03/insideout-03.png" alt="It's always been that way" width="550" height="202" /></p>

<p>At the moment, it seems to me that the main reason why systems are still being built with mutable
databases is just inertia: that’s the way we’ve building applications for decades, and we don’t
really have good tools to do it differently. So, let’s think about what other possibilities we have
for building stateful systems.</p>

<p>In order to try to figure out what routes we could take, I’d like to look at four different examples
of things that databases currently do, and things that we do with databases. And these four examples
might give us an indicator of the directions in which we could take these systems forward in future.</p>

<p><img src="/2015/03/insideout-04.png" alt="Title: 1. Replication" width="550" height="172" /></p>

<p>The first example I’d like to look at is <em>replication</em>. You probably know about the basics of
replication: the idea is that you have a copy of the same data on multiple machines (nodes), so that
you can serve reads in parallel, and so that the system keeps running if you lose a machine.</p>

<p>It’s the database’s job to keep those replicas in sync. A common architecture for replication is
that you send your writes to one designated node (which you may call the <em>leader</em>, <em>master</em> or
<em>primary</em>), and it’s the leader’s responsibility to ensure that the writes are copied to the other
nodes (which you may call <em>followers</em>, <em>slaves</em> or <em>standbys</em>). There are also other ways of doing
it, but leader-based replication is familiar — many systems are built that way.</p>

<p>Let’s look at an example of replication to see what’s actually happening under the hood. Take
a shopping cart, for instance.</p>

<p><img src="/2015/03/insideout-05.png" alt="Shopping cart example" width="550" height="412" /></p>

<p>This is using a relational data model, but the same principles apply with other data models too. Say
you have a table with three columns: customers, products, and quantity. Each row indicates that
a particular customer has a particular quantity of a particular product in their shopping cart.</p>

<p>Now say customer 123 changes their mind, and instead of wanting quantity 1 of product 999, they
actually want quantity 3 of that product. So they issue an <em>update</em> query to the database, which
matches the row for customer 123 and product 999, and it changes the value of the quantity column
from 1 to 3.</p>

<p><img src="/2015/03/insideout-06.png" alt="Updating quantity of item in the shopping cart" width="550" height="412" /></p>

<p>The result is that the database overwrites the quantity value with the new value, i.e. it applies
the update in the appropriate place.</p>

<p>Now, I was talking about replication. What does this update do in the context of replication? Well,
first of all, you send this update query to your leader, and it executes the query, figures out
which rows match the condition, and applies the write locally:</p>

<p><img src="/2015/03/insideout-07.png" alt="Replicating a write from a leader to a follower" width="550" height="412" /></p>

<p>Now, how does this write get applied to the other replicas? There are several different ways how you
can implement replication. One option is to send the same update query to the follower, and it
executes the same statement on its own copy of the database. Another option is to ship the
write-ahead log from the leader to the follower.</p>

<p>A third option for replication, which I’ll focus on here, is called a <em>logical log</em>. In this case,
the leader writes out the effect that the query had — i.e. which rows were inserted, updated or
deleted — like a kind of diff. For an update, like in this example, the logical log identifies the
row that was changed (using a primary key or some kind of internal tuple identifier), gives the new
value of that row, and perhaps also the old value.</p>

<p>This might seem like nothing special, but notice that something interesting has happened here:</p>

<p><img src="/2015/03/insideout-08.png" alt="Update statement is imperative, replication event is immutable" width="550" height="412" /></p>

<p>At the top we have the update statement, an imperative statement describing the state mutation. It
is an instruction to the database, telling it to modify certain rows in the database that match
certain conditions.</p>

<p>On the other hand, when the write is replicated from the leader to the follower as part of the
logical log, it takes a different form: it becomes an event, stating that at a particular point in
time, a particular customer changed the quantity of a particular product in their cart from 1 to 3.
And this is a <em>fact</em> — even if the customer later removes the item from their cart, or changes the
quantity again, or goes away and never comes back, that doesn’t change the fact that this state
change occurred. The fact always remains true.</p>

<p>This distinction between an imperative modification and an immutable fact is something you may have
seen in the context of
<a href="http://blog.confluent.io/2015/01/29/making-sense-of-stream-processing/">event sourcing</a>. That’s
a method of database design that says you should structure all of your data as immutable facts, and
it’s an interesting idea.</p>

<p>However, what I’m saying here is: even if you use your database in the traditional way, overwriting
old state with new state, the database’s internal replication mechanism may still be translating
those imperative statements into a stream of immutable events.</p>

<p>Hold that thought for now: I’m going to talk about some completely different things, and return to
this idea later.</p>

<p><img src="/2015/03/insideout-09.png" alt="Title: 2. Secondary indexes" width="550" height="302" /></p>

<p>The second one of the four things I want to talk about is <em>secondary indexing</em>. You’re probably
familiar with secondary indexes — they are the bread and butter of relational databases. Using the
shopping cart example again:</p>

<p><img src="/2015/03/insideout-10.png" alt="Two indexes on one table" width="550" height="412" /></p>

<p>You have a table with different columns, and you may have several different indexes on that table in
order to be able to efficiently find rows that match a particular query. For example, you may run
some SQL to create two indexes: one on the customer_id column, and a separate index on the
product_id column.</p>

<p>Using the index on customer_id you can then efficiently find all the items that a particular
customer has in their cart. Using the index on product_id you can efficiently find all the carts
that contain a particular product.</p>

<p>What does the database do when you run one of these CREATE INDEX queries?</p>

<p><img src="/2015/03/insideout-11.png" alt="An index is a data structure derived from table data" width="550" height="412" /></p>

<p>The database scans over the entire table, and it creates an auxiliary data structure for each index.
An index is a data structure that represents the information in the base table in some different
way. In this case, the index is a key-value-like structure: the keys are the contents of the column
that you’re indexing, and the values are the rows that contain this particular key.</p>

<p><img src="/2015/03/insideout-12.png" alt="Values in table cells become keys in the index" width="550" height="412" /></p>

<p>Put another way: to build the index for the customer_id column, the database takes all the values
that appear in that column, and uses them as keys in a dictionary. A value points at all of the
occurrences of that value — for example, the index entry 123 points at all of the rows which have
a customer_id of 123. Similarly for the other index.</p>

<p>The important point here is that the process of going from the base table to the indexes is
completely mechanical. You simply tell the database that you want a particular index to exist, and
it goes away and builds that index for you.</p>

<p><img src="/2015/03/insideout-13.png" alt="Index is generated from table through a derivation function" width="550" height="412" /></p>

<p>The index doesn’t add any new information to the database — it just represents the existing data in
a different structure. (Put another way, if you drop the index, that doesn’t delete any data from
your database.) It’s a redundant data structure that only exists to make certain queries faster. And
that data structure can be entirely <em>derived</em> from the original table.</p>

<p>Creating an index is essentially a transformation which takes a database table as input, and
produces an index as output. The transformation consists of going through all the rows in the table,
picking out the field that you want to index, and restructuring the data so that you can look up by
that field. That transformation is built into the database, so you don’t need to implement it
yourself. You just tell the database that you want an index on a particular field to exist, and it
does all the work of building it.</p>

<p>Another great thing about indexes: whenever the data in the underlying table changes, the database
automatically updates the indexes to be consistent with the new data in the table. In other words,
this transformation function which derives the index from the original table is not just applied
once when you create the index, but applied continuously.</p>

<p>With many databases, these index updates are even done in a transactionally consistent way. This
means that any later transactions will see the data in the index in the same state as it is in the
underlying table. If a transaction aborts and rolls back, the index modifications are also rolled
back. That’s a really great feature which we often don’t appreciate!</p>

<p><img src="/2015/03/insideout-14.png" alt="Create index concurrently" width="550" height="362" /></p>

<p>What’s even better is that some databases let you build an index at the same time as continuing to
process write queries. In PostgreSQL, for example, you can say
<a href="http://www.postgresql.org/docs/9.4/static/sql-createindex.html#SQL-CREATEINDEX-CONCURRENTLY">CREATE INDEX CONCURRENTLY</a>.
On a large table, creating an index could take several hours, and on a production database you
wouldn’t want to have to stop writing to the table while the index is being built. The index builder
really needs to be a background process which can run while your application is simultaneously
reading and writing to the database as usual.</p>

<p>The fact that databases can do this is quite impressive. After all, to build an index, the database
has to scan the entire table contents, but those contents are changing at the same time as the scan
is happening. The index builder is tracking a moving target. At the end, the database ends up with
a transactionally consistent index, despite the fact that the data was changing concurrently.</p>

<p>In order to do this, the database needs to build the index from a consistent snapshot at one point
in time, and also keep track of all the changes that occurred since that snapshot while the index
build was in progress. That’s a really cool feature.</p>

<p>So far we’ve discussed two aspects of databases: replication and secondary indexing. Let’s move on
to number 3: caching.</p>

<p><img src="/2015/03/insideout-15.png" alt="Title: 3. Caching" width="550" height="194" /></p>

<p>What I’m talking about here is caching that is explicitly done by the application. (You also get
caching happening automatically at various levels, such as the operating system’s page cache and the
CPU caches, but that’s not what I’m talking about here.)</p>

<p>Say you have a website that becomes popular, and it becomes too expensive or too slow to hit the
database for every web request, so you introduce a caching layer — often using
<a href="http://memcached.org/">memcached</a> or <a href="http://redis.io/">Redis</a> or something of that sort. And often
this cache is managed in application code, which typically looks something like this:</p>

<p><img src="/2015/03/insideout-16.png" alt="Reading from a cache; filling cache from DB on miss" width="550" height="412" /></p>

<p>When a request arrives at the application, you first look in a cache to see whether the data you
want is already there. The cache lookup is typically by some key that describes the data you want.
If the data is in the cache, you can return it straight to the client.</p>

<p>If the data you want isn’t in the cache, that’s a cache miss. You then go to the underlying
database, and query the data that you want. On the way out, the application also writes that data to
the cache, so that it’s there for the next request that needs it. The thing it writes to the cache
is whatever the application would have wanted to see there in the first place. Then the application
returns the data to the client.</p>

<p>This is a very common pattern, but there are several big problems with it.</p>

<p><img src="/2015/03/insideout-17.png" alt="Problems with read-through caching" width="550" height="412" /></p>

<p>The first problem is that clichéd quote about there being only
<a href="http://martinfowler.com/bliki/TwoHardThings.html">two hard problems in computer science</a> (which
I can’t stand any more). But seriously, if you’re managing a cache like this, then cache
invalidation really is tricky. When data in the underlying database changes, how do you know what
entries in the cache to expire or update?  One option is to have an expiry algorithm which figures
out which database change affects which cache entries, but those algorithms are brittle and
error-prone. Alternatively, you can just have a time-to-live (expiry time) and accept that you
sometimes read stale data from the cache, but such staleness is often unacceptable.</p>

<p>Another problem is that this architecture is very prone to race conditions. For example, say you
have two processes concurrently writing to the database and also updating the cache. They might
update the database in one order, and the cache in the other order, and now the two are
inconsistent. Or if you fill the cache on read, you may read and write concurrently, and so the
cache is updated with a stale value while the concurrent write is occurring. I suspect that most of
us building these systems just pretend that the race conditions don’t exist, because they are just
too much to think about.</p>

<p>A third problem is cold start. If you reboot your memcached servers and they lose all their cached
contents, suddenly every request is a cache miss, the database is overloaded because of the sudden
surge in requests, and you’re in a world of pain. If you want to create a new cache, you need some
way of bootstrapping its contents without overloading other parts of the system.</p>

<p><img src="/2015/03/insideout-18.png" alt="Creating an index is simple, maintaining a cache is a mess" width="550" height="361" /></p>

<p>So, here we have a contrast: on the one hand, creating a secondary index in a database is
beautifully simple, one line of SQL — the database handles it automatically, keeping everything
up-to-date, and even making the index transactionally consistent. On the other hand,
application-level cache maintenance is a complete mess of complicated invalidation logic, race
conditions and operational problems.</p>

<p>Why should it be that way? Secondary indexes and caches are not fundamentally different. We said
earlier that a secondary index is just a redundant data structure on the side, which structures the
same data in a different way, in order to speed up read queries. A cache is just the same.</p>

<p><img src="/2015/03/insideout-19.png" alt="But a cache is also generated from a database through a derivation function" width="550" height="323" /></p>

<p>If you think about it, a cache is also the result of taking your data in one form (the form in which
it’s stored in the database) and transforming it into a different form for faster reads. In other
words, the contents of the cache are derived from the contents of the database.</p>

<p>We said that a secondary index is built by picking out one field from every record, and using that
as the key in a dictionary. In the case of a cache, we may apply an arbitrary function to the data:
the data from the database may have gone through some kind of business logic or rendering before
it’s put in the cache, and it may be the result of joining several records from different tables.
But the end result is similar: if you lose your cache, you can rebuild it from the underlying
database; thus, the contents of the cache are derived from the database.</p>

<p>In a read-through cache, this transformation happens on the fly, when there is a cache miss. But we
could perhaps imagine making the process of building and updating a cache more systematic, and more
similar to secondary indexes. Let’s return to that idea later.</p>

<p>I said I was going to talk about four different aspects of database. Let’s move on to the fourth:
<em>materialized views</em>.</p>

<p><img src="/2015/03/insideout-20.png" alt="Title 4. Materialized views" width="550" height="313" /></p>

<p>You may already know what materialized views are, but let me explain them briefly in case you’ve not
previously come across them. You may be more familiar with “normal” views — non-materialized views,
or virtual views, or whatever you want to call them. They work like this:</p>

<p><img src="/2015/03/insideout-21.png" alt="How a normal (non-materialized) view works" width="550" height="412" /></p>

<p>In a relational database, where views are common, you would create a view by saying “CREATE VIEW
viewname…” followed by a SELECT query. When you now look at this view in the database, it looks
somewhat like a table — you can use it in read queries like any other table. And when you do this,
say you SELECT * from that view, the database’s query planner actually rewrites the query into the
underlying query that you used in the definition of the view.</p>

<p>So you can think of a view as a kind of convenient alias, a wrapper that allows you to create an
abstraction, hiding a complicated query behind a simpler interface.</p>

<p>Contrast that with a <em>materialized</em> view, which is defined using almost identical syntax:</p>

<p><img src="/2015/03/insideout-22.png" alt="Creating a materialized views: copy of data" width="550" height="412" /></p>

<p>You also define a materialized view in terms of a SELECT query; the only syntactic difference is
that you say CREATE MATERIALIZED VIEW instead of CREATE VIEW. However, the implementation is totally
different.</p>

<p>When you create a materialized view, the database starts with the underlying tables — that is, the
tables you’re querying in the SELECT statement of the view (“bar” in the example). The database
scans over the entire contents of those tables, executes that SELECT query on all of the data, and
copies the results of that query into something like a temporary table.</p>

<p>The results of this query are actually written to disk, in a form that’s very similar to a a normal
table. And that’s really what “materialized” means in this context: it just means that the view’s
query has been executed and the results written to disk.</p>

<p>Remember that with the non-materialized view, the database would expand the view into the underlying
query at query time. On the other hand, when you query a materialized view, the database can read
its contents directly from disk, just like a table. The view’s underlying query has already been
executed ahead of time, so the database now just needs to read the result. This is especially useful
if the view’s underlying query is expensive.</p>

<p><img src="/2015/03/insideout-23.png" alt="Derivation function for materialized view" width="550" height="316" /></p>

<p>If you’re thinking “this seems like a cache of query results”, you would be right — that’s exactly
what it is. However, the big difference between a materialized view and application-managed caches
is the responsibility for keeping it up-to-date.</p>

<p>With a materialized view, you declare once how you want the materialized view to be defined, and the
database takes care of building that view from a consistent snapshot of the underlying tables (much
like building a secondary index). Moreover, when the data in the underlying tables changes, the
database takes responsibility for maintaining the materialized view, keeping it up-to-date. Some
databases do this materialized view maintenance on an ongoing basis, and some require you to
periodically refresh the view so that changes take effect. But you certainly don’t have to do cache
invalidation in your application code.</p>

<p>Another feature of application-managed caches is that you can apply arbitrary business logic to the
data before storing it in the cache, so that you can do less work at query time, or reduce the
amount of data you need to cache. Could a materialized view do something similar?</p>

<p><img src="/2015/03/insideout-24.png" alt="Use JavaScript stored procedure in derivation function" width="550" height="412" /></p>

<p>In a relational database, materialized views are defined using SQL, so the transformations they can
apply to the data are limited to the operations that are built into SQL (which are very restricted
compared to a general-purpose programming language). However, many databases can also be extended
using stored procedures — code that runs inside the database and can be called from SQL. For
example, you can <a href="http://pgxn.org/dist/plv8/">use JavaScript</a> to write PostgreSQL stored procedures.
This would let you implement something like an application-level cache, including arbitrary business
logic, running as a materialized view inside a database.</p>

<p>I am not convinced that this is necessarily a good idea: with code running inside your database,
it’s much harder to reason about monitoring, versioning, deployments, performance impact,
multi-tenant resource isolation, and so on. I don’t think I would advocate stored procedures as an
application development platform. However, the idea of materialized views is nevertheless
interesting.</p>

<p><img src="/2015/03/insideout-25.png" alt="Comparison of replication, secondary indexing, caching and materialized views" width="550" height="412" /></p>

<p>Let’s recap the four aspects of databases that we discussed: replication, secondary indexing,
caching, and materialized views. What they all have in common is that they are dealing with <em>derived
data</em> in some way: some secondary data structure is derived from an underlying, primary dataset, via
a transformation process.</p>

<ul>
  <li>We first discussed replication, i.e. keeping a copy of the same data on multiple machines. It
generally works very well, so we’ll give it a green smiley. There are some operational quirks with
some databases, and some of the tooling is a bit weird, but on the whole it’s mature
well-understood, and well-supported.</li>
  <li>Similarly, secondary indexing works very well. You can build a secondary index concurrently with
processing write queries, and the database somehow manages to do this in a transactionally
consistent way.</li>
  <li>On the other hand, application-level caching is a complete mess. Red frowny face.</li>
  <li>And materialized views are so-so: the idea is good, but the way they’re implemented is not what
you’d want from a modern application development platform. Maintaining the materialized view puts
additional load on the database, while actually the whole point of a cache is to <em>reduce</em> load on
the database!</li>
</ul>

<p><img src="/2015/03/insideout-26.png" alt="Magically self-updating cache..." width="550" height="412" /></p>

<p>However, there’s something really compelling about this idea of materialized views. I see
a materialized view almost as a kind of cache that magically keeps itself up-to-date. Instead of
putting all of the complexity of cache invalidation in the application (risking race conditions and
all the discussed problems), materialized views say that cache maintenance should be the
responsibility of the data infrastructure.</p>

<p><img src="/2015/03/insideout-27.png" alt="Let's rethink materialized views!" width="550" height="412" /></p>

<p>So let’s think about this: can we reinvent materialized views, implement them in a modern and
scalable way, and use them as a general mechanism for cache maintenance?</p>

<p>If we started with a clean slate, without the historical baggage of existing databases, what would
the ideal architecture for applications look like?</p>

<p><img src="/2015/03/insideout-28.png" alt="Traditionally, the replication stream is an implementation detail" width="550" height="412" /></p>

<p>Think back to leader-based replication which we discussed earlier. You make your writes to a leader,
which first applies the writes locally, and then sends those writes over the network to follower
nodes. In other words, the leader sends a stream of data changes to the followers. We discussed
a logical log as one way of implementing this.</p>

<p>In a traditional database architecture, application developers are not supposed to think about that
replication stream. It’s an implementation detail that is hidden by the database abstraction. SQL
queries and responses are the database’s public interface — and the replication stream is not part
of the public interface. You’re not supposed to go and parse that stream, and use it for your own
purposes. (Yes, there are <a href="http://tungsten-replicator.org/">tools that do this</a>, but in traditional
databases they are on the periphery of what is supported, whereas the SQL interface is the dominant
access method.)</p>

<p>And in some ways this is reasonable — the relational model is a pretty good abstraction, which is
why it has been so popular for several decades. But SQL is not the last word in databases.</p>

<p>What if we took that replication stream, and made it a first-class citizen in our data architecture?
What if we changed our infrastructure so that the replication stream was not an implementation
detail, but a key part of the public interface of the database? What if we <em>turn the database
inside-out</em>, take the implementation detail that was previously hidden, and make it a top-level
concern?  What would that look like?</p>

<p><img src="/2015/03/insideout-29.png" alt="Unbundle the database: make the transaction log a first-class component" width="550" height="412" /></p>

<p>Well, you could call that replication stream a <em>“transaction log”</em> or an <em>“event stream”</em>. You can
format all your writes as immutable events (facts), like we saw earlier in the context of a logical
log. Now each write is just an immutable event that you can append to the end of the transaction
log. The transaction log is a really simple, append-only data structure.</p>

<p>There are various ways of implementing this, but one good choice for the transaction log is to use
<a href="http://kafka.apache.org/">Apache Kafka</a>. It provides an append-only log data structure, but it does
so in a reliable and scalable manner — it durably writes everything to disk, it replicates data
across multiple machines (so that you don’t lose any data if you lose a machine), and it partitions
the stream across multiple machines for horizontal scalability. It easily handles
<a href="https://engineering.linkedin.com/kafka/benchmarking-apache-kafka-2-million-writes-second-three-cheap-machines">millions of writes</a>
per second on very modest hardware.</p>

<p>When you do this, you don’t need to necessarily make your writes through a leader database — you
could also imagine directly appending your writes to the log. (Going through a leader would still be
useful if you want to validate that writes meet certain constraints before writing them to the log.)</p>

<p>Writing to this system is now super fast and scalable, because the only thing you’re doing is
appending an event to a log. But what about reads? Reading data that has been written to the log is
now really inconvenient, because you have to scan the entire log to find the thing that you want.</p>

<p>The solution is to build materialized views from the writes in the transaction log. The materialized
views are just like the secondary indexes we talked about earlier: data structures that are derived
from the data in the log, and optimized for fast reading. A materialized view is just a
<a href="http://www.cidrdb.org/cidr2015/Papers/CIDR15_Paper16.pdf">cached subset of the log</a>, and you could
rebuild it from the log at any time. There could be many different materialized views onto the same
data: a key-value store, a full-text search index, a graph index, an analytics system, and so on.</p>

<p>You can think of this as <em>“unbundling”</em> the database. All the stuff that was previously packed into
a single monolithic software package is being broken out into modular components that can be
composed in flexible ways.</p>

<p><img src="/2015/03/insideout-30.png" alt="Derive materialized views from the transaction log using Samza" width="550" height="412" /></p>

<p>If you use Kafka to implement the log, how do you implement these materialized views? That’s where
<a href="http://samza.apache.org/">Apache Samza</a> comes in. It’s a stream processing framework that is
designed to go well with Kafka. With Samza, you write jobs that consume the events in a log, and
build cached views of the data in the log. When a job first starts up, it can build up its state by
consuming all the events in the log. And on an ongoing basis, whenever a new event appears in the
stream, it can update the view accordingly. The view can be any existing database or index — Samza
just provides the framework for processing the stream.</p>

<p>Anyone who wants to read data can now query those materialized views that are maintained by the
Samza jobs. Those views are just databases, indexes or caches, and you can send read-only requests
to them in the usual way. The difference to traditional database architecture is that if you want to
write to the system, you don’t write directly to the same databases that you read from. Instead, you
write to the log, and there is an explicit transformation process which takes the data on the log
and applies it to the materialized views.</p>

<p><img src="/2015/03/insideout-31.png" alt="Make writes an append-only stream of immutable facts" width="550" height="412" /></p>

<p>This separation of reads and writes is really the key idea here. By putting writes only in the log,
we can make them much simpler: we don’t need to update state in place, so we move away from the
problems of concurrent mutation of global shared state. Instead, we just keep an append-only log of
immutable events. This gives excellent performance (appending to a file is sequential I/O, which is
much faster than random-access I/O), is easily scalable (independent events can be put in separate
partitions), and is much easier to make reliable.</p>

<p>If it’s too expensive for you to keep the entire history of every change that ever happened to your
data, Kafka supports compaction, which is a kind of garbage collection process that runs in the
background. It’s very similar to the log compaction that databases do internally. But that doesn’t
change the basic principle of working with streams of immutable events.</p>

<p>These ideas are nothing new. To mention just a few examples,
<a href="http://blog.confluent.io/2015/01/29/making-sense-of-stream-processing/">Event Sourcing</a> is a data
modelling technique based on the same principle; query languages like
<a href="http://www.researchgate.net/profile/Letizia_Tanca/publication/3296132_What_you_always_wanted_to_know_about_Datalog_(and_never_dared_toask)/links/0fcfd50ca2d20473ca000000.pdf">Datalog</a>
have been based on immutable facts for decades; databases like <a href="http://www.datomic.com/">Datomic</a>
are built on immutability, enabling neat features like point-in-time historical queries; and the
<a href="http://manning.com/marz/">Lambda Architecture</a> is one possible approach for dealing with immutable
datasets at scale. At many levels of the stack, immutability is being
<a href="http://www.cidrdb.org/cidr2015/Papers/CIDR15_Paper16.pdf">applied successfully</a>.</p>

<p><img src="/2015/03/insideout-32.png" alt="Make reads from materialized views" width="550" height="344" /></p>

<p>On the read side, we need to start thinking less about querying databases, and more about consuming
and joining streams, and maintaining materialized views of the data in the form in which we want to
read it.</p>

<p>To be clear, I think querying databases will continue to be important: for example, when an analyst
is running exploratory ad-hoc queries against a data warehouse of historical data, it doesn’t make
much sense to use materialized views — for those kinds of queries it’s better to just keep all the
raw events, and to build databases which can scan over them very quickly. Modern column stores have
become very good at that.</p>

<p>But in situations where you might use application-managed caches (namely, an OLTP context where the
queries are known in advance and predictable), materialized views are very helpful.</p>

<p><img src="/2015/03/insideout-33.png" alt="Precompute and maintain materialized views from the log" width="550" height="412" /></p>

<p>There are a few differences between a read-through cache (which gets invalidated or updated within
the application code) and a materialized view (which is maintained by consuming a log):</p>

<ul>
  <li>With the materialized view, there is a principled <em>translation process</em> from the write-optimized
data in the log into the read-optimized data in the view. That translation runs in a separate
process which you can monitor, debug, scale and maintain independently from the rest of your
application. By contrast, in the typical read-through caching approach, the cache management logic
is deeply intertwined with the rest of the application, it’s easy to introduce bugs, and it’s
difficult to understand what is happening.</li>
  <li>A cache is filled on demand when there is a cache miss (so the first request for a given object is
always slow). By contrast, a materialized view is <em>precomputed</em>, i.e. its entire contents are
computed before anyone asks for it — just like a secondary index. This means there is no such
thing as a cache miss: if an item doesn’t exist in the materialized view, it doesn’t exist in the
database. There is no need to fall back to some kind of underlying database.</li>
  <li>Once you have this process for translating logs into views, you have great flexibility to create
new views: if you want to present your existing data in some new way, you can simply create a new
stream processing job, consume the input log from the beginning, and thus build a completely new
view onto all the existing data. (If you think about it, this is pretty much what a database does
internally when you create a new secondary index on an existing table.) You can then maintain both
views in parallel, gradually move applications to the new view, and eventually discard the old
view.  No more scary stop-the-world schema migrations.</li>
</ul>

<p><img src="/2015/03/insideout-34.png" alt="Mechanics that need to be solved for practical adoption" width="550" height="412" /></p>

<p>Of course, such a big change in application architecture and database architecture means that many
practical details need to be figured out: how do you deploy and monitor these stream processing
jobs, how do you make the system robust to various kinds of fault, how do you integrate with
existing systems, and so on? But the good news is that all of these issues are being worked on. It’s
a fast-moving area with lots of activity, so if you find it interesting, we’d love your
contributions to the open source projects.</p>

<p><img src="/2015/03/insideout-35.png" alt="Happiness" width="550" height="412" /></p>

<p>We are still figuring out how to build large-scale applications well — what techniques we can use to
make our systems scalable, reliable and maintainable. Put more bluntly, we need to figure out ways
to stop our applications turning into <a href="http://www.laputan.org/pub/foote/mud.pdf">big balls of mud</a>.</p>

<p>However, to me, this approach of immutable events and materialized views seems like a very promising
route forwards. I am optimistic that this kind of application architecture will help us build better
(more powerful and more reliable) software faster.</p>

<p><img src="/2015/03/insideout-36.png" alt="Why?" width="550" height="412" /></p>

<p>The changes I’ve proposed are quite radical, and it’s going to be a lot of work to put them into
practice. If we are going to completely change the way we use databases, we had better have some
very good reasons. So let me give three reasons why I think it’s worth moving towards a log of
immutable events.</p>

<p><img src="/2015/03/insideout-37.png" alt="Reason 1: Better data" width="550" height="345" /></p>

<p>Firstly, I think that writing data as a log produces better-quality data than if you update
a database directly. For example, if someone adds an item to their shopping cart and then removes it
again, those actions have information value. If you delete that information from the database when
a customer removes an item from the cart, you’ve just thrown away information that would have been
valuable for analytics and recommendation systems.</p>

<p>The entire long-standing debate about normalization in databases is predicated on the assumption
that data is going to be written and read in the same schema. A normalized database (with no
redundancy) is optimized for writing, whereas a denormalized database is optimized for reading. If
you separate the writing side (the log) from the reading side (materialized views), you can
denormalize the reading side to your heart’s content, but still retain the ability to process writes
efficiently.</p>

<p>Another very nice feature of an append-only log is that it allows much easier recovery from errors.
If you deploy some bad code that writes incorrect data to the database, or if a human enters some
incorrect data, you can look at the log to see the exact history of what happened, and
<a href="http://nathanmarz.com/blog/how-to-beat-the-cap-theorem.html">undo it</a>. That kind of recovery is
much harder if you’ve overwritten old data with new data, or even deleted data incorrectly. Also,
any kind of audit is much easier if you only ever append to a log — that’s why
<a href="http://blogs.msdn.com/b/pathelland/archive/2007/06/14/accountants-don-t-use-erasers.aspx">accountants don’t use erasers</a>.</p>

<p><img src="/2015/03/insideout-38.png" alt="Reason 2: Fully precomputed caches" width="550" height="347" /></p>

<p>Secondly, we can fix all the problems of read-through caches that we discussed earlier. The cold
start problem goes away, because we can simply precompute the entire contents of the cache (which
also means there’s no such thing as a cache miss).</p>

<p>If materialized views are only ever updated via the log, then a whole class of race conditions goes
away: the log defines the order in which writes are applied, so all the views that are based on the
same log apply the changes in the same order, so they end up being consistent with each other. The log
<a href="http://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying">squeezes the non-determinism</a>
of concurrency out of the stream of writes.</p>

<p>What I particularly like is that this architecture helps enable agile, incremental software
development. If you want to experiment with a new product feature, for which you need to present
existing data in a new way, you can just build a new view onto your data without affecting any of
the existing views. You can then show that view to a subset of users, and test whether it’s better
than the old thing. If yes, you can gradually move users to the new view; if not, you can just drop
the view as if nothing had happened. This is much more flexible than schema migrations, which are
generally an all-or-nothing affair. Being able to experiment freely with new features, without
onerous migration processes, is a tremendous enabler.</p>

<p><img src="/2015/03/insideout-39.png" alt="Reason 3: Streams everywhere" width="550" height="300" /></p>

<p>My third reason for wanting to change database architecture is that it allows us to put streams
everywhere. This point needs a bit more explanation.</p>

<p>Imagine what happens when a user of your application views some data. In a traditional database
architecture, the data is loaded from a database, perhaps transformed with some business logic, and
perhaps written to a cache. Data in the cache is rendered into a user interface in some way — for
example, by rendering it to HTML on the server, or by transferring it to the client as JSON and
rendering it on the client.</p>

<p>The result of template rendering is some kind of structure describing the user interface layout: in
a web browser, this would be the HTML DOM, and in a native application this would be using the
operating system’s UI components. Either way, a rendering engine eventually turns this description
of UI components into pixels in video memory, and this is what the user actually sees.</p>

<p><img src="/2015/03/insideout-40.png" alt="Transformation pipeline of materialized views" width="550" height="412" /></p>

<p>When you look at it like this, it looks very much like a data transformation pipeline. In fact, you
can think of each lower layer as a materialized view of the upper layer: the cache is a materialized
view of the database (the cache contents are derived from the database contents); the HTML DOM is
a materialized view of the cache (the HTML is derived from the JSON stored in the cache); and the
pixels in video memory are a materialized view of the HTML DOM (the rendering engine derives the
pixels from the UI layout).</p>

<p>Now, how well does each of these transformation steps work? I would argue that web browser rendering
engines are brilliant feats of engineering. You can use JavaScript to change some CSS class, or have
some CSS rules conditional on mouse-over, and the rendering engine automatically figures out which
rectangle of the page needs to be re-rendered as a result of the changes. It does
hardware-accelerated animations and even 3D transformations. The pixels in video memory are
automatically kept up-to-date with the underlying DOM state, and this very complex transformation
process works remarkably well.</p>

<p>What about the transformation from data objects to user interface components? I’ve given it a yellow
“so-so” smiley for now, as the techniques for updating user interface based on data changes are
still quite new. However, they are rapidly maturing: on the web, frameworks like
<a href="http://facebook.github.io/react/">Facebook’s React</a>, <a href="https://angularjs.org/">Angular</a> and
<a href="http://emberjs.com/">Ember</a> are enabling user interfaces that can be updated from a stream, and
Functional Reactive Programming (FRP) languages like <a href="http://elm-lang.org/">Elm</a> are in the same
area. There is a lot of activity in this field, and it is rapidly maturing towards a green smiley.</p>

<p>However, the transformation from database writes to cache/materialized view updates is still mostly
stuck in the dark ages. That’s what this entire talk is about: database-driven backend services are
currently the weakest link in this entire data transformation pipeline. Even if the user interface
can dynamically update when the underlying data changes, that’s not much use if the application
can’t detect when data changes!</p>

<p><img src="/2015/03/insideout-41.png" alt="Clients subscribe to materialized view changes" width="550" height="412" /></p>

<p>If we move to an architecture where materialized views are updated from a stream of changes, that
opens up an exciting new prospect: when a client reads from one of these views, it can keep the
connection open. If that view is later updated, due to some change that appeared in the stream, the
server can use this connection to notify the client about the change (for example, using
a <a href="https://developer.mozilla.org/en/docs/WebSockets">WebSocket</a> or
<a href="https://developer.mozilla.org/en-US/docs/Server-sent_events">Server-Sent Events</a>). The client can
then update its user interface accordingly.</p>

<p>This means that the client is not just reading the view at one point in time, but actually
subscribing to the stream of changes that may subsequently happen. Provided that the client’s
internet connection remains active, the server can push any changes to the client. After all, why
would you ever want outdated information on your screen if more recent information is available? The
notion of static web pages, which are requested once and then never change, is looking increasingly
anachronistic.</p>

<p><img src="/2015/03/insideout-42.png" alt="Move from request/response to subscribe/notify" width="550" height="339" /></p>

<p>However, allowing clients to subscribe to changes in data requires a big rethink of the way we write
applications. The request-response model is very deeply engrained in our thinking, in our network
protocols and in our programming languages: whether it’s a request to a RESTful service, or a method
call on an object, the assumption is generally that you’re going to make one request, and get one
response. There’s generally no provision for an ongoing stream of responses. Basically, I’m saying
that in the future, REST is not going to cut the mustard, because it’s based on a request-response
model.</p>

<p>Instead of thinking of requests and responses, we need to start thinking of subscribing to streams
and notifying subscribers of new events. And this needs to happen through all the layers of the
stack — the databases, the client libraries, the application servers, the business logic, the
frontends, and so on. If you want the user interface to dynamically update in response to data
changes, that will only be possible if we systematically apply stream thinking everywhere, so that
data changes can propagate through all the layers. I think we’re going to see a lot more people
using stream-friendly programming models based on actors and channels, or <em>reactive</em> frameworks such
as <a href="http://reactivex.io/">RxJava</a>.</p>

<p>I’m glad to see that some people are already working on this. A few weeks ago RethinkDB
<a href="http://rethinkdb.com/blog/realtime-web/">announced</a> that they are going to support clients
subscribing to query results, and being notified if the query results change.
<a href="https://www.meteor.com/">Meteor</a> and <a href="https://www.firebase.com/">Firebase</a> are also worth
mentioning, as frameworks which integrate the database backend and user interface layers so as to be
able to push changes into the user interface. These are excellent efforts. We need many more like
them.</p>

<p><img src="/2015/03/insideout-43.png" alt="Streams everywhere!" width="550" height="346" /></p>

<p>This brings us to the end of this talk. We started by observing that traditional databases and
caches are like global variables, a kind of shared mutable state that becomes messy at scale. We
explored four aspects of databases — replication, secondary indexing, caching and materialized views
— which naturally led us to the idea of streams of immutable events. We then looked at how things
would get better if we oriented our database architecture around streams and materialized views, and
found some compelling directions for the future.</p>

<p>Fortunately, this is not science fiction — it’s happening now. People are working on various parts
of the problem and finding good solutions. The tools at our disposal are rapidly becoming better.
It’s an exciting time to be building software.</p>

<p><em>If this excessively long article was not enough, and you want to read even more on the topic, I would recommend:</em></p>

<ul>
  <li><em>My upcoming book, <a href="http://dataintensive.net/">Designing Data-Intensive Applications</a>,
systematically explores the architecture of data systems. If you enjoyed this article, you’ll enjoy
the book too.</em></li>
  <li><em>I previously wrote about similar ideas in 2012, from a different perspective, in a blog post
called “<a href="/2012/10/01/rethinking-caching-in-web-apps.html">Rethinking caching in web apps</a>.”</em></li>
  <li><em>Jay Kreps has written several highly relevant articles, in particular about
<a href="http://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying">logs as a fundamental data abstraction</a>,
about the <a href="http://radar.oreilly.com/2014/07/questioning-the-lambda-architecture.html">lambda architecture</a>,
and about <a href="http://radar.oreilly.com/2014/07/why-local-state-is-a-fundamental-primitive-in-stream-processing.html">stateful stream processing</a>.</em></li>
  <li><em>The most common question people ask is: “but what about transactions?” — This is a somewhat open
research problem, but I think a promising way forward would be to layer a transaction protocol on
top of the asynchronous log. <a href="http://research.microsoft.com/pubs/199947/Tango.pdf">Tango</a> (from
Microsoft Research) describes one way of doing that, and <a href="http://www.bailis.org/">Peter Bailis</a> et
al.’s work on <a href="http://www.bailis.org/papers/ramp-sigmod2014.pdf">highly available transactions</a> is
also relevant.</em></li>
  <li><em>Pat Helland has been preaching this gospel for ages. His latest CIDR paper
<a href="http://www.cidrdb.org/cidr2015/Papers/CIDR15_Paper16.pdf">Immutability Changes Everything</a> is
a good summary.</em></li>
  <li><em>Jay Kreps has also written an applied guide to
<a href="http://blog.confluent.io/2015/02/25/stream-data-platform-1/">putting stream data to work in a company</a>.</em></li>
</ul>

                ]]></content:encoded>
            </item>
        
    </channel>
</rss>
